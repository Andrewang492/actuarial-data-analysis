@article{wolpertSTACKEDGENERALIZATION1992,
  title = {{{STACKED GENERALIZATION}}},
  author = {Wolpert, David H},
  year = {1992},
  pages = {58},
  abstract = {This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory.},
  file = {/Users/salvatorykessy/Zotero/storage/H25F2V9J/Wolpert - STACKED GENERALIZATION.pdf},
  language = {en}
}

@article{islamCreditDefaultMining2018,
  langid = {english},
  title = {Credit {{Default Mining Using Combined Machine Learning}} and {{Heuristic Approach}}},
  abstract = {Predicting potential credit default accounts in advance is challenging. Traditional statistical techniques typically cannot handle large amounts of data and the dynamic nature of fraud and humans. To tackle this problem, recent research has focused on artificial and computational intelligence based approaches. In this work, we present and validate a heuristic approach to mine potential default accounts in advance where a risk probability is precomputed from all previous data and the risk probability for recent transactions are computed as soon they happen. Beside our heuristic approach, we also apply a recently proposed machine learning approach that has not been applied previously on our targeted dataset [15]. As a result, we find that these applied approaches outperform existing state-of-the-art approaches.},
  date = {2018},
  pages = {7},
  author = {Islam, Sheikh Rabiul and Eberle, William and Ghafoor, Sheikh Khaled},
  file = {/Users/salvatorykessy/Zotero/storage/VR6BC4Y3/Islam et al. - Credit Default Mining Using Combined Machine Learn.pdf}
}

@article{ledellAUCMaximizingEnsemblesMetalearning2016,
  langid = {english},
  title = {{{AUC}}-{{Maximizing Ensembles}} through {{Metalearning}}},
  volume = {12},
  issn = {2194-573X, 1557-4679},
  url = {https://www.degruyter.com/doi/10.1515/ijb-2015-0035},
  doi = {10.1515/ijb-2015-0035},
  abstract = {Area Under the ROC Curve (AUC) is often used to measure the performance of an estimator in binary classification problems. An AUC-maximizing classifier can have significant advantages in cases where ranking correctness is valued or if the outcome is rare. In a Super Learner ensemble, maximization of the AUC can be achieved by the use of an AUC-maximining metalearning algorithm. We discuss an implementation of an AUC-maximization technique that is formulated as a nonlinear optimization problem. We also evaluate the effectiveness of a large number of different nonlinear optimization algorithms to maximize the cross-validated AUC of the ensemble fit. The results provide evidence that AUC-maximizing metalearners can, and often do, out-perform nonAUC-maximizing metalearning methods, with respect to ensemble AUC. The results also demonstrate that as the level of imbalance in the training data increases, the Super Learner ensemble outperforms the top base algorithm by a larger degree.},
  number = {1},
  journaltitle = {The International Journal of Biostatistics},
  urldate = {2020-07-15},
  date = {2016-05-01},
  pages = {203-218},
  author = {LeDell, Erin and van der Laan, Mark J. and Petersen, Maya},
  options = {useprefix=true},
  file = {/Users/salvatorykessy/Zotero/storage/RXW3HAWN/LeDell et al. - 2016 - AUC-Maximizing Ensembles through Metalearning.pdf}
}

@article{bergmeirUsefulnessCrossvalidationDirectional2014,
  title = {On the Usefulness of Cross-Validation for Directional Forecast Evaluation},
  author = {Bergmeir, Christoph and Costantini, Mauro and Ben{\'i}tez, Jos{\'e} M.},
  year = {2014},
  month = aug,
  volume = {76},
  pages = {132--143},
  issn = {01679473},
  doi = {10.1016/j.csda.2014.02.001},
  abstract = {The usefulness of a predictor evaluation framework which combines a blocked crossvalidation scheme with directional accuracy measures is investigated. The advantage of using a blocked cross-validation scheme with respect to the standard out-of-sample procedure is that cross-validation yields more precise error estimates of the prediction error since it makes full use of the data. In order to quantify the gain in precision when directional accuracy measures are considered, a Monte Carlo analysis using univariate and multivariate models is provided. The experiments indicate that more precise estimates are obtained with the blocked cross-validation procedure. An application is carried out on forecasting UK interest rate for illustration purposes. The results show that in such a situation with small samples the cross-validation scheme may have considerable advantages over the standard out-of-sample evaluation procedure as it may help to overcome problems induced by the limited information the directional accuracy measures contain due to their binary nature.},
  file = {/Users/salvatorykessy/Zotero/storage/ZILTJUHH/Bergmeir et al. - 2014 - On the usefulness of cross-validation for directio.pdf},
  journal = {Computational Statistics \& Data Analysis},
  language = {en}
}


@article{hoetingBayesianModelAveraging1999,
  title = {Bayesian {{Model Averaging}}: {{A Tutorial}}},
  author = {Hoeting, Jennifer A and Madigan, David and Raftery, Adrian E and Volinsky, Chris T},
  year = {1999},
  pages = {36},
  abstract = {Standard statistical practice ignores model uncertainty. Data analysts typically select a model from some class of models and then proceed as if the selected model had generated the data. This approach ignores the uncertainty in model selection, leading to over-confident inferences and decisions that are more risky than one thinks they are. Bayesian model averaging (BMA) provides a coherent mechanism for accounting for this model uncertainty. Several methods for implementing BMA have recently emerged. We discuss these methods and present a number of examples. In these examples, BMA provides improved out-ofsample predictive performance. We also provide a catalogue of currently available BMA software.},
  file = {/Users/salvatorykessy/Zotero/storage/BR3SCCI3/Hoeting et al. - Bayesian Model Averaging A Tutorial.pdf},
  language = {en}
}

@article{hugheyRobustMetaanalysisGene2015,
  langid = {english},
  title = {Robust Meta-Analysis of Gene Expression Using the Elastic Net},
  volume = {43},
  issn = {0305-1048, 1362-4962},
  url = {https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gkv229},
  doi = {10.1093/nar/gkv229},
  abstract = {Meta-analysis of gene expression has enabled numerous insights into biological systems, but current methods have several limitations. We developed a method to perform a meta-analysis using the elastic net, a powerful and versatile approach for classification and regression. To demonstrate the utility of our method, we conducted a meta-analysis of lung cancer gene expression based on publicly available data. Using 629 samples from five data sets, we trained a multinomial classifier to distinguish between four lung cancer subtypes. Our meta-analysis-derived classifier included 58 genes and achieved 91\% accuracy on leave-one-study-out cross-validation and on three independent data sets. Our method makes meta-analysis of gene expression more systematic and expands the range of questions that a metaanalysis can be used to address. As the amount of publicly available gene expression data continues to grow, our method will be an effective tool to help distill these data into knowledge.},
  number = {12},
  journaltitle = {Nucleic Acids Research},
  shortjournal = {Nucleic Acids Res},
  urldate = {2020-07-15},
  date = {2015-07-13},
  pages = {e79-e79},
  author = {Hughey, Jacob J. and Butte, Atul J.},
  file = {/Users/salvatorykessy/Zotero/storage/JV4WZXFR/Hughey and Butte - 2015 - Robust meta-analysis of gene expression using the .pdf}
}

@ARTICLE{RePEc:bla:jrinsu:v:73:y:2006:i:4:p:687-718,
title = {A Two‐Factor Model for Stochastic Mortality with Parameter Uncertainty: Theory and Calibration},
author = {Cairns, Andrew J. G. and Blake, David and Dowd, Kevin},
year = {2006},
journal = {Journal of Risk & Insurance},
volume = {73},
number = {4},
pages = {687-718},
abstract = {In this article, we consider the evolution of the post‐age‐60 mortality curve in the United Kingdom and its impact on the pricing of the risk associated with aggregate mortality improvements over time: so‐called longevity risk. We introduce a two‐factor stochastic model for the development of this curve through time. The first factor affects mortality‐rate dynamics at all ages in the same way, whereas the second factor affects mortality‐rate dynamics at higher ages much more than at lower ages. The article then examines the pricing of longevity bonds with different terms to maturity referenced to different cohorts. We find that longevity risk over relatively short time horizons is very low, but at horizons in excess of ten years it begins to pick up very rapidly. A key component of the article is the proposal and development of a method for calculating the market risk‐adjusted price of a longevity bond. The proposed adjustment includes not just an allowance for the underlying stochastic mortality, but also makes an allowance for parameter risk. We utilize the pricing information contained in the November 2004 European Investment Bank longevity bond to make inferences about the likely market prices of the risks in the model. Based on these, we investigate how future issues might be priced to ensure an absence of arbitrage between bonds with different characteristics.},
url = {https://EconPapers.repec.org/RePEc:bla:jrinsu:v:73:y:2006:i:4:p:687-718}
}

@article{armstrongForecastingDictionary2000,
  title = {The {{Forecasting Dictionary}}},
  author = {Armstrong, J Scott and Carroll, Lewis},
  year = {2000},
  pages = {62},
  file = {/Users/salvatorykessy/Zotero/storage/JSDPLLS5/Armstrong and Carroll - The Forecasting Dictionary.pdf},
  language = {en}
}

@article{doi:10.1080/10920277.2009.10597538,
author = { Andrew J. G.   Cairns  and  David   Blake  and  Kevin   Dowd  and  Guy D.   Coughlan  and  David   Epstein  and  Alen   Ong  and  Igor   Balevich },
title = {A Quantitative Comparison of Stochastic Mortality Models Using Data From England and Wales and the United States},
journal = {North American Actuarial Journal},
volume = {13},
number = {1},
pages = {1-35},
year  = {2009},
publisher = {Routledge},
doi = {10.1080/10920277.2009.10597538},

URL = { 
        https://doi.org/10.1080/10920277.2009.10597538
    
},
eprint = { https://doi.org/10.1080/10920277.2009.10597538
 }
}

@article{hansenModelConfidenceSet2011,
  title = {The {{Model Confidence Set}}},
  author = {Hansen, Peter and Lunde, Asger and Nason, James},
  year = {2011},
  volume = {79},
  pages = {453--497},
  issn = {0012-9682},
  doi = {10.3982/ECTA5771},
  abstract = {This paper introduces the model confidence set (MCS) and applies it to the selection of models. A MCS is a set of models that is constructed such that it will contain the best model with a given level of confidence. The MCS is in this sense analogous to a confidence interval for a parameter. The MCS acknowledges the limitations of the data, such that uninformative data yield a MCS with many models, whereas informative data yield a MCS with only a few models. The MCS procedure does not assume that a particular model is the true model; in fact, the MCS procedure can be used to compare more general objects, beyond the comparison of models. We apply the MCS procedure to two empirical problems. First, we revisit the inflation forecasting problem posed by Stock and Watson (1999), and compute the MCS for their set of inflation forecasts. Second, we compare a number of Taylor rule regressions and determine the MCS of the best regression in terms of in-sample likelihood criteria.},
  file = {/Users/salvatorykessy/Zotero/storage/EWAVWWMA/2011 - The Model Confidence Set.pdf},
  journal = {Econometrica},
  language = {en},
  number = {2}
}
@article{sridaranDataAnalyticsParadigm2018,
  title = {A {{Data Analytics Paradigm}} for the {{Construction}}, {{Selection}}, and {{Evaluation}} of {{Mortality Models}}},
  author = {Sridaran, Dilan},
  year = {2018},
  abstract = {Humanity has made, and continues to make, significant progress in averting and delaying death, which burdens society with increased longevity costs. This has brought to the fore the critical importance of mortality forecasting for actuaries and demographers. Consequently, numerous mortality models have been proposed, with the most popular and commonly- referenced models belonging to a generalised age-period-cohort framework (Lee and Carter 1992; Cairns, Blake, and Dowd 2006; Plat 2009). These models decompose observed historical mortality rates across the dimensions of age, period, and cohort (or year-of-birth), which can then be extrapolated to forecast future outcomes. Recently, a large number of models have been proposed within this framework, many of which are over-parameterised and produce spurious forecasts, particularly over long horizons and for noisy data sets.
In this thesis we exploit data analytics techniques to provide a comprehensive framework to construct, select, and evaluate discrete-time age-period-cohort mortality models. To devise this robust framework, we leverage two key statistical learning tools \textendash{} cross validation and regularisation \textendash{} to draw as much insight as possible from limited data sets. We first propose a cross validation framework for model selection, which can be tailored to determine the features of mortality models that are desired for di},
  file = {/Users/salvatorykessy/Zotero/storage/FPR63ZE2/Sridaran - 2018 - Dilan sridaran(2).pdf},
  number = {November}
}



@article{sridharProcessModelingUsing1996,
  title = {Process Modeling Using Stacked Neural Networks},
  author = {Sridhar, Dasaratha},
  year = {1996},
  pages = {139},
  file = {/Users/salvatorykessy/Zotero/storage/7MZ874LX/Sridhar - Process modeling using stacked neural networks.pdf},
  language = {en}
}

@article{aldaveSystematicEnsembleLearning2014,
  title = {Systematic {{Ensemble Learning}} for {{Regression}}},
  author = {Aldave, Roberto and Dussault, Jean-Pierre},
  year = {2014},
  month = mar,
  abstract = {The motivation of this work is to improve the performance of standard stacking approaches or ensembles, which are composed of simple, heterogeneous base models, through the integration of the generation and selection stages for regression problems. We propose two extensions to the standard stacking approach. In the first extension we combine a set of standard stacking approaches into an ensemble of ensembles using a two-step ensemble learning in the regression setting. The second extension consists of two parts. In the initial part a diversity mechanism is injected into the original training data set, systematically generating different training subsets or partitions, and corresponding ensembles of ensembles. In the final part after measuring the quality of the different partitions or ensembles, a max-min rulebased selection algorithm is used to select the most appropriate ensemble/partition on which to make the final prediction. We show, based on experiments over a broad range of data sets, that the second extension performs better than the best of the standard stacking approaches, and is as good as the oracle of databases, which has the best base model selected by cross-validation for each data set. In addition to that, the second extension performs better than two state-of-the-art ensemble methods for regression, and it is as good as a third state-of-the-art ensemble method.},
  annote = {Comment: 38 pages, 6 figures. Submitted to Machine Learning},
  archivePrefix = {arXiv},
  eprint = {1403.7267},
  eprinttype = {arxiv},
  file = {/Users/salvatorykessy/Zotero/storage/7JDLQHZL/Aldave and Dussault - 2014 - Systematic Ensemble Learning for Regression.pdf},
  journal = {arXiv:1403.7267 [stat]},
  keywords = {Statistics - Machine Learning},
  language = {en},
  primaryClass = {stat}
}

@article{dowdBacktestingStochasticMortality2008,
  title = {Backtesting {{Stochastic Mortality Models}}: {{An Ex}}-{{Post Evaluation}} of {{Multi}}-{{Period Ahead}}-{{Density Forecasts}}},
  shorttitle = {Backtesting {{Stochastic Mortality Models}}},
  author = {Dowd, Kevin and Cairns, Andrew J. G. and Blake, David P. and Coughlan, Guy and Epstein, David and {Khalaf-Allah}, Marwa},
  year = {2008},
  issn = {1556-5068},
  doi = {10.2139/ssrn.1396201},
  abstract = {This study sets out a backtesting framework applicable to the multiperiod-ahead forecasts from stochastic mortality models and uses it to evaluate the forecasting performance of six different stochastic mortality models applied to English \& Welsh male mortality data. The models considered are the following: Lee-Carter's 1992 one-factor model; a version of Renshaw-Haberman's 2006 extension of the Lee-Carter model to allow for a cohort effect; the age-period-cohort model, which is a simplified version of Renshaw-Haberman; Cairns, Blake, and Dowd's 2006 two-factor model; and two generalized versions of the last named with an added cohort effect. For the data set used herein, the results from applying this methodology suggest that the models perform adequately by most backtests and that prediction intervals that incorporate parameter uncertainty are wider than those that do not. We also find little difference between the performances of five of the models, but the remaining model shows considerable forecast instability.},
  file = {/Users/salvatorykessy/Zotero/storage/UD4V5283/Dowd et al. - 2008 - Backtesting Stochastic Mortality Models An Ex-Pos.pdf},
  journal = {SSRN Electronic Journal},
  language = {en}
}
@misc{Data2015,
author = {{University of California Berkeley and Max Planck Institute for Demographic Research}},
title = {{Human Mortality Database}},
year = {2018},
journal = {https://www.mortality.org/}
}

@article{armstrongRulebasedForecastingDevelopment2007,
  title = {Rule-Based {{Forecasting}}: {{Development}} and {{Validation}} of an {{Expert Systems Approach}} to {{Combining Time Series Extrapolations}}},
  author = {Armstrong, J Scott and Collopy, Fred},
  year = {2007},
  pages = {27},
  abstract = {This paper examines the feasibility of rule-based forecasting, a procedure that applies forecasting expertise and domain knowledge to produce forecasts according to features of the data. We developed a rule base to make annual extrapolation forecasts for economic and demographic time series. The development of the rule base drew upon protocol analyses of five experts on forecasting methods. This rule base, consisting of 99 rules, combined forecasts from four extrapolation methods (the random walk, regression, Brown's linear exponential smoothing, and Holt's exponential smoothing) according to rules using 18 features of time series. For one-year ahead ex ante forecasts of 90 annual series, the median absolute percentage error (MdAPE) for rule-based forecasting was 13\% less than that from equally-weighted combined forecasts. For six-year ahead ex ante forecasts, rule-based forecasting had a MdAPE that was 42\% less. The improvement in accuracy of the rule-based forecasts over equally-weighted combined forecasts was statistically significant. Rule-based forecasting was more accurate than equal-weights combining in situations involving significant trends, low uncertainty, stability, and good domain expertise.},
  file = {/Users/salvatorykessy/Zotero/storage/HRT4RNNM/Armstrong and Collopy - Rule-based Forecasting Development and Validation.pdf},
  language = {en}
}

@article{arnold-gailleForecastingMortalityTrends2013,
  title = {Forecasting {{Mortality Trends Allowing}} for {{Cause}}-of-{{Death Mortality Dependence}}},
  author = {{Arnold (-Gaille)}, S{\'e}verine and Sherris, Michael},
  year = {2013},
  month = oct,
  volume = {17},
  pages = {273--282},
  issn = {1092-0277},
  doi = {10.1080/10920277.2013.838141},
  annote = {doi: 10.1080/10920277.2013.838141},
  journal = {North American Actuarial Journal},
  number = {4}
}

@article{barrowCroggingCrossvalidationAggregation2013,
  ids = {barrowCroggingCrossvalidationAggregation2013a},
  title = {Crogging (Cross-Validation Aggregation) for Forecasting - {{A}} Novel Algorithm of Neural Network Ensembles on Time Series Subsamples},
  author = {Barrow, Devon K. and Crone, Sven F.},
  year = {2013},
  issn = {9781467361293},
  doi = {10.1109/IJCNN.2013.6706740},
  abstract = {In classification, regression and time series prediction alike, cross-validation is widely employed to estimate the expected accuracy of a predictive algorithm by averaging predictive errors across mutually exclusive subsamples of the data. Similarly, bootstrapping aims to increase the validity of estimating the expected accuracy by repeatedly sub-sampling the data with replacement, creating overlapping samples of the data. Estimates are then used to anticipate of future risk in decision making, or to guide model selection where multiple candidates are feasible. Beyond error estimation, bootstrapping has recently been extended to combine each of the diverse models created for estimation, and aggregating over each of their predictions (rather than their errors), coined bootstrap aggregation or bagging. However, similar extensions of cross-validation to create diverse forecasting models have not been considered. In accordance with bagging, we propose to combine the benefits of cross-validation and forecast aggregation, i.e. crogging. We assesses different levels of cross-validation, including a (single-fold) hold-out approach, 2-fold and 10-fold cross validation and Monte-Carlos cross validation, to create diverse base-models of neural networks for time series prediction trained on different data subsets, and average their individual multiple-step ahead predictions. Results of forecasting the 111 time series of the NN3 competition indicate significant improvements accuracy through Crogging relative to Bagging or individual model selection of neural networks. \textcopyright{} 2013 IEEE.},
  file = {/Users/salvatorykessy/Zotero/storage/2VBTCCDA/Barrow, Crone - 2013 - Crogging (cross-validation aggregation) for forecasting - A novel algorithm of neural network ensembles on time s.pdf;/Users/salvatorykessy/Zotero/storage/9EGQBH5U/Barrow, Crone - 2013 - Crogging (cross-validation aggregation) for forecasting - A novel algorithm of neural network ensembles on time s.pdf},
  journal = {Proceedings of the International Joint Conference on Neural Networks}
}

@article{batesCombinationForecasts1969,
  title = {The {{Combination}} of {{Forecasts}}},
  author = {Bates, J. M. and Granger, C. W. J.},
  year = {1969},
  volume = {20},
  pages = {451--451},
  doi = {10.2307/3008764},
  abstract = {Aggregating information by combining forecasts from two or more forecasting methods is an alternative to using just a single method. In this paper we provide extensive empirical results showing that combined forecasts obtained through weighted averages can be quite accurate. Five procedures for estimating weights are investigated, and two appear to be superior to the others. These two procedures provide forecasts that are more accurate overall than forecasts from individual methods. Furthermore, they are superior to forecasts found from a simple unweighted average of the same methods. CR - Copyright \textcopyright{} 1983 Royal Statistical Society},
  file = {/Users/salvatorykessy/Zotero/storage/7BPQ2F6L/Sensitivity_analysis_stacked.pdf;/Users/salvatorykessy/Zotero/storage/XQNR7XDH/Bates, Granger - 2006 - The Combination of Forecasts.pdf},
  journal = {or},
  number = {4}
}

@article{zouAdaptiveLassoIts2006,
  title = {The {{Adaptive Lasso}} and {{Its Oracle Properties}}},
  author = {Zou, Hui},
  year = {2006},
  month = dec,
  volume = {101},
  pages = {1418--1429},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214506000000735},
  file = {/Users/salvatorykessy/Zotero/storage/4PRIA5Y4/Zou - 2006 - The Adaptive Lasso and Its Oracle Properties.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {476}
}

@article{huntRobustnessConvergenceLee2015,
  title = {Robustness and Convergence in the {{Lee}}\textendash{{Carter}} Model with Cohort Effects},
  author = {Hunt, Andrew and Villegas, Andr{\'e}s M.},
  year = {2015},
  month = sep,
  volume = {64},
  pages = {186--202},
  issn = {01676687},
  doi = {10.1016/j.insmatheco.2015.05.004},
  abstract = {Interest in cohort effects in mortality data has increased dramatically in recent years, with much of the research focused on extensions of the Lee-Carter model incorporating cohort parameters. However, some studies find that these models are not robust to changes in the data or fitting algorithm, which limits their suitability for many purposes. It has been suggested that these robustness problems may be the result of an unresolved identifiability issue. In this paper, after investigating systemically the robustness of cohort extensions of the Lee-Carter model and the convergence of the algorithms used to fit it to data, we demonstrate the existence of such an identifiability issue and propose an additional approximate identifiability constraint which solves many of the problems found.},
  file = {/Users/salvatorykessy/Zotero/storage/5YVICTKB/Hunt and Villegas - 2015 - Robustness and convergence in the Lee–Carter model.pdf},
  journal = {Insurance: Mathematics and Economics},
  language = {en}
}

@article{batesCombinationForecasts2006a,
  title = {The {{Combination}} of {{Forecasts}}},
  author = {Bates, J. M. and Granger, C. W. J.},
  year = {2006},
  volume = {20},
  pages = {451--451},
  doi = {10.2307/3008764},
  abstract = {Aggregating information by combining forecasts from two or more forecasting methods is an alternative to using just a single method. In this paper we provide extensive empirical results showing that combined forecasts obtained through weighted averages can be quite accurate. Five procedures for estimating weights are investigated, and two appear to be superior to the others. These two procedures provide forecasts that are more accurate overall than forecasts from individual methods. Furthermore, they are superior to forecasts found from a simple unweighted average of the same methods. CR - Copyright \textcopyright{} 1983 Royal Statistical Society},
  file = {/Users/salvatorykessy/Zotero/storage/FBDWSS22/Bates, Granger - 2006 - The Combination of Forecasts(2).pdf},
  journal = {or},
  number = {4}
}

@article{batesCombinationForecasts2006b,
  title = {The {{Combination}} of {{Forecasts}}},
  author = {Bates, J. M. and Granger, C. W. J.},
  year = {2006},
  volume = {20},
  pages = {451--451},
  doi = {10.2307/3008764},
  abstract = {Aggregating information by combining forecasts from two or more forecasting methods is an alternative to using just a single method. In this paper we provide extensive empirical results showing that combined forecasts obtained through weighted averages can be quite accurate. Five procedures for estimating weights are investigated, and two appear to be superior to the others. These two procedures provide forecasts that are more accurate overall than forecasts from individual methods. Furthermore, they are superior to forecasts found from a simple unweighted average of the same methods. CR - Copyright \textcopyright{} 1983 Royal Statistical Society},
  file = {/Users/salvatorykessy/Zotero/storage/UDN3U7RI/Bates, Granger - 2006 - The Combination of Forecasts(2).pdf},
  journal = {or},
  number = {4}
}

@article{batesCombinationForecasts2006c,
  title = {The {{Combination}} of {{Forecasts}}},
  author = {Bates, J. M. and Granger, C. W. J.},
  year = {2006},
  volume = {20},
  pages = {451--451},
  doi = {10.2307/3008764},
  abstract = {Aggregating information by combining forecasts from two or more forecasting methods is an alternative to using just a single method. In this paper we provide extensive empirical results showing that combined forecasts obtained through weighted averages can be quite accurate. Five procedures for estimating weights are investigated, and two appear to be superior to the others. These two procedures provide forecasts that are more accurate overall than forecasts from individual methods. Furthermore, they are superior to forecasts found from a simple unweighted average of the same methods. CR - Copyright \textcopyright{} 1983 Royal Statistical Society},
  file = {/Users/salvatorykessy/Zotero/storage/SCVT92HG/Bates, Granger - 2006 - The Combination of Forecasts.pdf},
  journal = {or},
  number = {4}
}

@article{bergeron-boucherCoherentForecastsMortality2017,
  title = {Coherent Forecasts of Mortality with Compositional Data Analysis},
  author = {{Bergeron-Boucher}, Marie-Pier and {Canudas-Romo}, Vladimir and Oeppen, Jim and Vaupel, James W.},
  year = {2017},
  month = aug,
  volume = {37},
  pages = {527--566},
  issn = {1435-9871},
  doi = {10.4054/DemRes.2017.37.17},
  abstract = {Methods 530 2.1 The Lee\textendash Carter (LC) model 530 2.2 The Li\textendash Lee model: LC-coherent 531 2.3 Forecasting with compositional data analysis (CoDa) 531 2.4 The CoDa-coherent model 533 3 Data 534 4 The underlying models 535 4.1 The parameters' interpretation and forecasts 535 4.2 Explained variance and fitted models 537 5
Results 538 5.1 Evaluating the models 538 5.2 Life expectancy in 2050 541 5.2.1 More optimistic forecasts 541 5.2.2 Coherence in the forecasts 545 6 Discussion 546 7
Conclusion},
  file = {/Users/salvatorykessy/Zotero/storage/HMCNX2LL/Bergeron-Boucher et al. - 2017 - Coherent forecasts of mortality with compositional.pdf;/Users/salvatorykessy/Zotero/storage/KG2SCCFA/cv.pdf;/Users/salvatorykessy/Zotero/storage/KPGY8QVD/ddd.pdf},
  journal = {Demographic Research},
  language = {en}
}

@article{bergeron-boucherWhatForecastExploratory2019,
  title = {What to Forecast? {{An}} Exploratory Study of the Implication of Using Different Indicators to Forecast Mortality},
  author = {{Bergeron-Boucher}, Marie-Pier and Kj, S{\o}ren},
  year = {2019},
  pages = {15},
  abstract = {Researchers and private and public institutions have suggested different ways to forecast mortality over the years. Many of these forecasting models are based on extrapolative methods of the age-specific death rates. However, more recent studies have looked into forecasting models based on other mortality indicators, such as life expectancy or the life table deaths. We here ask what are the implication of choosing one specific indicator over another to forecast mortality? We compare five models based on singular value decomposition of the matrix by age and time of different indicators: logged death rates, logit of death probabilities, logit of survival probabilities, log-ratio transformation of the life table deaths and life expectancy at birth. The results show that the time\textendash indexes of all these indicators are very similar, but that forecasting using death rates and probabilities of death leads to more pessimistic forecasts than the other models.},
  file = {/Users/salvatorykessy/Zotero/storage/H9JSR5Q2/4.pdf;/Users/salvatorykessy/Zotero/storage/UFIJGL9M/Akaike_weights.pdf;/Users/salvatorykessy/Zotero/storage/V5G549Y7/Bergeron-Boucher and Kj - What to forecast An exploratory study of the impl.pdf},
  language = {en}
}

@article{bergmeirNoteValidityCrossvalidation2018,
  title = {A Note on the Validity of Cross-Validation for Evaluating Autoregressive Time Series Prediction},
  author = {Bergmeir, Christoph and Hyndman, Rob J. and Koo, Bonsoo},
  year = {2018},
  month = apr,
  volume = {120},
  pages = {70--83},
  issn = {01679473},
  doi = {10.1016/j.csda.2017.11.003},
  abstract = {One of the most widely used standard procedures for model evaluation in classification and regression is K-fold cross-validation (CV). However, when it comes to time series forecasting, because of the inherent serial correlation and potential non-stationarity of the data, its application is not straightforward and often omitted by practitioners in favour of an out-of-sample (OOS) evaluation. In this paper, we show that in the case of a purely autoregressive model, the use of standard K-fold CV is possible as long as the models considered have uncorrelated errors. Such a setup occurs, for example, when the models nest a more appropriate model. This is very common when Machine Learning methods are used for prediction, where CV in particular is suitable to control for overfitting the data. We present theoretical insights supporting our arguments. Furthermore, we present a simulation study and a real-world example where we show empirically that K-fold CV performs favourably compared to both OOS evaluation and other time-series-specific techniques such as non-dependent cross-validation.},
  file = {/Users/salvatorykessy/Zotero/storage/MS4CQRUE/Bergmeir et al. - 2018 - A note on the validity of cross-validation for eva.pdf},
  journal = {Computational Statistics \& Data Analysis},
  language = {en}
}

@article{blakeLongevityBondsFinancial2006,
  title = {Longevity {{Bonds}}: {{Financial Engineering}}, {{Valuation}}, and {{Hedging}}},
  shorttitle = {Longevity {{Bonds}}},
  author = {Blake, David and Cairns, Andrew and Dowd, Kevin and MacMinn, Richard},
  year = {2006},
  month = dec,
  volume = {73},
  pages = {647--672},
  issn = {0022-4367, 1539-6975},
  doi = {10.1111/j.1539-6975.2006.00193.x},
  abstract = {This article examines the main characteristics of longevity bonds (LBs) and shows that they can take a large variety of forms which can vary enormously in their sensitivities to longevity shocks. We examine different ways of financially engineering LBs and consider problems arising from the dearth of ultra-long government bonds and the choice of the reference population index. The article also looks at valuation issues in an incomplete markets context and finishes with an examination of how LBs can be used as a risk management tool for hedging longevity risks.},
  file = {/Users/salvatorykessy/Zotero/storage/53SIDH8K/Blake et al. - 2006 - Longevity Bonds Financial Engineering, Valuation,.pdf;/Users/salvatorykessy/Zotero/storage/MEX7UL8L/4.pdf},
  journal = {Journal of Risk \& Insurance},
  language = {en},
  number = {4}
}

@article{blakeStillLivingMortality2018,
  title = {Still {{Living With Mortality}}: {{The Longevity Risk Transfer Market After One Decade}}},
  shorttitle = {Still {{Living With Mortality}}},
  author = {Blake, David P. and Cairns, Andrew J. G. and Dowd, Kevin and Kessler, Amy R.},
  year = {2018},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3271283},
  abstract = {This paper updates Living with Mortality published in 2006. It describes how the longevity risk transfer market has developed over the intervening period, and, in particular, how insurance-based solutions \textendash{} buy-outs, buy-ins and longevity insurance \textendash{} have triumphed over capital markets solutions that were expected to dominate at the time. Some capital markets solutions \textendash{} longevity-spread bonds, longevity swaps, q-forwards, and tail-risk protection \textendash{} have come to market, but the volume of business has been disappointingly low. The reason for this is that when market participants compare the index-based solutions of the capital markets with the customized solutions of insurance companies in terms of basis risk, credit risk, regulatory capital, collateral, and liquidity, the former perform on balance less favourably despite a lower potential cost. We discuss the importance of stochastic mortality models for forecasting future longevity and examine some applications of these models, e.g., determining the longevity risk premium and estimating regulatory capital relief. The longevity risk transfer market is now beginning to recognize that there is insufficient capacity in the insurance and reinsurance industries to deal fully with demand and new solutions for attracting capital markets investors are now being examined \textendash{} such as longevity-linked securities and reinsurance sidecars.},
  file = {/Users/salvatorykessy/Zotero/storage/CBQBVELG/Blake et al. - 2018 - Still Living With Mortality The Longevity Risk Tr.pdf},
  journal = {SSRN Electronic Journal},
  language = {en}
}

@article{boothApplyingLeeCarterConditions2002,
  title = {Applying {{Lee}}-{{Carter}} under Conditions of Variable Mortality Decline},
  author = {Booth, Heather and Maindonald, John and Smith, Len},
  year = {2002},
  month = jan,
  volume = {56},
  pages = {325--336},
  issn = {0032-4728, 1477-4747},
  doi = {10.1080/00324720215935},
  abstract = {The Lee-Carter method of mortality forecasting assumes an invariant age component and most applications have adopted a linear time component. The use of the method with Australian data is compromised by significant departures from linearity in the time component and changes over time in the age component. We modify the method to adjust the time component to reproduce the age distribution of deaths, rather than total deaths, and to determine the optimal fitting period in order to address non-linearity in the time component. In the Australian case the modification has the added advantage that the assumption of invariance is better met. For Australian data, the modifications result in higher forecast life expectancy than the original Lee-Carter method and official projections, and a 50 per cent reduction in forecast error. The model is also expanded to take account of age-time interactions by incorporating additional terms, but these are not readily incorporated into forecasts.},
  file = {/Users/salvatorykessy/Zotero/storage/VUH4M49S/Booth et al. - 2002 - Applying Lee-Carter under conditions of variable m.pdf},
  journal = {Population Studies},
  language = {en},
  number = {3}
}

@article{boothMORTALITYMODELLINGFORECASTING2008a,
  ids = {boothMORTALITYMODELLINGFORECASTING2008,boothMORTALITYMODELLINGFORECASTING2008b},
  title = {{{MORTALITY MODELLING AND FORECASTING}}: {{A REVIEW OF METHODS By H}}. {{Booth}} and {{L}}. {{Tickle}}},
  author = {Booth, H. and Tickle, L.},
  year = {2008},
  volume = {3},
  pages = {3--43},
  abstract = {Continuing increases in life expectancy beyond previously-held limits have brought to the fore the critical importance of mortality forecasting. Significant developments in mortality forecasting since 1980 are reviewed under three broad approaches: expectation, extrapolation and explanation. Expectation is not generally a good basis for mortality forecasting, as it is subjective; expert expectations are invariably conservative. Explanation is restricted to certain causes of death with known determinants. Decomposition by cause of death poses problems associated with the lack of independence among causes and data difficulties. Most developments have been in extrapolative forecasting, and make use of statistical methods rather than models developed primarily for age-specific graduation. Methods using two-factor models (age-period or age-cohort) have been most successful. The two-factor Lee\^Carter method, and, in particular, its variants, have been successful in terms of accuracy, while recent advances have improved the estimation of forecast uncertainty. Regression-based (GLM) methods have been less successful, due to nonlinearities in time. Three-factor methods are more recent; the Lee\^Carter age-period- cohort model appears promising. Specialised software has been developed and made available. Research needs include further comparative evaluations of methods in terms of the accuracy of the point forecast and its uncertainty, encompassing a wide range of mortality situations. keywords},
  file = {/Users/salvatorykessy/Zotero/storage/2XSCGEGU/Booth and Tickle - 2008 - MORTALITY MODELLING AND FORECASTING A REVIEW OF M.pdf},
  journal = {Annals of actuarial science},
  keywords = {Cause of Death,Cohort,contact,Decomposition,Extrapolation,Forecasting,GLM,Lee-Carter,Modelling,Mortality,p-splines,Software,Uncertainty},
  number = {I/II}
}

@article{brandongreenwellGeneralizedBoostedRegression2019,
  title = {Generalized {{Boosted Regression Models}}},
  author = {{Brandon Greenwell} and {Bradley Boehmke} and {Jay Cunningham}},
  year = {2019},
  abstract = {An implementation of extensions to Freund and Schapire's AdaBoost
algorithm and Friedman's gradient boosting machine. Includes regression
methods for least squares, absolute loss, t-distribution loss, quantile
regression, logistic, multinomial logistic, Poisson, Cox proportional hazards
partial likelihood, AdaBoost exponential loss, Huberized hinge loss, and
Learning to Rank measures (LambdaMart). Originally developed by Greg Ridgeway},
  file = {/Users/salvatorykessy/Zotero/storage/WECHGDEP/Brandon Greenwell, Bradley Boehmke, Jay Cunningham - 2019 - Generalized Boosted Regression Models.pdf}
}

@article{brandongreenwellGeneralizedBoostedRegression2019a,
  title = {Generalized {{Boosted Regression Models}}},
  author = {{Brandon Greenwell} and {Bradley Boehmke} and {Jay Cunningham}},
  year = {2019},
  abstract = {An implementation of extensions to Freund and Schapire's AdaBoost
algorithm and Friedman's gradient boosting machine. Includes regression
methods for least squares, absolute loss, t-distribution loss, quantile
regression, logistic, multinomial logistic, Poisson, Cox proportional hazards
partial likelihood, AdaBoost exponential loss, Huberized hinge loss, and
Learning to Rank measures (LambdaMart). Originally developed by Greg Ridgeway},
  file = {/Users/salvatorykessy/Zotero/storage/UFRNVYX5/Brandon Greenwell, Bradley Boehmke, Jay Cunningham - 2019 - Generalized Boosted Regression Models.pdf}
}

@article{breimanStackedRegressions2004,
  title = {Stacked Regressions},
  author = {Breiman, Leo},
  year = {2004},
  volume = {24},
  pages = {49--64},
  doi = {10.1007/bf00117832},
  abstract = {Stacking regressions is a method for forming linear combinations of different predictors to give improved prediction accuracy. The idea is to use cross-validation data and least squares under non-negativity constraints to determine the coefficients in the combination. Its effectiveness is demonstrated in stacking regression trees of different sizes add in a simulation stacking linear subset and ridge regressions. Reasons why this method works are explored. The idea of stacking originated with Wolpert (1992)},
  file = {/Users/salvatorykessy/Zotero/storage/L7EZKMRQ/Breiman - 2004 - Stacked regressions.pdf},
  journal = {Machine Learning},
  keywords = {combinations,non-negativity,stacking,subset regression,trees},
  number = {1}
}

@article{breimanStackedRegressions2004a,
  title = {Stacked Regressions},
  author = {Breiman, Leo},
  year = {2004},
  volume = {24},
  pages = {49--64},
  doi = {10.1007/bf00117832},
  abstract = {Stacking regressions is a method for forming linear combinations of different predictors to give improved prediction accuracy. The idea is to use cross-validation data and least squares under non-negativity constraints to determine the coefficients in the combination. Its effectiveness is demonstrated in stacking regression trees of different sizes add in a simulation stacking linear subset and ridge regressions. Reasons why this method works are explored. The idea of stacking originated with Wolpert (1992)},
  file = {/Users/salvatorykessy/Zotero/storage/I4QX47WE/Breiman - 2004 - Stacked regressions.pdf},
  journal = {Machine Learning},
  keywords = {combinations,non-negativity,stacking,subset regression,trees},
  number = {1}
}

@article{brouhnsPoissonLogbilinearRegression2002,
  title = {A {{Poisson}} Log-Bilinear Regression Approach to the Construction of Projected Lifetables},
  author = {Brouhns, Natacha and Denuit, Michel and Vermunt, Jeroen K.},
  year = {2002},
  month = dec,
  volume = {31},
  pages = {373--393},
  issn = {01676687},
  doi = {10.1016/S0167-6687(02)00185-3},
  abstract = {This paper implements Wilmoth's [Computational methods for fitting and extrapolating the Lee\textendash Carter model of mortality change, Technical report, Department of Demography, University of California, Berkeley] and Alho's [North American Actuarial Journal 4 (2000) 91] recommendation for improving the Lee\textendash Carter approach to the forecasting of demographic components. Specifically, the original method is embedded in a Poisson regression model, which is perfectly suited for age\textendash sex-specific mortality rates. This model is fitted for each sex to a set of age-specific Belgian death rates. A time-varying index of mortality is forecasted in an ARIMA framework. These forecasts are used to generate projected age-specific mortality rates, life expectancies and life annuities net single premiums. Finally, a Brass-type relational model is proposed to adapt the projections to the annuitants population, allowing for estimating the cost of adverse selection in the Belgian whole life annuity market.},
  file = {/Users/salvatorykessy/Zotero/storage/3CWA3W9U/Meta-learning.pdf;/Users/salvatorykessy/Zotero/storage/RPUDKSI2/Brouhns et al. - 2002 - A Poisson log-bilinear regression approach to the .pdf},
  journal = {Insurance: Mathematics and Economics},
  language = {en},
  number = {3}
}

@article{brouhnsPoissonLogbilinearRegression2002a,
  title = {A {{Poisson}} Log-Bilinear Regression Approach to the Construction of Projected Lifetables},
  author = {Brouhns, Natacha and Denuit, Michel and Vermunt, Jeroen K.},
  year = {2002},
  month = dec,
  volume = {31},
  pages = {373--393},
  issn = {01676687},
  doi = {10.1016/S0167-6687(02)00185-3},
  abstract = {This paper implements Wilmoth's [Computational methods for fitting and extrapolating the Lee\textendash Carter model of mortality change, Technical report, Department of Demography, University of California, Berkeley] and Alho's [North American Actuarial Journal 4 (2000) 91] recommendation for improving the Lee\textendash Carter approach to the forecasting of demographic components. Specifically, the original method is embedded in a Poisson regression model, which is perfectly suited for age\textendash sex-specific mortality rates. This model is fitted for each sex to a set of age-specific Belgian death rates. A time-varying index of mortality is forecasted in an ARIMA framework. These forecasts are used to generate projected age-specific mortality rates, life expectancies and life annuities net single premiums. Finally, a Brass-type relational model is proposed to adapt the projections to the annuitants population, allowing for estimating the cost of adverse selection in the Belgian whole life annuity market.},
  file = {/Users/salvatorykessy/Zotero/storage/NZ8TJ67B/Brouhns et al. - 2002 - A Poisson log-bilinear regression approach to the .pdf},
  journal = {Insurance: Mathematics and Economics},
  language = {en},
  number = {3}
}

@article{cairnsBayesianStochasticMortality2011,
  title = {Bayesian {{Stochastic Mortality Modelling}} for {{Two Populations}}},
  author = {Cairns, Andrew J G and Blake, David and Dowd, Kevin and Coughlan, Guy D and {Khalaf-Allah}, Marwa},
  year = {2011},
  pages = {35},
  abstract = {This paper introduces a new framework for modelling the joint development over time of mortality rates in a pair of related populations with the primary aim of producing consistent mortality forecasts for the two populations. The primary aim is achieved by combining a number of recent and novel developments in stochastic mortality modelling, but these, additionally, provide us with a number of side benefits and insights for stochastic mortality modelling.},
  file = {/Users/salvatorykessy/Zotero/storage/WK6CXQMT/Cairns et al. - Bayesian Stochastic Mortality Modelling for Two Po.pdf},
  language = {en}
}

@article{cairnsQuantitativeComparisonStochastic,
  title = {A Quantitative Comparison of Stochastic Mortality Models Using Data from {{England}} \& {{Wales}} and the {{United States}}},
  author = {Cairns, Andrew J G and Blake, David and Dowd, Kevin and Coughlan, Guy D and Epstein, David and Ong, Alen and Balevich, Igor},
  pages = {76},
  abstract = {We compare quantitatively eight stochastic models explaining improvements in mortality rates in England \& Wales and in the US. On the basis of the Bayes Information Criterion (BIC), we find that an extension of the Cairns, Blake \& Dowd (2006b) model that incorporates the cohort e\AE ect fits the England \& Wales data best, while for US data, the Renshaw \& Haberman (2006) extension to the Lee \& Carter (1992) model that also allows for a cohort e\AE ect provides the best fit. However, we identify problems with the robustness of parameter estimates of these models over di\AE erent time periods. A di\AE erent extension to the Cairns, Blake \& Dowd (2006b) model that allows not only for a cohort e\AE ect, but also for a quadratic age e\AE ect, while ranking below the other models in terms of the BIC, exhibits parameter stability across di\AE erent time periods for both data sets. This model also shows, for both data sets, that there have been approximately linear improvements over time in mortality rates at all ages, but that the improvements have been greater at lower ages than at higher ages, and that there are significant cohort e\AE ects.},
  file = {/Users/salvatorykessy/Zotero/storage/XS2XTFTA/Cairns et al. - A quantitative comparison of stochastic mortality .pdf},
  language = {en}
}

@article{cairnsQuantitativeComparisonStochastic2007,
  title = {A {{Quantitative Comparison}} of {{Stochastic Mortality Models Using Data}} from {{England}} \& {{Wales}} and the {{United States}}},
  author = {Cairns, Andrew J. G. and Blake, David P. and Dowd, Kevin and Coughlan, Guy and Epstein, David},
  year = {2007},
  issn = {1556-5068},
  doi = {10.2139/ssrn.1340389},
  abstract = {We compare quantitatively eight stochastic models explaining improvements in mortality rates in England \& Wales and in the US. On the basis of the Bayes Information Criterion (BIC), we find that an extension of the Cairns, Blake \& Dowd (2006b) model that incorporates the cohort e\AE ect fits the England \& Wales data best, while for US data, the Renshaw \& Haberman (2006) extension to the Lee \& Carter (1992) model that also allows for a cohort e\AE ect provides the best fit. However, we identify problems with the robustness of parameter estimates of these models over di\AE erent time periods. A di\AE erent extension to the Cairns, Blake \& Dowd (2006b) model that allows not only for a cohort e\AE ect, but also for a quadratic age e\AE ect, while ranking below the other models in terms of the BIC, exhibits parameter stability across di\AE erent time periods for both data sets. This model also shows, for both data sets, that there have been approximately linear improvements over time in mortality rates at all ages, but that the improvements have been greater at lower ages than at higher ages, and that there are significant cohort e\AE ects.},
  file = {/Users/salvatorykessy/Zotero/storage/DK4P3YMJ/Cairns et al. - 2007 - A Quantitative Comparison of Stochastic Mortality .pdf},
  journal = {SSRN Electronic Journal},
  language = {en}
}

@article{carracedoDetectingSpatiotemporalMortality2018,
  title = {Detecting Spatio-Temporal Mortality Clusters of {{European}} Countries by Sex and Age},
  author = {Carracedo, Patricia and Deb{\'o}n, Ana and Iftimi, Adina and Montes, Francisco},
  year = {2018},
  month = dec,
  volume = {17},
  pages = {38},
  issn = {1475-9276},
  doi = {10.1186/s12939-018-0750-z},
  abstract = {Background: Mortality decreased in European Union (EU) countries during the last century. Despite these similar trends, there are still considerable differences in the levels of mortality between Eastern and Western European countries. Sub-group analysis of mortality in Europe for different age and sex groups is common, however to our knowledge a spatio-temporal methodology as in this study has not been applied to detect significant spatial dependence and interaction with time. Thus, the objective of this paper is to quantify the dynamics of mortality in Europe and detect significant clusters of mortality between European countries, applying spatio-temporal methodology. In addition, the joint evolution between the mortality of European countries and their neighbours over time was studied.
Methods: The spatio-temporal methodology used in this study takes into account two factors: time and the geographical location of countries and, consequently, the neighbourhood relationships between them. This methodology was applied to 26 European countries for the period 1990-2012.
Results: Principally, for people older than 64 years two significant clusters were obtained: one of high mortality formed by Eastern European countries and the other of low mortality composed of Western countries. In contrast, for ages below or equal to 64 years only the significant cluster of high mortality formed by Eastern European countries was observed. In addition, the joint evolution between the 26 European countries and their neighbours during the period 1990-2012 was confirmed. For this reason, it can be said that mortality in EU not only depends on differences in the health systems, which are a subject to national discretion, but also on supra-national developments.
Conclusions: This paper proposes statistical tools which provide a clear framework for the successful implementation of development public policies to help the UE meet the challenge of rethinking its social model (Social Security and health care) and make it sustainable in the medium term.},
  file = {/Users/salvatorykessy/Zotero/storage/NKCXUUJL/Carracedo et al. - 2018 - Detecting spatio-temporal mortality clusters of Eu.pdf},
  journal = {International Journal for Equity in Health},
  language = {en},
  number = {1}
}

@article{caruana106028592005,
  title = {10.1.1.60.2859},
  author = {Caruana, Rich and Crew, Geoff},
  year = {2005},
  pages = {1--9},
  number = {1996}
}

@article{caruana106028592005a,
  title = {10.1.1.60.2859},
  author = {Caruana, Rich and Crew, Geoff},
  year = {2005},
  pages = {1--9},
  number = {1996}
}

@article{caruana106028592005b,
  title = {10.1.1.60.2859},
  author = {Caruana, Rich and Crew, Geoff},
  year = {2005},
  pages = {1--9},
  number = {1996}
}

@inproceedings{caruanaEnsembleSelectionLibraries2004,
  title = {Ensemble Selection from Libraries of Models},
  booktitle = {Twenty-First International Conference on {{Machine}} Learning  - {{ICML}} '04},
  author = {Caruana, Rich and {Niculescu-Mizil}, Alexandru and Crew, Geoff and Ksikes, Alex},
  year = {2004},
  pages = {18},
  publisher = {{ACM Press}},
  address = {{Banff, Alberta, Canada}},
  doi = {10.1145/1015330.1015432},
  abstract = {We present a method for constructing ensembles from libraries of thousands of models. Model libraries are generated using different learning algorithms and parameter settings. Forward stepwise selection is used to add to the ensemble the models that maximize its performance. Ensemble selection allows ensembles to be optimized to performance metric such as accuracy, cross entropy, mean precision, or ROC Area. Experiments with seven test problems and ten metrics demonstrate the benefit of ensemble selection.},
  file = {/Users/salvatorykessy/Zotero/storage/TUF9FR72/Caruana et al. - 2004 - Ensemble selection from libraries of models.pdf},
  language = {en}
}

@book{casellaSpringerTextsStatistics2006,
  title = {Springer {{Texts}} in {{Statistics}}},
  author = {Casella, George and Fienberg, Stephen and Olkin, Ingram},
  year = {2006},
  volume = {102},
  doi = {10.1016/j.peva.2007.06.006},
  abstract = {Review From the reviews: .,."There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006 "The reader sees not only how measure theory is used to develop probability theory, but also how probability theory is used in applications. a The discourse is delivered in a theorem proof format and thus is better suited for classroom a . The authors prose is generally well thought out a . will make an attractive choice for a two-semester course on measure and probability, or as a second course for students with a semester of measure or probability theory under their belt." (Peter C. Kiessler, Journal of the American Statistical Association, Vol. 102 (479), 2007) "The book is a well written self-contained textbook on measure and probability theory. It consists of 18 chapters. Every chapter contains many well chosen examples and ends with several problems related to the earlier developed theory (some with hints). a At the very end of the book there is an appendix collecting necessary facts from set theory, calculus and metric spaces. The authors suggest a few possibilities on how to use their book." (Kazimierz Musial, Zentralblatt MATH, Vol. 1125 (2), 2008) "The title of the book consists of the names of its two basic parts. The booka (TM)s third part is comprised of some special topics from probability theory. a The authors suggest using the book intwo-semester graduate programs in statistics or a one-semester seminar on special topics. The material of the book is standard a is clear, comprehensive and a \textasciitilde without being intimidatinga (TM)." (Rimas NorvaiAa, Mathematical Reviews, Issue 2007 f) Product Description This is a graduate level textbook on measure theory and probability theory. The book can be used as a text for a two semester sequence of courses in measure theory and probability theory, with an option to include supplemental material on stochastic processes and special topics. It is intended primarily for first year Ph.D. students in mathematics and statistics although mathematically advanced students from engineering and economics would also find the book useful. Prerequisites are kept to the minimal level of an understanding of basic real analysis concepts such as limits, continuity, differentiability, Riemann integration, and convergence of sequences and series. A review of this material is included in the appendix. The book starts with an informal introduction that provides some heuristics into the abstract concepts of measure and integration theory, which are then rigorously developed. The first part of the book can be used for a standard real analysis course for both mathematics and statistics Ph.D. students as it provides full coverage of topics such as the construction of Lebesgue-Stieltjes measures on real line and Euclidean spaces, the basic convergence theorems, L p spaces, signed measures, Radon-Nikodym theorem, Lebesgue's decomposition theorem and the fundamental theorem of Lebesgue integration on R, product spaces and product measures, and Fubini-Tonelli theorems. It also provides an elementary introduction to Banach and Hilbert spaces, convolutions, Fourier series and Fourier and Plancherel transforms. Thus part I would be particularly useful for students in a typical Statistics Ph.D. program if a separate course on real analysis is not a standard requirement. Part II (chapters 6-13) provides full coverage of standard graduate level probability theory. It starts with Kolmogorov's probability model and Kolmogorov's existence theorem. It then treats thoroughly the laws of large numbers including renewal theory and ergodic theorems with applications and then weak convergence of probability distributions, characteristic functions, the Levy-Cramer continuity theorem and the central limit theorem as well as stable laws. It ends with conditional expectations and conditional probability, and an introduction to the theory of discrete time martingales. Part III (chapters 14-18) provides a modest coverage of discrete time Markov chains with countable and general state spaces, MCMC, continuous time discrete space jump Markov processes, Brownian motion, mixing sequences, bootstrap methods, and branching processes. It could be used for a topics/seminar course or as an introduction to stochastic processes. From the reviews: "...There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006},
  file = {/Users/salvatorykessy/Zotero/storage/3BAZCADH/ChoiceofMeta.pdf;/Users/salvatorykessy/Zotero/storage/8RI3IFHY/Casella, Fienberg, Olkin - 2006 - Springer Texts in Statistics.pdf},
  isbn = {978-0-387-78188-4}
}

@book{casellaSpringerTextsStatistics2006a,
  title = {Springer {{Texts}} in {{Statistics}}},
  author = {Casella, George and Fienberg, Stephen and Olkin, Ingram},
  year = {2006},
  volume = {102},
  doi = {10.1016/j.peva.2007.06.006},
  abstract = {Review From the reviews: .,."There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006 "The reader sees not only how measure theory is used to develop probability theory, but also how probability theory is used in applications. a The discourse is delivered in a theorem proof format and thus is better suited for classroom a . The authors prose is generally well thought out a . will make an attractive choice for a two-semester course on measure and probability, or as a second course for students with a semester of measure or probability theory under their belt." (Peter C. Kiessler, Journal of the American Statistical Association, Vol. 102 (479), 2007) "The book is a well written self-contained textbook on measure and probability theory. It consists of 18 chapters. Every chapter contains many well chosen examples and ends with several problems related to the earlier developed theory (some with hints). a At the very end of the book there is an appendix collecting necessary facts from set theory, calculus and metric spaces. The authors suggest a few possibilities on how to use their book." (Kazimierz Musial, Zentralblatt MATH, Vol. 1125 (2), 2008) "The title of the book consists of the names of its two basic parts. The booka (TM)s third part is comprised of some special topics from probability theory. a The authors suggest using the book intwo-semester graduate programs in statistics or a one-semester seminar on special topics. The material of the book is standard a is clear, comprehensive and a \textasciitilde without being intimidatinga (TM)." (Rimas NorvaiAa, Mathematical Reviews, Issue 2007 f) Product Description This is a graduate level textbook on measure theory and probability theory. The book can be used as a text for a two semester sequence of courses in measure theory and probability theory, with an option to include supplemental material on stochastic processes and special topics. It is intended primarily for first year Ph.D. students in mathematics and statistics although mathematically advanced students from engineering and economics would also find the book useful. Prerequisites are kept to the minimal level of an understanding of basic real analysis concepts such as limits, continuity, differentiability, Riemann integration, and convergence of sequences and series. A review of this material is included in the appendix. The book starts with an informal introduction that provides some heuristics into the abstract concepts of measure and integration theory, which are then rigorously developed. The first part of the book can be used for a standard real analysis course for both mathematics and statistics Ph.D. students as it provides full coverage of topics such as the construction of Lebesgue-Stieltjes measures on real line and Euclidean spaces, the basic convergence theorems, L p spaces, signed measures, Radon-Nikodym theorem, Lebesgue's decomposition theorem and the fundamental theorem of Lebesgue integration on R, product spaces and product measures, and Fubini-Tonelli theorems. It also provides an elementary introduction to Banach and Hilbert spaces, convolutions, Fourier series and Fourier and Plancherel transforms. Thus part I would be particularly useful for students in a typical Statistics Ph.D. program if a separate course on real analysis is not a standard requirement. Part II (chapters 6-13) provides full coverage of standard graduate level probability theory. It starts with Kolmogorov's probability model and Kolmogorov's existence theorem. It then treats thoroughly the laws of large numbers including renewal theory and ergodic theorems with applications and then weak convergence of probability distributions, characteristic functions, the Levy-Cramer continuity theorem and the central limit theorem as well as stable laws. It ends with conditional expectations and conditional probability, and an introduction to the theory of discrete time martingales. Part III (chapters 14-18) provides a modest coverage of discrete time Markov chains with countable and general state spaces, MCMC, continuous time discrete space jump Markov processes, Brownian motion, mixing sequences, bootstrap methods, and branching processes. It could be used for a topics/seminar course or as an introduction to stochastic processes. From the reviews: "...There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006},
  file = {/Users/salvatorykessy/Zotero/storage/YVTF9Y9D/Casella, Fienberg, Olkin - 2006 - Springer Texts in Statistics.pdf},
  isbn = {978-0-387-78188-4}
}

@article{castellanosNoTitle2016,
  title = {No {{Title}}},
  author = {Castellanos, Jason Alfred},
  year = {2016},
  file = {/Users/salvatorykessy/Zotero/storage/UWUPA9TE/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.ris}
}

@article{castellanosNoTitle2016a,
  title = {No {{Title}}},
  author = {Castellanos, Jason Alfred},
  year = {2016},
  file = {/Users/salvatorykessy/Zotero/storage/S258A4A9/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf}
}

@article{castellanosNoTitle2016b,
  title = {No {{Title}}},
  author = {Castellanos, Jason Alfred},
  year = {2016}
}

@article{cesareForecastingMortalityDifferent2009,
  title = {Forecasting {{Mortality}}, {{Different Approaches}} for {{Different Cause}} of {{Deaths}}? {{The Cases}} of {{Lung Cancer}}; {{Influenza}}, {{Pneumonia}}, and {{Bronchitis}}; and {{Motor Vehicle Accidents}}},
  shorttitle = {Forecasting {{Mortality}}, {{Different Approaches}} for {{Different Cause}} of {{Deaths}}?},
  author = {Cesare, Mariachiara Di and Murphy, Mike},
  year = {2009},
  volume = {15},
  pages = {185--211},
  issn = {1357-3217, 2044-0456},
  doi = {10.1017/S1357321700005560},
  abstract = {Most of the methods of mortality forecasting have been assessed using performance on overall mortality, and few studies address the issue of identifying the appropriate forecasting models for specific causes of deaths. This study analyses trends and forecasts mortality rates for three major causes of death \"o lung cancer, influenza-pneumonia-bronchitis, and motor vehicle accidents \"o using Lee\^Carter, Booth\^Maindonald\^Smith, Age-Period-Cohort, and Bayesian models, to assess how far different causes of death need different forecasting methods. Using data from the Twentieth and Twenty-First Century Mortality databases for England and Wales, results show major differences among the different forecasting techniques. In particular, when linearity is the main driver of past trends, Lee\^Carter-based approaches are preferred due to their straightforward assumptions and limited need for subjective judgment. When a clear cohort pattern is detectable, such as with lung cancer, the Age-Period-Cohort model shows the best outcome. When complete and reliable historical trends are available the Bayesian model does not produce better results than the other models.},
  file = {/Users/salvatorykessy/Zotero/storage/IKES6BAB/Cesare and Murphy - 2009 - Forecasting Mortality, Different Approaches for Di.pdf},
  journal = {British Actuarial Journal},
  language = {en},
  number = {S1}
}

@article{clarkeComparingBayesModel2003,
  ids = {clarkeComparingBayesModel2003a,clarkeComparingBayesModel2003b,clarkeComparingBayesModel2004,clarkeComparingBayesModel2004a},
  title = {Comparing {{Bayes Model Averaging}} and {{Stacking When Model Approximation Error Cannot}} Be {{Ignored}}},
  author = {Clarke, Bertrand and Ca, Bertrand@stat Ubc},
  year = {2003},
  volume = {4},
  pages = {683--712},
  abstract = {We compare Bayes Model Averaging, BMA, to a non-Bayes form of model averaging called stacking. In stacking, the weights are no longer posterior probabilities of models; they are obtained by a technique based on cross-validation. When the correct data generating model (DGM) is on the list of models under consideration BMA is never worse than stacking and often is demonstrably better, provided that the noise level is of order commensurate with the coefficients and explanatory variables. Here, however, we focus on the case that the correct DGM is not on the model list and may not be well approximated by the elements on the model list. We give a sequence of computed examples by choosing model lists and DGM's to contrast the risk performance of stacking and BMA. In the first examples, the model lists are chosen to reflect geometric principles that should give good performance. In these cases, stacking typically outper-forms BMA, sometimes by a wide margin. In the second set of examples we examine how stacking and BMA perform when the model list includes all subsets of a set of potential predictors. When we standardize the size of terms and coefficients in this setting, we find that BMA outperforms stacking when the deviant terms in the DGM 'point' in directions accommodated by the model list but that when the deviant term points outside the model list stacking seems to do better. Overall, our results suggest the stacking has better robustness properties than BMA in the most important settings.},
  file = {/Users/salvatorykessy/Zotero/storage/2DCV5BJI/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/4LX4XCVG/Clarke and Ca - 2003 - Comparing Bayes Model Averaging and Stacking When .pdf;/Users/salvatorykessy/Zotero/storage/E3282DMT/Clarke - 2004 - Comparing bayes model averaging and stacking when model approximation error cannot be ignored.pdf;/Users/salvatorykessy/Zotero/storage/XIDDVLC3/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.ris},
  journal = {Journal of Machine Learning Research},
  keywords = {1,bayes model averaging,Bayes model averaging,consider the following toy,dgm,e,i,iid,introduction and an example,is a linear regression,key words,model selection,Model selection,model with outcomes y,problem,robustness,Robustness,stacking,Stacking,suppose the true model,that produced the data,the data generating model,we want to analyze}
}

@article{currieFittingGeneralizedLinear2016,
  title = {On Fitting Generalized Linear and Non-Linear Models of Mortality},
  author = {Currie, Iain D.},
  year = {2016},
  volume = {2016},
  pages = {356--383},
  doi = {10.1080/03461238.2014.928230},
  abstract = {Many common models of mortality can be expressed compactly in the language of either generalized linear models or generalized non-linear models. The R language provides a description of these models which parallels the usual algebraic definitions but has the advantage of a transparent and flexible model specification. We compare eight model structures for mortality. For each structure, we consider (a) the Poisson models for the force of mortality with both log and logit link functions and (b) the binomial models for the rate of mortality with logit and complementary log\textendash log link functions. Part of this work shows how to extend the usual smooth two-dimensional P-spline model for the force of mortality with Poisson error and log link to the other smooth two-dimensional P-spline models with Poisson and binomial errors defined in (a) and (b). Our comments are based on the results of fitting these models to data from six countries:Australia, France, Japan, Sweden, UK and USA. We also discuss the possibility of forecasting with these models; in particular, the introduction of cohort terms generally leads to an improvement in overall fit, but can also make forecasting with these models problematic.},
  file = {/Users/salvatorykessy/Zotero/storage/7TG7D9GC/Currie - 2016 - On fitting generalized linear and non-linear models of mortality.pdf},
  journal = {Scandinavian Actuarial Journal},
  keywords = {constraints,forecasting,generalized linear models,identifiability,mortality,R language},
  number = {4}
}

@article{currieFittingGeneralizedLinear2016a,
  title = {On Fitting Generalized Linear and Non-Linear Models of Mortality},
  author = {Currie, Iain D.},
  year = {2016},
  volume = {2016},
  pages = {356--383},
  doi = {10.1080/03461238.2014.928230},
  abstract = {Many common models of mortality can be expressed compactly in the language of either generalized linear models or generalized non-linear models. The R language provides a description of these models which parallels the usual algebraic definitions but has the advantage of a transparent and flexible model specification. We compare eight model structures for mortality. For each structure, we consider (a) the Poisson models for the force of mortality with both log and logit link functions and (b) the binomial models for the rate of mortality with logit and complementary log\textendash log link functions. Part of this work shows how to extend the usual smooth two-dimensional P-spline model for the force of mortality with Poisson error and log link to the other smooth two-dimensional P-spline models with Poisson and binomial errors defined in (a) and (b). Our comments are based on the results of fitting these models to data from six countries:Australia, France, Japan, Sweden, UK and USA. We also discuss the possibility of forecasting with these models; in particular, the introduction of cohort terms generally leads to an improvement in overall fit, but can also make forecasting with these models problematic.},
  file = {/Users/salvatorykessy/Zotero/storage/2B9ZFAYN/Currie - 2016 - On fitting generalized linear and non-linear models of mortality.pdf},
  journal = {Scandinavian Actuarial Journal},
  keywords = {constraints,forecasting,generalized linear models,identifiability,mortality,R language},
  number = {4}
}

@article{currieSmoothingForecastingMortality1947,
  title = {Smoothing and {{Forecasting Mortality Rates}} with {{P}}-Splines},
  author = {Currie, Iain},
  year = {1947},
  pages = {61},
  file = {/Users/salvatorykessy/Zotero/storage/7H5MRR4J/Currie - 1947 - Smoothing and Forecasting Mortality Rates with P-s.pdf},
  language = {en}
}

@article{currieSmoothingForecastingMortality2004,
  title = {Smoothing and Forecasting Mortality Rates},
  author = {Currie, Iain D and Durban, Maria and Eilers, Paul HC},
  year = {2004},
  month = dec,
  volume = {4},
  pages = {279--298},
  issn = {1471-082X, 1477-0342},
  doi = {10.1191/1471082X04st080oa},
  abstract = {The prediction of future mortality rates is a problem of fundamental importance for the insurance and pensions industry. We show how the method of P-splines can be extended to the smoothing and forecasting of two-dimensional mortality tables. We use a penalized generalized linear model with Poisson errors and show how to construct regression and penalty matrices appropriate for twodimensional modelling. An important feature of our method is that forecasting is a natural consequence of the smoothing process. We illustrate our methods with two data sets provided by the Continuous Mortality Investigation Bureau, a central body for the collection and processing of UK insurance and pensions data.},
  file = {/Users/salvatorykessy/Zotero/storage/3QBIWB6Q/Currie et al. - 2004 - Smoothing and forecasting mortality rates.pdf},
  journal = {Statistical Modelling: An International Journal},
  language = {en},
  number = {4}
}

@article{czadoBayesianPoissonLogbilinear2005,
  title = {Bayesian {{Poisson}} Log-Bilinear Mortality Projections},
  author = {Czado, Claudia and Delwarde, Antoine and Denuit, Michel},
  year = {2005},
  month = jun,
  volume = {36},
  pages = {260--284},
  issn = {01676687},
  doi = {10.1016/j.insmatheco.2005.01.001},
  abstract = {Mortality projections are major concerns for public policy, social security and private insurance. This paper implements a Bayesian log-bilinear Poisson regression model to forecast mortality. Computations are carried out using Markov Chain Monte Carlo methods in which the degree of smoothing is learnt from the data. Comparisons are made with the approach proposed by Brouhns, Denuit \& Vermunt (2002a,b), as well as with the original model of Lee \& Carter (1992).},
  file = {/Users/salvatorykessy/Zotero/storage/HK9UIUNY/Czado et al. - 2005 - Bayesian Poisson log-bilinear mortality projection.pdf},
  journal = {Insurance: Mathematics and Economics},
  language = {en},
  number = {3}
}

@article{demsarStatisticalComparisonsClassifiers2006,
  title = {Statistical {{Comparisons}} of {{Classifiers}} over {{Multiple Data Sets}}},
  author = {Demsar, Janez},
  year = {2006},
  pages = {30},
  abstract = {While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.},
  file = {/Users/salvatorykessy/Zotero/storage/DRLVZF2H/Demsˇar and Demsar - Statistical Comparisons of Classiﬁers over Multipl.pdf},
  language = {en}
}

@article{deprezMachineLearningTechniques2017,
  title = {Machine Learning Techniques for Mortality Modeling},
  author = {Deprez, Philippe and Shevchenko, Pavel V. and W{\"u}thrich, Mario V.},
  year = {2017},
  volume = {7},
  pages = {337--352},
  doi = {10.1007/s13385-017-0152-4},
  abstract = {Various stochastic models have been proposed to estimate mortality rates. In this paper we illustrate how machine learning techniques allow us to analyze the quality of such mortality models. In addition, we present how these techniques can be used for differentiating the different causes of death in mortality modeling.},
  file = {/Users/salvatorykessy/Zotero/storage/HZHR7EAV/Deprez, Shevchenko, Wüthrich - 2017 - Machine learning techniques for mortality modeling.pdf},
  journal = {European Actuarial Journal},
  keywords = {Boosting,Cause-of-death mortality,Machine learning,Mortality modeling,Regression},
  number = {2}
}

@article{deprezMachineLearningTechniques2017a,
  title = {Machine Learning Techniques for Mortality Modeling},
  author = {Deprez, Philippe and Shevchenko, Pavel V. and W{\"u}thrich, Mario V.},
  year = {2017},
  volume = {7},
  pages = {337--352},
  doi = {10.1007/s13385-017-0152-4},
  abstract = {Various stochastic models have been proposed to estimate mortality rates. In this paper we illustrate how machine learning techniques allow us to analyze the quality of such mortality models. In addition, we present how these techniques can be used for differentiating the different causes of death in mortality modeling.},
  file = {/Users/salvatorykessy/Zotero/storage/QKIJWR4Z/Deprez, Shevchenko, Wüthrich - 2017 - Machine learning techniques for mortality modeling.pdf},
  journal = {European Actuarial Journal},
  keywords = {Boosting,Cause-of-death mortality,Machine learning,Mortality modeling,Regression},
  number = {2}
}

@article{doumposModelCombinationCredit2007,
  title = {Model Combination for Credit Risk Assessment: {{A}} Stacked Generalization Approach},
  shorttitle = {Model Combination for Credit Risk Assessment},
  author = {Doumpos, Michael and Zopounidis, Constantin},
  year = {2007},
  month = feb,
  volume = {151},
  pages = {289--306},
  issn = {0254-5330, 1572-9338},
  doi = {10.1007/s10479-006-0120-x},
  abstract = {The development of credit risk assessment models is often considered within a classification context. Recent studies on the development of classification models have shown that a combination of methods often provides improved classification results compared to a single-method approach. Within this context, this study explores the combination of different classification methods in developing efficient models for credit risk assessment. A variety of methods are considered in the combination, including machine learning approaches and statistical techniques. The results illustrate that combined models can outperform individual models for credit risk analysis. The analysis also covers important issues such as the impact of using different parameters for the combined models, the effect of attribute selection, as well as the effects of combining strong or weak models.},
  file = {/Users/salvatorykessy/Zotero/storage/LJT5CBNL/Doumpos and Zopounidis - 2007 - Model combination for credit risk assessment A st.pdf},
  journal = {Annals of Operations Research},
  language = {en},
  number = {1}
}

@article{duttaMeasuringDiversityRegression2009,
  title = {Measuring Diversity in Regression Ensembles},
  author = {Dutta, H.},
  year = {2009},
  file = {/Users/salvatorykessy/Zotero/storage/LYP8ZNPR/Dutta - 2009 - Measuring diversity in regression ensembles.pdf},
  journal = {Indian International Conference on artificial Intelligence}
}

@article{duttaMeasuringDiversityRegression2009a,
  title = {Measuring Diversity in Regression Ensembles},
  author = {Dutta, H.},
  year = {2009},
  file = {/Users/salvatorykessy/Zotero/storage/QCYNER7Z/Dutta - 2009 - Measuring diversity in regression ensembles.pdf},
  journal = {Indian International Conference on artificial Intelligence}
}

@article{dzeroskiCombiningClassifiersBetter2002,
  title = {Is {{Combining Classifiers Better}} than {{Selecting}} the {{Best One}}?},
  author = {Dzeroski, Saso and Bernard, Zenko and Zenko, Bernard},
  year = {2002},
  volume = {54},
  pages = {255--273},
  issn = {1-55860-873-7},
  abstract = {We empirically evaluate several state-of-the- art methods for constructing ensembles of heterogeneous classifiers with stacking and show that they perform (at best) compara- bly to selecting the best classifier from the ensemble by cross validation. We then pro- pose a new method for stacking, that uses multi-response model trees at the meta-level, and show that it clearly outperforms existing stacking approaches and selecting the best classifier by cross validation.},
  journal = {Machine Learning},
  number = {3}
}

@article{eldardiryAcrossmodelCollectiveEnsemble2011,
  title = {Across-Model Collective Ensemble Classification},
  author = {Eldardiry, Hoda},
  year = {2011},
  pages = {343--349},
  issn = {9781577355083},
  abstract = {Ensemble classification methods that independently construct component models (e.g., bagging) improve accuracy over single models by reducing the error due to variance. Some work has been done to extend ensemble techniques for classification in relational domains by taking relational data characteristics or multiple link types into account during model construction. However, since these approaches follow the conventional approach to ensemble learning, they improve performance by reducing the error due to variance in learning. We note however, that variance in inference can be an additional source of error in relational methods that use collective classification, since inferred values are propagated during inference. We propose a novel ensemble mechanism for collective classification that reduces both learning and inference variance, by incorporating prediction averaging into the collective inference process itself. We show that our proposed method significantly outperforms a straightforward relational ensemble baseline on both synthetic and real-world datasets.},
  file = {/Users/salvatorykessy/Zotero/storage/8H7M76N4/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf},
  journal = {Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence},
  keywords = {Machine Learning}
}

@article{eldardiryAcrossmodelCollectiveEnsemble2011a,
  title = {Across-Model Collective Ensemble Classification},
  author = {Eldardiry, Hoda},
  year = {2011},
  pages = {343--349},
  abstract = {Ensemble classification methods that independently construct component models (e.g., bagging) improve accuracy over single models by reducing the error due to variance. Some work has been done to extend ensemble techniques for classification in relational domains by taking relational data characteristics or multiple link types into account during model construction. However, since these approaches follow the conventional approach to ensemble learning, they improve performance by reducing the error due to variance in learning. We note however, that variance in inference can be an additional source of error in relational methods that use collective classification, since inferred values are propagated during inference. We propose a novel ensemble mechanism for collective classification that reduces both learning and inference variance, by incorporating prediction averaging into the collective inference process itself. We show that our proposed method significantly outperforms a straightforward relational ensemble baseline on both synthetic and real-world datasets.},
  file = {/Users/salvatorykessy/Zotero/storage/F6PBL56B/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.ris;/Users/salvatorykessy/Zotero/storage/LMGE2PH8/Lopez de Prado - 2019.pdf},
  journal = {Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence},
  keywords = {Machine Learning}
}

@article{eldardiryAcrossmodelCollectiveEnsemble2011b,
  title = {Across-Model Collective Ensemble Classification},
  author = {Eldardiry, Hoda},
  year = {2011},
  pages = {343--349},
  issn = {9781577355083},
  abstract = {Ensemble classification methods that independently construct component models (e.g., bagging) improve accuracy over single models by reducing the error due to variance. Some work has been done to extend ensemble techniques for classification in relational domains by taking relational data characteristics or multiple link types into account during model construction. However, since these approaches follow the conventional approach to ensemble learning, they improve performance by reducing the error due to variance in learning. We note however, that variance in inference can be an additional source of error in relational methods that use collective classification, since inferred values are propagated during inference. We propose a novel ensemble mechanism for collective classification that reduces both learning and inference variance, by incorporating prediction averaging into the collective inference process itself. We show that our proposed method significantly outperforms a straightforward relational ensemble baseline on both synthetic and real-world datasets.},
  journal = {Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence},
  keywords = {Machine Learning}
}

@article{enchevMultipopulationMortalityModels2017,
  title = {Multi-Population Mortality Models: Fitting, Forecasting and Comparisons},
  author = {Enchev, Vasil and Kleinow, Torsten and Cairns, Andrew J G},
  year = {2017},
  volume = {2017},
  pages = {319--342},
  doi = {10.1080/03461238.2015.1133450},
  abstract = {We review a number of multi-population mortality models: variations of the Li \& Lee model, and the common-age-effect (CAE) model of Kleinow. Model parameters are estimated using maximum likelihood. Although this introduces some challenging identifiability problems and complicates the estimation process it allows a fair comparison of the different models. We propose to solve these identifiability problems by applying two- dimensional constraints over the parameters. Using data from six countries, we compare and rank, both visually and numerically, the models' fitting qualities and develop forecasting models that produce non-diverging, joint mortality rate scenarios. It is found that the CAE model fits best. But we also find that the Li and Lee model potentially suffers from robustness problems when calibrated using maximum likelihood.},
  file = {/Users/salvatorykessy/Zotero/storage/USIRG9J4/Enchev, Kleinow, Cairns - 2017 - Multi-population mortality models fitting, forecasting and comparisons(2).pdf},
  journal = {Scandinavian Actuarial Journal},
  keywords = {common age effect model,Li and Lee model,multi-population,Stochastic mortality model},
  number = {4}
}

@article{enchevMultipopulationMortalityModels2017a,
  title = {Multi-Population Mortality Models: Fitting, Forecasting and Comparisons},
  author = {Enchev, Vasil and Kleinow, Torsten and Cairns, Andrew J G},
  year = {2017},
  volume = {2017},
  pages = {319--342},
  doi = {10.1080/03461238.2015.1133450},
  abstract = {We review a number of multi-population mortality models: variations of the Li \& Lee model, and the common-age-effect (CAE) model of Kleinow. Model parameters are estimated using maximum likelihood. Although this introduces some challenging identifiability problems and complicates the estimation process it allows a fair comparison of the different models. We propose to solve these identifiability problems by applying two- dimensional constraints over the parameters. Using data from six countries, we compare and rank, both visually and numerically, the models' fitting qualities and develop forecasting models that produce non-diverging, joint mortality rate scenarios. It is found that the CAE model fits best. But we also find that the Li and Lee model potentially suffers from robustness problems when calibrated using maximum likelihood.},
  journal = {Scandinavian Actuarial Journal},
  keywords = {common age effect model,Li and Lee model,multi-population,Stochastic mortality model},
  number = {4}
}

@article{enchevMultipopulationMortalityModels2017b,
  title = {Multi-Population Mortality Models: Fitting, Forecasting and Comparisons},
  author = {Enchev, Vasil and Kleinow, Torsten and Cairns, Andrew J G},
  year = {2017},
  volume = {2017},
  pages = {319--342},
  doi = {10.1080/03461238.2015.1133450},
  abstract = {We review a number of multi-population mortality models: variations of the Li \& Lee model, and the common-age-effect (CAE) model of Kleinow. Model parameters are estimated using maximum likelihood. Although this introduces some challenging identifiability problems and complicates the estimation process it allows a fair comparison of the different models. We propose to solve these identifiability problems by applying two- dimensional constraints over the parameters. Using data from six countries, we compare and rank, both visually and numerically, the models' fitting qualities and develop forecasting models that produce non-diverging, joint mortality rate scenarios. It is found that the CAE model fits best. But we also find that the Li and Lee model potentially suffers from robustness problems when calibrated using maximum likelihood.},
  file = {/Users/salvatorykessy/Zotero/storage/LIFTJNWS/Enchev, Kleinow, Cairns - 2017 - Multi-population mortality models fitting, forecasting and comparisons(2).pdf;/Users/salvatorykessy/Zotero/storage/VD57GLP4/bayesiancairns.pdf},
  journal = {Scandinavian Actuarial Journal},
  keywords = {common age effect model,Li and Lee model,multi-population,Stochastic mortality model},
  number = {4}
}

@article{floydModelSelectionMetaLearning2011,
  title = {Model {{Selection Meta}}-{{Learning}} for the {{Prognosis}} of {{Pancreatic Cancer}}},
  author = {Floyd, Stuart and Ruiz, Carolina and Alvarez, Sergio A and Tseng, Jennifer and Whalen, Giles},
  year = {2011},
  pages = {29--37},
  doi = {10.5220/0002711000290037}
}

@article{floydModelSelectionMetaLearning2011a,
  title = {Model {{Selection Meta}}-{{Learning}} for the {{Prognosis}} of {{Pancreatic Cancer}}},
  author = {Floyd, Stuart and Ruiz, Carolina and Alvarez, Sergio A and Tseng, Jennifer and Whalen, Giles},
  year = {2011},
  pages = {29--37},
  doi = {10.5220/0002711000290037}
}

@article{floydModelSelectionMetaLearning2011b,
  title = {Model {{Selection Meta}}-{{Learning}} for the {{Prognosis}} of {{Pancreatic Cancer}}},
  author = {Floyd, Stuart and Ruiz, Carolina and Alvarez, Sergio A and Tseng, Jennifer and Whalen, Giles},
  year = {2011},
  pages = {29--37},
  doi = {10.5220/0002711000290037}
}

@book{forecastingNoTitle,
  title = {No {{Title}}},
  author = {Forecasting, Demographic},
  file = {/Users/salvatorykessy/Zotero/storage/MLW4TZ4Y/Forecasting - Unknown - No Title.pdf},
  isbn = {978-0-691-13094-1}
}

@article{forumLifeExpectancy2002,
  title = {Life {{Expectancy}}},
  author = {Forum, Policy},
  year = {2002},
  volume = {296},
  pages = {2001--2002},
  file = {/Users/salvatorykessy/Zotero/storage/PQ5GVBPK/Forum - 2002 - Life Expectancy.pdf},
  number = {May}
}

@article{gabrielliIndividualClaimsHistory2018,
  title = {An {{Individual Claims History Simulation Machine}}},
  author = {Gabrielli, Andrea and W{\"u}thrich, Mario V},
  year = {2018},
  doi = {10.3390/risks6020029},
  file = {/Users/salvatorykessy/Zotero/storage/FKDU4L9G/Gabrielli, Wüthrich - 2018 - An Individual Claims History Simulation Machine.pdf},
  keywords = {chain-ladder,claims cash flows,claims covariates,claims reserving,claims simulation,individual,individual claims,individual claims features,loss reserving,micro-level stochastic reserving,neural network reserving}
}

@article{gailleAgePatternsTrends2010,
  title = {Age {{Patterns}} and {{Trends}} in {{Mortality}} by {{Cause}} of {{Death}} and {{Implications}} for {{Modeling Longevity Risk}}},
  author = {Gaille, Severine and Sherris, Michael},
  year = {2010},
  issn = {1556-5068},
  doi = {10.2139/ssrn.1694272},
  file = {/Users/salvatorykessy/Zotero/storage/LSYPZCYF/Gaille and Sherris - 2010 - Age Patterns and Trends in Mortality by Cause of D.pdf},
  journal = {SSRN Electronic Journal},
  language = {en}
}

@article{gailleForecastingMortalityTrends,
  title = {Forecasting {{Mortality Trends}} Allowing for {{Cause}}-of-{{Death Mortality Dependence}}},
  author = {Gaille, Severine and Sherris, Michael},
  pages = {16},
  abstract = {Longevity risk is amongst the most important factors to consider for pricing and risk management of longevity products. Past improvements in mortality over many years, and the uncertainty of these improvements, have attracted the attention of experts, both practitioners and academics. Since aggregate mortality rates reflect underlying trends in causes of death, insurers and demographers are increasingly considering cause-of-death data to better understand risks in their mortality assumptions. The relative importance of causes of death has changed over many years. As one cause reduces, others increase or decrease. The dependence between mortality for different causes of death is important when projecting future mortality. However, for scenario analysis based on causes of death, the assumption usually made is that causes of death are independent. Recent models, in the form of Vector Error Correction Models (VECM), have been developed for multivariate dynamic systems and capture time dependency with common stochastic trends. These models include long-run stationary relations between the variables, and thus allow a better understanding of the nature of this dependence. This paper applies VECM to cause-of-death mortality rates in order to assess the dependence between these competing risks. We analyze the five main causes of death in Switzerland. Our analysis confirms the existence of a long-run stationary relationship between these five causes. This estimated relationship is then used to forecast mortality rates, which are shown to be an improvement over forecasts from more traditional ARIMA processes, that do not allow for cause-of-death dependencies.},
  file = {/Users/salvatorykessy/Zotero/storage/9WDJ9WM5/Gaille and Sherris - Forecasting Mortality Trends allowing for Cause-of.pdf},
  language = {en}
}

@article{gailleModellingMortalityCommon2011,
  title = {Modelling {{Mortality}} with {{Common Stochastic Long}}-{{Run Trends}}},
  author = {Gaille, S{\'e}verine and Sherris, Michael},
  year = {2011},
  month = oct,
  volume = {36},
  pages = {595--621},
  issn = {1018-5895, 1468-0440},
  doi = {10.1057/gpp.2011.19},
  file = {/Users/salvatorykessy/Zotero/storage/REXRFT5J/Gaille and Sherris - 2011 - Modelling Mortality with Common Stochastic Long-Ru.pdf},
  journal = {The Geneva Papers on Risk and Insurance - Issues and Practice},
  language = {en},
  number = {4}
}

@article{gailleModellingMortalityCommon2011a,
  title = {Modelling {{Mortality}} with {{Common Stochastic Long}}-{{Run Trends}}},
  author = {Gaille, S{\'e}verine and Sherris, Michael},
  year = {2011},
  month = oct,
  volume = {36},
  pages = {595--621},
  issn = {1018-5895, 1468-0440},
  doi = {10.1057/gpp.2011.19},
  journal = {The Geneva Papers on Risk and Insurance - Issues and Practice},
  language = {en},
  number = {4}
}

@article{ganValuationLargeVariable2015,
  title = {Valuation of Large Variable Annuity Portfolios under Nested Simulation: {{A}} Functional Data Approach},
  author = {Gan, Guojun and Lin, X Sheldon},
  year = {2015},
  pages = {13},
  abstract = {A variable annuity (VA) is equity-linked annuity product that has rapidly grown in popularity around the world in recent years. Research up to date on VA largely focuses on the valuation of guarantees embedded in a single VA contract. However, methods developed for individual VA contracts based on option pricing theory cannot be extended to large VA portfolios. Insurance companies currently use nested simulation to valuate guarantees for VA portfolios but efficient valuation under nested simulation for a large VA portfolio has been a real challenge. The computation in nested simulation is highly intensive and often prohibitive. In this paper, we propose a novel approach that combines a clustering technique with a functional data analysis technique to address the issue. We create a highly non-homogeneous synthetic VA portfolio of 100,000 contracts and use it to estimate the dollar Delta of the portfolio at each time step of outer loop scenarios under the nested simulation framework over a period of 25 years. Our test results show that the proposed approach performs well in terms of accuracy and efficiency.},
  file = {/Users/salvatorykessy/Zotero/storage/8AIC7J66/Gan and Lin - 2015 - Valuation of large variable annuity portfolios und.pdf},
  language = {en}
}

@inproceedings{gardnerStatisticalFrameworkPredictive2017,
  title = {A {{Statistical Framework}} for {{Predictive Model Evaluation}} in {{MOOCs}}},
  booktitle = {Proceedings of the {{Fourth}} (2017) {{ACM Conference}} on {{Learning}} @ {{Scale}} - {{L}}@{{S}} '17},
  author = {Gardner, Josh and Brooks, Christopher},
  year = {2017},
  pages = {269--272},
  publisher = {{ACM Press}},
  address = {{Cambridge, Massachusetts, USA}},
  doi = {10.1145/3051457.3054002},
  abstract = {Feature extraction and model selection are two essential processes when building predictive models of student success. In this work we describe and demonstrate a statistical approach to both tasks, comparing five modeling techniques (a lasso penalized logistic regression model, na\"ive Bayes, random forest, SVM, and classification tree) across three sets of features (week-only, summed, and appended, from [7]). We conduct this comparison on a dataset compiled from 30 total offerings of five different MOOCs run on the Coursera platform. Through the use of the Friedman test with a corresponding post-hoc Nemenyi test, we present comparative performance results for several classifiers across the three different feature extraction methods, demonstrating a rigorous inferential process intended to guide future analyses of student success systems.},
  file = {/Users/salvatorykessy/Zotero/storage/IF2G5KHK/Gardner and Brooks - 2017 - A Statistical Framework for Predictive Model Evalu.pdf},
  isbn = {978-1-4503-4450-0},
  language = {en}
}

@article{geometryNoZhuGuanDeJianKangGanwoZhongXintositaZaiZhaiGaoLingZheniokeruJianKangGuanLianZhiBiaoniGuansuruGongFenSanGouZaoFenXiTitle,
  title = {No 主観的健康感を中心とした在宅高齢者における {{健康関連指標に関する共分散構造分析Title}}},
  author = {Geometry, Riemannian and Analysis, Geometric},
  issn = {9783540773405},
  file = {/Users/salvatorykessy/Zotero/storage/IABUDRWV/Geometry, Analysis - Unknown - No 主観的健康感を中心とした在宅高齢者における 健康関連.pdf}
}

@article{geometryNoZhuGuanDeJianKangGanwoZhongXintositaZaiZhaiGaoLingZheniokeruJianKangGuanLianZhiBiaoniGuansuruGongFenSanGouZaoFenXiTitlea,
  title = {No 主観的健康感を中心とした在宅高齢者における {{健康関連指標に関する共分散構造分析Title}}},
  author = {Geometry, Riemannian and Analysis, Geometric},
  issn = {9783540773405},
  file = {/Users/salvatorykessy/Zotero/storage/EXN3EBXE/Geometry, Analysis - Unknown - No 主観的健康感を中心とした在宅高齢者における 健康関連.pdf}
}

@book{girosiDemographicForecasting2008,
  title = {Demographic Forecasting},
  author = {Girosi, Federico and King, Gary},
  year = {2008},
  publisher = {{Princeton University Press}},
  address = {{Princeton}},
  file = {/Users/salvatorykessy/Zotero/storage/IQG4DTVI/Girosi and King - 2008 - Demographic forecasting.pdf},
  isbn = {978-0-691-13094-1 978-0-691-13095-8},
  keywords = {Demography,Forecasting,Forecasting Methodology,methods,Models; Statistical,Mortality,Statistical methods},
  language = {en},
  lccn = {HB1321 .K56 2008},
  note = {OCLC: 213223859}
}

@article{grandgirardCostsSecondaryParasitism2002,
  title = {Costs of Secondary Parasitism in the Facultative Hyperparasitoid {{Pachycrepoideus}} Dubius: {{Does}} Host Size Matter?},
  author = {Grandgirard, Julie and Poinsot, Denis and Krespi, Liliane and N{\'e}non, Jean Pierre and Cortesero, Anne Marie},
  year = {2002},
  volume = {103},
  pages = {239--248},
  issn = {1010933404324},
  doi = {10.1023/A},
  abstract = {Although hyperparasitism frequently occur in parasitic insects, many aspects of this strategy remain unknown. We investigated possible fitness costs of hyperparasitism as influenced by host size. Our study was conducted with the facultative hyperparasitoid Pachycrepoideus dubius Ashmead (Hymenoptera: Pteromalidae), which parasitizes host species differing greatly in size. We compared some fitness traits (level of successful parasitism, development time, sex ratio and offspring size) of P. dubius developing on large secondary / primary (Delia radicum L. (Diptera: Anthomyiidae)/ Trybliographa rapae Westwood (Hymenoptera: Figitidae)) or small secondary / primary host species (Drosophila melanogaster L. / Asobara tabida Nees (Hymenoptera: Braconidae)). In no-choice and choice experiments, P. dubius was able to develop on different stages of T. rapae (L2 (endophagous), L4 (ectophagous), and pupae) but that it preferred to parasitize unparasitized D. radicum pupae over pupae parasitized by T. rapae. Furthermore, in P. dubius, hyperparasitism was associated with fitness costs (lower level of successful parasitism, smaller adult size) and these costs were greater on the smallest host complex. We hypothesize that the size of D. melanogaster pupae parasitized by A. tabida may be close to the suboptimal host size for P. dubius beneath which the costs of hyperparasitism make this strategy nonadaptive. Hyperparasitism in terms of trade-offs between host quality and abundance of competitors is discussed.},
  file = {/Users/salvatorykessy/Zotero/storage/DLV4Q7YH/Grandgirard et al. - 2002 - Costs of secondary parasitism in the facultative hyperparasitoid Pachycrepoideus dubius Does host size matte.pdf},
  journal = {Entomologia Experimentalis et Applicata},
  keywords = {Asobara tabida,Delia radicum,Drosophila melanogaster,Facultative hyperparasitism,Fitness,Host quality,Host size,Hymenoptera,Pachycrepoideus dubius,Pteromalidae,Trybliographa rapae},
  number = {3}
}

@article{grandgirardCostsSecondaryParasitism2002a,
  title = {Costs of Secondary Parasitism in the Facultative Hyperparasitoid {{Pachycrepoideus}} Dubius: {{Does}} Host Size Matter?},
  author = {Grandgirard, Julie and Poinsot, Denis and Krespi, Liliane and N{\'e}non, Jean Pierre and Cortesero, Anne Marie},
  year = {2002},
  volume = {103},
  pages = {239--248},
  issn = {1010933404324},
  doi = {10.1023/A},
  abstract = {Although hyperparasitism frequently occur in parasitic insects, many aspects of this strategy remain unknown. We investigated possible fitness costs of hyperparasitism as influenced by host size. Our study was conducted with the facultative hyperparasitoid Pachycrepoideus dubius Ashmead (Hymenoptera: Pteromalidae), which parasitizes host species differing greatly in size. We compared some fitness traits (level of successful parasitism, development time, sex ratio and offspring size) of P. dubius developing on large secondary / primary (Delia radicum L. (Diptera: Anthomyiidae)/ Trybliographa rapae Westwood (Hymenoptera: Figitidae)) or small secondary / primary host species (Drosophila melanogaster L. / Asobara tabida Nees (Hymenoptera: Braconidae)). In no-choice and choice experiments, P. dubius was able to develop on different stages of T. rapae (L2 (endophagous), L4 (ectophagous), and pupae) but that it preferred to parasitize unparasitized D. radicum pupae over pupae parasitized by T. rapae. Furthermore, in P. dubius, hyperparasitism was associated with fitness costs (lower level of successful parasitism, smaller adult size) and these costs were greater on the smallest host complex. We hypothesize that the size of D. melanogaster pupae parasitized by A. tabida may be close to the suboptimal host size for P. dubius beneath which the costs of hyperparasitism make this strategy nonadaptive. Hyperparasitism in terms of trade-offs between host quality and abundance of competitors is discussed.},
  file = {/Users/salvatorykessy/Zotero/storage/JDY29C72/Grandgirard et al. - 2002 - Costs of secondary parasitism in the facultative hyperparasitoid Pachycrepoideus dubius Does host size matte.pdf},
  journal = {Entomologia Experimentalis et Applicata},
  keywords = {Asobara tabida,Delia radicum,Drosophila melanogaster,Facultative hyperparasitism,Fitness,Host quality,Host size,Hymenoptera,Pachycrepoideus dubius,Pteromalidae,Trybliographa rapae},
  number = {3}
}

@article{greenwellGeneralizedBoostedRegression2019,
  title = {Generalized {{Boosted Regression Models}}},
  author = {Greenwell, Brandon and Boehmke, Bradley and Cunningham, Jay},
  year = {2019},
  abstract = {An implementation of extensions to Freund and Schapire's AdaBoost
algorithm and Friedman's gradient boosting machine. Includes regression
methods for least squares, absolute loss, t-distribution loss, quantile
regression, logistic, multinomial logistic, Poisson, Cox proportional hazards
partial likelihood, AdaBoost exponential loss, Huberized hinge loss, and
Learning to Rank measures (LambdaMart). Originally developed by Greg Ridgeway},
  file = {/Users/salvatorykessy/Zotero/storage/SULWQFXC/Brandon Greenwell, Bradley Boehmke, Jay Cunningham - 2019 - Generalized Boosted Regression Models.pdf}
}

@article{gunesStackedEnsembleModels2017,
  title = {Stacked {{Ensemble Models}} for {{Improved Prediction Accuracy}}},
  author = {Gunes, Funda and Wolfinger, Russ and Tan, Pei-Yi},
  year = {2017},
  pages = {1--19},
  abstract = {Ensemble modeling is now a well-established means for improving prediction accuracy; it enables you to average out noise from diverse models and thereby enhance the generalizable signal. Basic stacked ensemble techniques combine predictions from multiple machine learning algorithms and use these predictions as inputs to second-level learning models. This paper shows how you can generate a diverse set of models by various methods such as forest, gradient boosted decision trees, factorization machines, and logistic regression and then combine them with stacked-ensemble techniques such as hill climbing, gradient boosting, and nonnegative least squares in SAS \textregistered{} Visual Data Mining and Machine Learning. The application of these techniques to real-world big data problems demonstrates how using stacked ensembles produces greater prediction accuracy and robustness than do individual models. The approach is powerful and compelling enough to alter your initial data mining mindset from finding the single best model to finding a collection of really good complementary models. It does involve additional cost due both to training a large number of models and the proper use of cross validation to avoid overfitting. This paper shows how to efficiently handle this computational expense in a modern SAS \textregistered{} environment and how to manage an ensemble workflow by using parallel computation in a distributed framework.},
  file = {/Users/salvatorykessy/Zotero/storage/S82CCQSA/Gunes, Wolfinger, Tan - 2017 - Stacked Ensemble Models for Improved Prediction Accuracy.pdf},
  journal = {Sas}
}

@article{gunesStackedEnsembleModels2017a,
  title = {Stacked {{Ensemble Models}} for {{Improved Prediction Accuracy}}},
  author = {Gunes, Funda and Wolfinger, Russ and Tan, Pei-Yi},
  year = {2017},
  pages = {1--19},
  abstract = {Ensemble modeling is now a well-established means for improving prediction accuracy; it enables you to average out noise from diverse models and thereby enhance the generalizable signal. Basic stacked ensemble techniques combine predictions from multiple machine learning algorithms and use these predictions as inputs to second-level learning models. This paper shows how you can generate a diverse set of models by various methods such as forest, gradient boosted decision trees, factorization machines, and logistic regression and then combine them with stacked-ensemble techniques such as hill climbing, gradient boosting, and nonnegative least squares in SAS \textregistered{} Visual Data Mining and Machine Learning. The application of these techniques to real-world big data problems demonstrates how using stacked ensembles produces greater prediction accuracy and robustness than do individual models. The approach is powerful and compelling enough to alter your initial data mining mindset from finding the single best model to finding a collection of really good complementary models. It does involve additional cost due both to training a large number of models and the proper use of cross validation to avoid overfitting. This paper shows how to efficiently handle this computational expense in a modern SAS \textregistered{} environment and how to manage an ensemble workflow by using parallel computation in a distributed framework.},
  file = {/Users/salvatorykessy/Zotero/storage/V29CGMZP/Gunes, Wolfinger, Tan - 2017 - Stacked Ensemble Models for Improved Prediction Accuracy.pdf},
  journal = {Sas}
}

@article{h.akaikeNewLookStatistical1974,
  title = {A New Look at the Statistical Model Identification},
  author = {{H. Akaike}},
  year = {1974},
  month = dec,
  volume = {19},
  pages = {716--723},
  doi = {10.1109/TAC.1974.1100705},
  file = {/Users/salvatorykessy/Zotero/storage/JEVXFWUS/H. Akaike - 1974 - A new look at the statistical model identification.pdf},
  journal = {IEEE Transactions on Automatic Control},
  keywords = {Art,Estimation theory,History,Linear systems,Maximum likelihood estimation,maximum-likelihood (ML) estimation,Parameter identification,Roundoff errors,Sampling methods,Stochastic processes,Testing,Time series,Time series analysis},
  number = {6}
}

@article{hainautNeuralnetworkAnalyzerMortality2018,
  title = {A Neural-Network Analyzer for Mortality Forecast},
  author = {Hainaut, Donatien},
  year = {2018},
  volume = {48},
  pages = {481--508},
  doi = {10.1017/asb.2017.45},
  abstract = {This article proposes a neural-network approach to predict and simulate human mortality rates. This semi-parametric model is capable to detect and duplicate non-linearities observed in the evolution of log-forces of mortality. The method proceeds in two steps. During the first stage, a neural-network-based generalization of the principal component analysis summarizes the information carried by the surface of log-mortality rates in a small number of latent factors. In the second step, these latent factors are forecast with an econometric model. The term structure of log-forces of mortality is next reconstructed by an inverse transformation. The neural analyzer is adjusted to French, UK and US mortality rates, over the period 1946\textendash 2000 and validated with data from 2001 to 2014. Numerical experiments reveal that the neural approach has an excellent predictive power, compared to the Lee\textendash Carter model with and without cohort effects.},
  file = {/Users/salvatorykessy/Zotero/storage/PNXYNIHM/Hainaut - 2018 - A neural-network analyzer for mortality forecast.pdf;/Users/salvatorykessy/Zotero/storage/XXYSAY85/WhatToForecast(2).pdf},
  journal = {ASTIN Bulletin},
  keywords = {Lee-Carter,Longevity,Mortality,Neural network,Perceptron},
  number = {2}
}

@article{hansenDataMiningTime2002,
  title = {Data Mining of Time Series Using Stacked Generalizers},
  author = {Hansen, James V and Nelson, Ray D},
  year = {2002},
  month = mar,
  volume = {43},
  pages = {173--184},
  issn = {09252312},
  doi = {10.1016/S0925-2312(00)00364-7},
  abstract = {Data mining is the search for valuable information in large volumes of data. Finding patterns in time series databases is important to a variety of applications, including stock market trading and budget forecasting. This paper reports on an extension of neural network methods for planning and budgeting in the State of Utah. In particular, historical time series are analyzed using stacked generalization, a methodology devised to aid in developing models that generalize well to future time periods. Stacked generalization is compared to ARIMA and to standalone neural networks. The results are consistent and suggest promise for the stacked generalization method in other time series domains. 2002 Elsevier Science B.V. All rights reserved.},
  file = {/Users/salvatorykessy/Zotero/storage/V9YJZVTQ/Hansen and Nelson - 2002 - Data mining of time series using stacked generaliz.pdf},
  journal = {Neurocomputing},
  language = {en},
  number = {1-4}
}

@article{hatzopoulosDynamicParameterizationModeling2011,
  title = {A Dynamic Parameterization Modeling for the Age\textendash Period\textendash Cohort Mortality},
  author = {Hatzopoulos, P. and Haberman, S.},
  year = {2011},
  month = sep,
  volume = {49},
  pages = {155--174},
  issn = {01676687},
  doi = {10.1016/j.insmatheco.2011.02.007},
  abstract = {A new common mortality modeling structure is presented for analyzing mortality dynamics for a pool of countries, under the framework of generalized linear models (GLM). The countries are first classified by fuzzy c-means cluster analysis in order to construct the common sparse age-period model structure for the mortality experience. Next, we propose a method to create the common sex difference age-period model structure and then use this to produce the residual age-period model structure for each country and sex. The time related principal components are extrapolated using dynamic linear regression (DLR) models and coherent mortality forecasts are investigated. We make use of mortality data from the ``Human Mortality Database''.},
  file = {/Users/salvatorykessy/Zotero/storage/H4EGMLMY/Hatzopoulos and Haberman - 2011 - A dynamic parameterization modeling for the age–pe.pdf;/Users/salvatorykessy/Zotero/storage/RJVP5R9F/hybrid.pdf},
  journal = {Insurance: Mathematics and Economics},
  language = {en},
  number = {2}
}

@article{hiltonProjectingUKMortality2019,
  title = {Projecting {{UK}} Mortality by Using {{Bayesian}} Generalized Additive Models},
  author = {Hilton, Jason and Dodd, Erengul and Forster, Jonathan J. and Smith, Peter W. F.},
  year = {2019},
  month = jan,
  volume = {68},
  pages = {29--49},
  issn = {0035-9254, 1467-9876},
  doi = {10.1111/rssc.12299},
  abstract = {Forecasts of mortality provide vital information about future populations, with implications for pension and healthcare policy as well as for decisions made by private companies about life insurance and annuity pricing. The paper presents a Bayesian approach to the forecasting of mortality that jointly estimates a generalized additive model (GAM) for mortality for the majority of the age range and a parametric model for older ages where the data are sparser. The GAM allows smooth components to be estimated for age, cohort and age-specific improvement rates, together with a non-smoothed period effect. Forecasts for the UK are produced by using data from the human mortality database spanning the period 1961\textendash 2013. A metric that approximates predictive accuracy is used to estimate weights for the `stacking' of forecasts from models with different points of transition between the GAM and parametric elements. Mortality for males and females is estimated separately at first, but a joint model allows the asymptotic limit of mortality at old ages to be shared between sexes and furthermore provides for forecasts accounting for correlations in period innovations.},
  file = {/Users/salvatorykessy/Zotero/storage/527WIPYZ/Hilton et al. - 2019 - Projecting UK mortality by using Bayesian generali.pdf;/Users/salvatorykessy/Zotero/storage/Q9T5QXFY/sheris3.pdf},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  language = {en},
  number = {1}
}

@article{hiltonProjectingUKMortality2019a,
  title = {Projecting {{UK}} Mortality by Using {{Bayesian}} Generalized Additive Models},
  author = {Hilton, Jason and Dodd, Erengul and Forster, Jonathan J. and Smith, Peter W.F.},
  year = {2019},
  volume = {68},
  pages = {29--49},
  doi = {10.1111/rssc.12299},
  abstract = {Forecasts of mortality provide vital information about future populations, with implications for pension and health-care policy as well as for decisions made by private companies about life insurance and annuity pricing. Stochastic mortality forecasts allow the uncertainty in mortality predictions to be taken into consideration when making policy decisions and setting product prices. Longer lifespans imply that forecasts of mortality at ages 90 and above will become more important in such calculations. This paper presents a Bayesian approach to the forecasting of mortality that jointly estimates a Generalised Additive Model (GAM) for mortality for the majority of the age-range and a parametric model for older ages where the data are sparser. The GAM allows smooth components to be estimated for age, cohort and age-specific improvement rates, together with a non-smoothed period effect. Forecasts for the United Kingdom are produced using data from the Human Mortality Database spanning the period 1961-2013. A metric that approximates predictive accuracy under Leave-One-Out cross-validation is used to estimate weights for the `stacking' of forecasts with different points of transition between the GAM and parametric elements. Mortality for males and females are estimated separately at first, but a joint model allows the asymptotic limit of mortality at old ages to be shared between sexes, and furthermore provides for forecasts accounting for correlations in period innovations. The joint and single sex model forecasts estimated using data from 1961-2003 are compared against observed data from 2004-2013 to facilitate model assessment.},
  file = {/Users/salvatorykessy/Zotero/storage/WC9MEACX/Hilton et al. - 2019 - Projecting UK mortality by using Bayesian generalized additive models.pdf},
  journal = {Journal of the Royal Statistical Society. Series C: Applied Statistics},
  keywords = {Age–period–cohort,Bayesian analysis,Forecasting; Generalized additive models,Mortality},
  number = {1}
}

@article{hiltonProjectingUKMortality2019b,
  title = {Projecting {{UK}} Mortality by Using {{Bayesian}} Generalized Additive Models},
  author = {Hilton, Jason and Dodd, Erengul and Forster, Jonathan J. and Smith, Peter W.F.},
  year = {2019},
  volume = {68},
  pages = {29--49},
  doi = {10.1111/rssc.12299},
  abstract = {Forecasts of mortality provide vital information about future populations, with implications for pension and health-care policy as well as for decisions made by private companies about life insurance and annuity pricing. Stochastic mortality forecasts allow the uncertainty in mortality predictions to be taken into consideration when making policy decisions and setting product prices. Longer lifespans imply that forecasts of mortality at ages 90 and above will become more important in such calculations. This paper presents a Bayesian approach to the forecasting of mortality that jointly estimates a Generalised Additive Model (GAM) for mortality for the majority of the age-range and a parametric model for older ages where the data are sparser. The GAM allows smooth components to be estimated for age, cohort and age-specific improvement rates, together with a non-smoothed period effect. Forecasts for the United Kingdom are produced using data from the Human Mortality Database spanning the period 1961-2013. A metric that approximates predictive accuracy under Leave-One-Out cross-validation is used to estimate weights for the `stacking' of forecasts with different points of transition between the GAM and parametric elements. Mortality for males and females are estimated separately at first, but a joint model allows the asymptotic limit of mortality at old ages to be shared between sexes, and furthermore provides for forecasts accounting for correlations in period innovations. The joint and single sex model forecasts estimated using data from 1961-2003 are compared against observed data from 2004-2013 to facilitate model assessment.},
  file = {/Users/salvatorykessy/Zotero/storage/BKBJW8XR/Hilton et al. - 2019 - Projecting UK mortality by using Bayesian generalized additive models.pdf},
  journal = {Journal of the Royal Statistical Society. Series C: Applied Statistics},
  keywords = {Age–period–cohort,Bayesian analysis,Forecasting; Generalized additive models,Mortality},
  number = {1}
}

@article{hochreiterLstm1997,
  title = {Lstm},
  author = {Hochreiter, Sepp and Urgen Schmidhuber, Jj},
  year = {1997},
  volume = {9},
  pages = {1735--1780},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called \textbackslash Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error through \textbackslash constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
  file = {/Users/salvatorykessy/Zotero/storage/BZCZKEQL/Hochreiter, Urgen Schmidhuber - 1997 - Lstm.pdf},
  journal = {Neural Computation},
  number = {8}
}

@article{hochreiterLstm1997a,
  title = {Lstm},
  author = {Hochreiter, Sepp and Urgen Schmidhuber, Jj},
  year = {1997},
  volume = {9},
  pages = {1735--1780},
  issn = {08997667 (ISSN)},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called \textbackslash Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error through \textbackslash constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
  file = {/Users/salvatorykessy/Zotero/storage/Q2GISNHP/Hochreiter, Urgen Schmidhuber - 1997 - Lstm.pdf},
  journal = {Neural Computation},
  number = {8}
}

@article{hochreiterLstm1997b,
  title = {Lstm},
  author = {Hochreiter, Sepp and Urgen Schmidhuber, Jj},
  year = {1997},
  volume = {9},
  pages = {1735--1780},
  issn = {08997667 (ISSN)},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called \textbackslash Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error through \textbackslash constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
  file = {/Users/salvatorykessy/Zotero/storage/69FCDDWP/Hochreiter, Urgen Schmidhuber - 1997 - Lstm.pdf},
  journal = {Neural Computation},
  number = {8}
}

@article{hyndmanCoherentMortalityForecasting2013,
  title = {Coherent {{Mortality Forecasting}}: {{The Product}}-{{Ratio Method With Functional Time Series Models}}},
  shorttitle = {Coherent {{Mortality Forecasting}}},
  author = {Hyndman, Rob J. and Booth, Heather and Yasmeen, Farah},
  year = {2013},
  month = feb,
  volume = {50},
  pages = {261--283},
  issn = {0070-3370, 1533-7790},
  doi = {10.1007/s13524-012-0145-5},
  abstract = {When independence is assumed, forecasts of mortality for subpopulations are almost always divergent in the long term. We propose a method for coherent forecasting of mortality rates for two or more subpopulations, based on functional principal components models of simple and interpretable functions of rates. The product-ratio functional forecasting method models and forecasts the geometric mean of subpopulation rates and the ratio of subpopulation rates to product rates. Coherence is imposed by constraining the forecast ratio function through stationary time series models. The method is applied to sex specific data for Sweden and state-specific data for Australia. Based on out-of-sample forecasts, the coherent forecasts are at least as accurate in overall terms as comparable independent forecasts, and forecast accuracy is homogenized across subpopulations.},
  file = {/Users/salvatorykessy/Zotero/storage/9QVH6E9U/Hyndman et al. - 2013 - Coherent Mortality Forecasting The Product-Ratio .pdf},
  journal = {Demography},
  language = {en},
  number = {1}
}

@article{hyndmanFunctionalTimeSeries2013,
  title = {Functional Time Series Forecasting},
  author = {Hyndman, Rob J and Shang, Han Lin},
  year = {2013},
  keywords = {2000 msc,62g08,62g09,62h25,62j07,62p05,corresponding author,demographic forecasting,functional data,functional partial least squares,functional principal components,functional time series,primary,secondary}
}

@article{hyndmanRobustForecastingMortality2007,
  title = {Robust Forecasting of Mortality and Fertility Rates: {{A}} Functional Data Approach},
  shorttitle = {Robust Forecasting of Mortality and Fertility Rates},
  author = {Hyndman, Rob J. and Shahid Ullah, Md.},
  year = {2007},
  month = jun,
  volume = {51},
  pages = {4942--4956},
  issn = {01679473},
  doi = {10.1016/j.csda.2006.07.028},
  file = {/Users/salvatorykessy/Zotero/storage/MTQTZGEX/Hyndman and Shahid Ullah - 2007 - Robust forecasting of mortality and fertility rate.pdf},
  journal = {Computational Statistics \& Data Analysis},
  language = {en},
  number = {10}
}

@book{jamesIntroductionStatisticalLearning2013,
  title = {An Introduction to Statistical Learning: With Applications in {{R}}},
  shorttitle = {An Introduction to Statistical Learning},
  editor = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2013},
  publisher = {{Springer}},
  address = {{New York}},
  file = {/Users/salvatorykessy/Zotero/storage/UB4G2TH9/James et al. - 2013 - An introduction to statistical learning with appl.pdf},
  isbn = {978-1-4614-7137-0},
  keywords = {Mathematical models,Mathematical statistics,Problems; exercises; etc,R (Computer program language),Statistics},
  language = {en},
  lccn = {QA276 .I585 2013},
  note = {OCLC: ocn828488009},
  number = {103},
  series = {Springer Texts in Statistics}
}

@article{janssenAdvancesMortalityForecasting2018,
  title = {Advances in Mortality Forecasting: Introduction},
  author = {Janssen, Fanny},
  year = {2018},
  volume = {74},
  issn = {4111801800457},
  doi = {10.1186/s41118-018-0045-7},
  file = {/Users/salvatorykessy/Zotero/storage/RFMJUX6X/Janssen - 2018 - Advances in mortality forecasting introduction.pdf},
  journal = {Genus},
  number = {1}
}

@article{janssenAdvancesMortalityForecasting2018a,
  title = {Advances in Mortality Forecasting: Introduction},
  author = {Janssen, Fanny},
  year = {2018},
  volume = {74},
  issn = {4111801800457},
  doi = {10.1186/s41118-018-0045-7},
  file = {/Users/salvatorykessy/Zotero/storage/XU6BDAUL/Janssen - 2018 - Advances in mortality forecasting introduction.pdf},
  journal = {Genus},
  number = {1}
}

@article{janssenChoiceTrendsBasis2007,
  title = {The Choice among Past Trends as a Basis for the Prediction of Future Trends in Old-Age Mortality},
  author = {Janssen, Fanny and Kunst, Anton},
  year = {2007},
  month = nov,
  volume = {61},
  pages = {315--326},
  issn = {0032-4728, 1477-4747},
  doi = {10.1080/00324720701571632},
  file = {/Users/salvatorykessy/Zotero/storage/DNFKVU7M/Janssen and Kunst - 2007 - The choice among past trends as a basis for the pr.pdf},
  journal = {Population Studies},
  language = {en},
  number = {3}
}

@article{janssenDEMOGRAPHICRESEARCHVOLUME2013,
  title = {{{DEMOGRAPHIC RESEARCH VOLUME}} 29 , {{ARTICLE}} 13 , {{PAGES}} 323-354 {{Impact}} of Different Mortality Forecasting Methods and Explicit Assumptions on Projected Future Life Expectancy : {{The}} Case of the {{Netherlands Lenny Stoeldraijer Coen}} van {{Duin Leo}} van {{Wissen Table}} of {{Contents}}},
  author = {Janssen, Fanny},
  year = {2013},
  volume = {29},
  pages = {323--354},
  doi = {10.4054/DemRes.2013.29.13},
  file = {/Users/salvatorykessy/Zotero/storage/GK4HHW2E/Janssen - 2013 - DEMOGRAPHIC RESEARCH VOLUME 29 , ARTICLE 13 , PAGES 323-354 Impact of different mortality forecasting methods and expli.pdf},
  number = {August}
}

@article{janssenIncludingSmokingEpidemic2013,
  title = {Including the {{Smoking Epidemic}} in {{Internationally Coherent Mortality Projections}}},
  author = {Janssen, Fanny and {van Wissen}, Leo J. G. and Kunst, Anton E.},
  year = {2013},
  month = aug,
  volume = {50},
  pages = {1341--1362},
  issn = {0070-3370, 1533-7790},
  doi = {10.1007/s13524-012-0185-x},
  abstract = {We present a new mortality projection methodology that distinguishes smoking- and non-smoking-related mortality and takes into account mortality trends of the opposite sex and in other countries. We evaluate to what extent future projections of life expectancy at birth (e0) for the Netherlands up to 2040 are affected by the application of these components. All-cause mortality and non-smoking-related mortality for the years 1970-2006 are projected by the Lee-Carter and Li-Lee methodologies. Smoking-related mortality is projected according to assumptions on future smokingattributable mortality. Projecting all-cause mortality in the Netherlands, using the LeeCarter model, leads to high gains in e0 (4.1 for males; 4.4 for females) and divergence between the sexes. Coherent projections, which include the mortality experience of the other 2 1 sex- and country-specific populations, result in much higher gains for males (6.4) and females (5.7), and convergence. The separate projection of smoking and nonsmoking-related mortality produces a steady increase in e0 for males (4.8) and a nonlinear trend for females, with lower gains in e0 in the short run, resulting in temporary sex convergence. The latter effect is also found in coherent projections. Our methodology provides more robust projections, especially thanks to the distinction between smoking- and non-smoking-related mortality.},
  file = {/Users/salvatorykessy/Zotero/storage/VP75DMKT/Janssen et al. - 2013 - Including the Smoking Epidemic in Internationally .pdf},
  journal = {Demography},
  language = {en},
  number = {4}
}

@article{johnson32Stacking2014,
  title = {32.{{Stacking}}},
  author = {Johnson, Reid and Aguiar, Everaldo},
  year = {2014},
  file = {/Users/salvatorykessy/Zotero/storage/YCR2K335/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf}
}

@article{johnson32Stacking2014a,
  title = {32.{{Stacking}}},
  author = {Johnson, Reid and Aguiar, Everaldo},
  year = {2014},
  file = {/Users/salvatorykessy/Zotero/storage/B4N7C2AK/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.ris}
}

@article{johnson32Stacking2014b,
  title = {32.{{Stacking}}},
  author = {Johnson, Reid and Aguiar, Everaldo},
  year = {2014},
  file = {/Users/salvatorykessy/Zotero/storage/BQBSJBGS/Johnson and Aguiar - 2014 - 32.Stacking.pdf}
}

@article{johnsonComparingBayesModel2018,
  ids = {johnsonComparingBayesModel2018a,rajaniStackingAuxiliaryFeatures2018,rajaniStackingAuxiliaryFeatures2018a,rajaniStackingAuxiliaryFeatures2018b},
  title = {Comparing {{Bayes Model Averaging}} and {{Stacking When Model Approximation Error Cannot}} Be {{Ignored}}},
  author = {Johnson, Reid and Aguiar, Everaldo and Eldardiry, Hoda and June, E T H Zurich and Hg, Rooms and Zhang, Yi and Clarke, Bertrand and Ca, Bertrand@stat Ubc and Kansara, Dhvani and Singh, Rashika and Sanghvi, Deep and Kanani, Pratik and Engineering, Information Technology and Rajani, Nazneen Fatema and Mooney, Raymond and Palaniyammal, V and Naimi, Ashley I and Street, Desoto and Caruana, Rich and Crew, Geoff and Ling, Xiaoliang and Gu, Chen and Ju, Cheng and Laan, Mark J Van Der and Lee, Kun Chang and Cho, Heeryon and {Magnuson-skeels}, Bonnie Rose and Grantham, Theodore E and Rokach, Lior and Rose, Sherri and Einstein, Albert and Chambers, Brendan and Levy, Maayan and Dechery, Joseph B and Maclean, Jason N and Sharma, Shubham and Kumar, Sandeep and Castellanos, Jason Alfred and Brown, Gavin and Wyatt, Jeremy L and Lin, Hsuan-tien and Mirza, Bilal and Wang, Wei and Wang, Jie and Choi, Howard and Chung, Neo Christopher and Floyd, Stuart and Ruiz, Carolina and Alvarez, Sergio A and Tseng, Jennifer and Whalen, Giles and Scharth, Marcel and Model, Learning and Rose, Sherri and Ting, Kai Ming and Witten, Ian H},
  year = {2018},
  volume = {4},
  pages = {343--349},
  issn = {9781577355083},
  doi = {10.18653/v1/n18-1201},
  abstract = {Visual Question Answering (VQA) is a well-known and challenging task that requires systems to jointly reason about natural language and vision. Deep learning models in various forms have been the standard for solving VQA. However, some of these VQA models are better at certain types of image-question pairs than other models. Ensembling VQA models intelligently to leverage their diverse expertise is, therefore, advantageous. Stacking With Auxiliary Features (SWAF) is an intelligent ensembling technique which learns to combine the results of multiple models using features of the current problem as context. We propose four categories of auxiliary features for ensembling for VQA. Three out of the four categories of features can be inferred from an image-question pair and do not require querying the component models. The fourth category of auxiliary features uses model-specific explanations. In this paper, we describe how we use these various categories of auxiliary features to improve performance for VQA. Using SWAF to effectively ensemble three recent systems, we obtain a new state-of-the-art. Our work also highlights the advantages of explainable AI models.},
  file = {/Users/salvatorykessy/Zotero/storage/39HNFS2C/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/3SQ3ZHQ8/Lin, Li - 2008 - Support Vector Machinery for Infinite Ensemble Learning.pdf;/Users/salvatorykessy/Zotero/storage/4X6VSIRU/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/5BIPP823/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.ris;/Users/salvatorykessy/Zotero/storage/5Q3EVDQH/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/6Y2I4P9K/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/75CKPKXA/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/7M8D6VXG/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/93L2P4RT/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/AKQL6LEJ/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.ris;/Users/salvatorykessy/Zotero/storage/DT9CCJPN/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/DWBT83DK/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/EFJ8HM6X/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/GJ4R7YZU/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/IN97GM4N/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/TGEGET9R/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/U8CZWXZ5/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/UT4YZ9LX/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/X33E34R2/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/XZZLTVXF/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/YPIPF5DP/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/ZQZ7W5H2/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf},
  journal = {Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence},
  keywords = {1,5,bayes model averaging,boosting,c4,cart,class imbalance,classifier,classifiers bagging,click prediction,consider the following toy,context prediction,correlation learning,curse of dimensionality,data,data integration,data mining weka ensemble,dgm,diversity,dnn,e,ensemble,ensemble learning,Ensemble learning,ensemble methods,gbdt,gbn markov blanket-assisted ensemble,general bayesian,hessian matrix,heterogeneous,heterogeneous models,i,id3,iid,information theory,Information theory,introduction and an example,is a linear regression,kernel,key words,location prediction,machine learning,Machine Learning,Machine Learning: Ensemble Methods,missing data,model ensemble,model selection,model with outcomes y,multi-omics,Natural Language Processing: Information Extractio,negative,network,network analysis,Network analysis,network motifs,Network motifs,neural networks,oil consumption,problem,regression estimators,robustness,scalability,simulation and modeling,Simulation and modeling,smle,stacking,stacking wbcd set,support vector machine,suppose the true model,svm,synaptic connectivity,Synaptic connectivity,that produced the data,the data generating model,time series forecasting,we want to analyze},
  number = {1021}
}

@article{juneKeynoteTalks2019,
  title = {Keynote {{Talks}} :},
  author = {June, E T H Zurich and Hg, Rooms},
  year = {2019},
  file = {/Users/salvatorykessy/Zotero/storage/DETJP4N5/June, Hg - 2019 - Keynote Talks.pdf;/Users/salvatorykessy/Zotero/storage/PYW9GQRY/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.ris}
}

@article{juneKeynoteTalks2019a,
  title = {Keynote {{Talks}} :},
  author = {June, E T H Zurich and Hg, Rooms},
  year = {2019},
  file = {/Users/salvatorykessy/Zotero/storage/HJUC7XHA/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/PGL3VKGU/June, Hg - 2019 - Keynote Talks.pdf}
}

@article{juneKeynoteTalks2019b,
  title = {Keynote {{Talks}} :},
  author = {June, E T H Zurich and Hg, Rooms},
  year = {2019},
  file = {/Users/salvatorykessy/Zotero/storage/EA5J8VX7/June, Hg - 2019 - Keynote Talks.pdf}
}

@article{juRelativePerformanceEnsemble2018,
  title = {The Relative Performance of Ensemble Methods with Deep Convolutional Neural Networks for Image Classification},
  author = {Ju, Cheng and Bibaut, Aur{\'e}lien and {van der Laan}, Mark},
  year = {2018},
  volume = {45},
  pages = {2800--2818},
  doi = {10.1080/02664763.2018.1441383},
  abstract = {Artificial neural networks have been successfully applied to a variety of machine learning tasks, including image recognition, semantic segmentation, and machine translation. However, few studies fully investigated ensembles of artificial neural networks. In this work, we investigated multiple widely used ensemble methods, including unweighted averaging, majority voting, the Bayes Optimal Classifier, and the (discrete) Super Learner, for image recognition tasks, with deep neural networks as candidate algorithms. We designed several experiments, with the candidate algorithms being the same network structure with different model checkpoints within a single training process, networks with same structure but trained multiple times stochastically, and networks with different structure. In addition, we further studied the over-confidence phenomenon of the neural networks, as well as its impact on the ensemble methods. Across all of our experiments, the Super Learner achieved best performance among all the ensemble methods in this study.},
  journal = {Journal of Applied Statistics},
  keywords = {convolutional neural network,Ensemble learning,super learner},
  number = {15}
}

@article{juRelativePerformanceEnsemble2018a,
  title = {The Relative Performance of Ensemble Methods with Deep Convolutional Neural Networks for Image Classification},
  author = {Ju, Cheng and Bibaut, Aur{\'e}lien and {van der Laan}, Mark},
  year = {2018},
  volume = {45},
  pages = {2800--2818},
  doi = {10.1080/02664763.2018.1441383},
  abstract = {Artificial neural networks have been successfully applied to a variety of machine learning tasks, including image recognition, semantic segmentation, and machine translation. However, few studies fully investigated ensembles of artificial neural networks. In this work, we investigated multiple widely used ensemble methods, including unweighted averaging, majority voting, the Bayes Optimal Classifier, and the (discrete) Super Learner, for image recognition tasks, with deep neural networks as candidate algorithms. We designed several experiments, with the candidate algorithms being the same network structure with different model checkpoints within a single training process, networks with same structure but trained multiple times stochastically, and networks with different structure. In addition, we further studied the over-confidence phenomenon of the neural networks, as well as its impact on the ensemble methods. Across all of our experiments, the Super Learner achieved best performance among all the ensemble methods in this study.},
  journal = {Journal of Applied Statistics},
  keywords = {convolutional neural network,Ensemble learning,super learner},
  number = {15}
}

@article{juRelativePerformanceEnsemble2018b,
  title = {The Relative Performance of Ensemble Methods with Deep Convolutional Neural Networks for Image Classification},
  author = {Ju, Cheng and Bibaut, Aur{\'e}lien and {van der Laan}, Mark},
  year = {2018},
  volume = {45},
  pages = {2800--2818},
  doi = {10.1080/02664763.2018.1441383},
  abstract = {Artificial neural networks have been successfully applied to a variety of machine learning tasks, including image recognition, semantic segmentation, and machine translation. However, few studies fully investigated ensembles of artificial neural networks. In this work, we investigated multiple widely used ensemble methods, including unweighted averaging, majority voting, the Bayes Optimal Classifier, and the (discrete) Super Learner, for image recognition tasks, with deep neural networks as candidate algorithms. We designed several experiments, with the candidate algorithms being the same network structure with different model checkpoints within a single training process, networks with same structure but trained multiple times stochastically, and networks with different structure. In addition, we further studied the over-confidence phenomenon of the neural networks, as well as its impact on the ensemble methods. Across all of our experiments, the Super Learner achieved best performance among all the ensemble methods in this study.},
  journal = {Journal of Applied Statistics},
  keywords = {convolutional neural network,Ensemble learning,super learner},
  number = {15}
}

@article{kansaraImprovingAccuracyReal2018,
  title = {Improving {{Accuracy}} of {{Real Estate Valuation Using Stacked Regression}}},
  author = {Kansara, Dhvani and Singh, Rashika and Sanghvi, Deep and Kanani, Pratik and Engineering, Information Technology},
  year = {2018},
  volume = {6},
  pages = {571--577},
  file = {/Users/salvatorykessy/Zotero/storage/KSRB4ZIC/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.ris},
  number = {3}
}

@article{kansaraImprovingAccuracyReal2018a,
  title = {Improving {{Accuracy}} of {{Real Estate Valuation Using Stacked Regression}}},
  author = {Kansara, Dhvani and Singh, Rashika and Sanghvi, Deep and Kanani, Pratik and Engineering, Information Technology},
  year = {2018},
  volume = {6},
  pages = {571--577},
  file = {/Users/salvatorykessy/Zotero/storage/L6XPALDC/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf},
  number = {3}
}

@article{kansaraImprovingAccuracyReal2018b,
  title = {Improving {{Accuracy}} of {{Real Estate Valuation Using Stacked Regression}}},
  author = {Kansara, Dhvani and Singh, Rashika and Sanghvi, Deep and Kanani, Pratik and Engineering, Information Technology},
  year = {2018},
  volume = {6},
  pages = {571--577},
  number = {3}
}

@article{kassBayesFactors1995,
  title = {Bayes {{Factors}}},
  author = {Kass, Robert E. and Raftery, Adrian E.},
  year = {1995},
  month = jun,
  volume = {90},
  pages = {773--795},
  issn = {0162-1459},
  doi = {10.1080/01621459.1995.10476572},
  annote = {doi: 10.1080/01621459.1995.10476572},
  journal = {Journal of the American Statistical Association},
  number = {430}
}

@article{khairallaShortTermForecastingEnergy2018,
  title = {Short-{{Term Forecasting}} for {{Energy Consumption}} through {{Stacking Heterogeneous Ensemble Learning Model}}},
  author = {Khairalla, Mergani A and Ning, Xu and {AL-Jallad}, Nashat T and {El-Faroug}, Musaab O},
  year = {2018},
  volume = {11},
  doi = {10.3390/en11061605},
  abstract = {In the real-life, time-series data comprise a complicated pattern, hence it may be challenging to increase prediction accuracy rates by using machine learning and conventional statistical methods as single learners. This research outlines and investigates the Stacking Multi-Learning Ensemble (SMLE) model for time series prediction problem over various horizons with a focus on the forecasts accuracy, directions hit-rate, and the average growth rate of total oil demand. This investigation presents a flexible ensemble framework in light of blend heterogeneous models for demonstrating and forecasting nonlinear time series. The proposed SMLE model combines support vector regression (SVR), backpropagation neural network (BPNN), and linear regression (LR) learners, the ensemble architecture consists of four phases: generation, pruning, integration, and ensemble prediction task. We have conducted an empirical study to evaluate and compare the performance of SMLE using Global Oil Consumption (GOC). Thus, the assessment of the proposed model was conducted at single and multistep horizon prediction using unique benchmark techniques. The final results reveal that the proposed SMLE model outperforms all the other benchmark methods listed in this study at various levels such as error rate, similarity, and directional accuracy by 0.74\%, 0.020\%, and 91.24\%, respectively. Therefore, this study demonstrates that the ensemble model is an extremely encouraging methodology for complex time series forecasting.},
  file = {/Users/salvatorykessy/Zotero/storage/7RW93R8D/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.ris},
  journal = {Energies},
  keywords = {Ensemble learning,Heterogeneous models,Oil consumption,SMLE,Time series forecasting},
  number = {6}
}

@article{khairallaShortTermForecastingEnergy2018a,
  title = {Short-{{Term Forecasting}} for {{Energy Consumption}} through {{Stacking Heterogeneous Ensemble Learning Model}}},
  author = {Khairalla, Mergani A. and Ning, Xu and {AL-Jallad}, Nashat T. and {El-Faroug}, Musaab O.},
  year = {2018},
  volume = {11},
  issn = {8618876090760},
  doi = {10.3390/en11061605},
  abstract = {In the real-life, time-series data comprise a complicated pattern, hence it may be challenging to increase prediction accuracy rates by using machine learning and conventional statistical methods as single learners. This research outlines and investigates the Stacking Multi-Learning Ensemble (SMLE) model for time series prediction problem over various horizons with a focus on the forecasts accuracy, directions hit-rate, and the average growth rate of total oil demand. This investigation presents a flexible ensemble framework in light of blend heterogeneous models for demonstrating and forecasting nonlinear time series. The proposed SMLE model combines support vector regression (SVR), backpropagation neural network (BPNN), and linear regression (LR) learners, the ensemble architecture consists of four phases: generation, pruning, integration, and ensemble prediction task. We have conducted an empirical study to evaluate and compare the performance of SMLE using Global Oil Consumption (GOC). Thus, the assessment of the proposed model was conducted at single and multistep horizon prediction using unique benchmark techniques. The final results reveal that the proposed SMLE model outperforms all the other benchmark methods listed in this study at various levels such as error rate, similarity, and directional accuracy by 0.74\%, 0.020\%, and 91.24\%, respectively. Therefore, this study demonstrates that the ensemble model is an extremely encouraging methodology for complex time series forecasting.},
  file = {/Users/salvatorykessy/Zotero/storage/9RDGP2HL/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf},
  journal = {Energies},
  keywords = {Ensemble learning,Heterogeneous models,Oil consumption,SMLE,Time series forecasting},
  number = {6}
}

@article{khairallaShortTermForecastingEnergy2018b,
  title = {Short-{{Term Forecasting}} for {{Energy Consumption}} through {{Stacking Heterogeneous Ensemble Learning Model}}},
  author = {Khairalla, Mergani A. and Ning, Xu and {AL-Jallad}, Nashat T. and {El-Faroug}, Musaab O.},
  year = {2018},
  volume = {11},
  issn = {8618876090760},
  doi = {10.3390/en11061605},
  abstract = {In the real-life, time-series data comprise a complicated pattern, hence it may be challenging to increase prediction accuracy rates by using machine learning and conventional statistical methods as single learners. This research outlines and investigates the Stacking Multi-Learning Ensemble (SMLE) model for time series prediction problem over various horizons with a focus on the forecasts accuracy, directions hit-rate, and the average growth rate of total oil demand. This investigation presents a flexible ensemble framework in light of blend heterogeneous models for demonstrating and forecasting nonlinear time series. The proposed SMLE model combines support vector regression (SVR), backpropagation neural network (BPNN), and linear regression (LR) learners, the ensemble architecture consists of four phases: generation, pruning, integration, and ensemble prediction task. We have conducted an empirical study to evaluate and compare the performance of SMLE using Global Oil Consumption (GOC). Thus, the assessment of the proposed model was conducted at single and multistep horizon prediction using unique benchmark techniques. The final results reveal that the proposed SMLE model outperforms all the other benchmark methods listed in this study at various levels such as error rate, similarity, and directional accuracy by 0.74\%, 0.020\%, and 91.24\%, respectively. Therefore, this study demonstrates that the ensemble model is an extremely encouraging methodology for complex time series forecasting.},
  journal = {Energies},
  keywords = {Ensemble learning,Heterogeneous models,Oil consumption,SMLE,Time series forecasting},
  number = {6}
}

@article{khorasaneeRiskSharingBenefitSmoothing2012,
  title = {Risk-{{Sharing}} and {{Benefit Smoothing}} in {{A Hybrid Pension Plan}}},
  author = {Khorasanee, Zaki M.},
  year = {2012},
  month = oct,
  volume = {16},
  pages = {449--461},
  issn = {1092-0277, 2325-0453},
  doi = {10.1080/10920277.2012.10597642},
  abstract = {A hybrid pension plan with an explicit formula for sharing risk between the plan sponsor and the members is proposed. The performance of this plan is analyzed using a modified version of the model used by Dufresne (1988). Formulas for the variance of the contribution income and benefit outgo are derived, assuming investment returns are independent and identically distributed. The performance of the hybrid plan is compared with a defined contribution (DC) plan providing the same expected retirement benefit. It is shown that the hybrid plan is more efficient in the control of investment risk, and that this gain in efficiency is greater when ``lifestyle'' investment strategies are adopted in the DC plan. Modifications to the proposed hybrid benefit structure that might be required for a real plan are suggested.},
  file = {/Users/salvatorykessy/Zotero/storage/EZUYKMD7/Khorasanee - 2012 - Risk-Sharing and Benefit Smoothing in A Hybrid Pen.pdf},
  journal = {North American Actuarial Journal},
  language = {en},
  number = {4}
}

@article{kjaergaardImportanceReferencePopulation2015,
  title = {The Importance of the Reference Population for Coherent Mortality Forecasting Models},
  author = {Kj{\ae}rgaard, S{\o}ren and {Canudas-romo}, Vladimir and Vaupel, James W},
  year = {2015},
  file = {/Users/salvatorykessy/Zotero/storage/7ZZ5TNNR/Kjærgaard, Canudas-romo, Vaupel - 2015 - The importance of the reference population for coherent mortality forecasting models.pdf},
  keywords = {coherent forecast,li-lee,life expectancy,mortality forecasting}
}

@article{kontisFutureLifeExpectancy2017,
  title = {Future Life Expectancy in 35 Industrialised Countries: Projections with a {{Bayesian}} Model Ensemble},
  author = {Kontis, Vasilis and Bennett, James E. and Mathers, Colin D. and Li, Guangquan and Foreman, Kyle and Ezzati, Majid},
  year = {2017},
  volume = {389},
  pages = {1323--1335},
  doi = {10.1016/S0140-6736(16)32381-9},
  abstract = {Background Projections of future mortality and life expectancy are needed to plan for health and social services and pensions. Our aim was to forecast national age-specific mortality and life expectancy using an approach that takes into account the uncertainty related to the choice of forecasting model. Methods We developed an ensemble of 21 forecasting models, all of which probabilistically contributed towards the final projections. We applied this approach to project age-specific mortality to 2030 in 35 industrialised countries with high-quality vital statistics data. We used age-specific death rates to calculate life expectancy at birth and at age 65 years, and probability of dying before age 70 years, with life table methods. Findings Life expectancy is projected to increase in all 35 countries with a probability of at least 65\% for women and 85\% for men. There is a 90\% probability that life expectancy at birth among South Korean women in 2030 will be higher than 86{$\cdot$}7 years, the same as the highest worldwide life expectancy in 2012, and a 57\% probability that it will be higher than 90 years. Projected female life expectancy in South Korea is followed by those in France, Spain, and Japan. There is a greater than 95\% probability that life expectancy at birth among men in South Korea, Australia, and Switzerland will surpass 80 years in 2030, and a greater than 27\% probability that it will surpass 85 years. Of the countries studied, the USA, Japan, Sweden, Greece, Macedonia, and Serbia have some of the lowest projected life expectancy gains for both men and women. The female life expectancy advantage over men is likely to shrink by 2030 in every country except Mexico, where female life expectancy is predicted to increase more than male life expectancy, and in Chile, France, and Greece where the two sexes will see similar gains. More than half of the projected gains in life expectancy at birth in women will be due to enhanced longevity above age 65 years. Interpretation There is more than a 50\% probability that by 2030, national female life expectancy will break the 90 year barrier, a level that was deemed unattainable by some at the turn of the 21st century. Our projections show continued increases in longevity, and the need for careful planning for health and social services and pensions. Funding UK Medical Research Council and US Environmental Protection Agency.},
  file = {/Users/salvatorykessy/Zotero/storage/C5JMPP26/Kontis et al. - 2017 - Future life expectancy in 35 industrialised countries projections with a Bayesian model ensemble.pdf},
  journal = {The Lancet},
  number = {10076}
}

@article{kourentzesAnotherLookForecast2019,
  title = {Another Look at Forecast Selection and Combination: {{Evidence}} from Forecast Pooling},
  shorttitle = {Another Look at Forecast Selection and Combination},
  author = {Kourentzes, Nikolaos and Barrow, Devon and Petropoulos, Fotios},
  year = {2019},
  month = mar,
  volume = {209},
  pages = {226--235},
  issn = {09255273},
  doi = {10.1016/j.ijpe.2018.05.019},
  file = {/Users/salvatorykessy/Zotero/storage/L5KGSG4D/Kourentzes et al. - 2019 - Another look at forecast selection and combination.pdf},
  journal = {International Journal of Production Economics},
  language = {en}
}

@article{kroghNeuralNetworkEnsembles1995,
  title = {Neural {{Network Ensembles}}, {{Cross Validation}}, and {{Active Learning}}},
  author = {Krogh, Anders and Vedelsby, Jesper},
  year = {1995},
  pages = {231--238},
  issn = {0262201046},
  abstract = {Learning of continuous valued functions using neural network ensembles (committees) can give improved accuracy, reliable estimation of the generalization error, and active learning. The ambiguity is defined as the variation of the output of ensemble members averaged over unlabeled data, so it quantifies the disagreement among the networks. It is discussed how to use the ambiguity in combination with cross-validation to give a reliable estimate of the ensemble generalization error, and how this type of ensemble cross-validation can sometimes improve performance. It is shown how to estimate the optimal weights of the ensemble members using unlabeled data. By a generalization of query by committee, it is finally shown how the ambiguity can be used to select new training data to be labeled in an active learning scheme. 1 INTRODUCTION It is well known that a combination of many different predictors can improve predictions. In the neural networks community "ensembles" of neural networks h...},
  file = {/Users/salvatorykessy/Zotero/storage/GBKPV8LE/developing.pdf;/Users/salvatorykessy/Zotero/storage/HKEWUWCC/Krogh, Vedelsby - 1995 - Neural Network Ensembles, Cross Validation, and Active Learning(2).pdf},
  journal = {Advances in Neural Information Processing Systems 7}
}

@article{kroghNeuralNetworkEnsembles1995a,
  title = {Neural {{Network Ensembles}}, {{Cross Validation}}, and {{Active Learning}}},
  author = {Krogh, Anders and Vedelsby, Jesper},
  year = {1995},
  pages = {231--238},
  issn = {0262201046},
  abstract = {Learning of continuous valued functions using neural network ensembles (committees) can give improved accuracy, reliable estimation of the generalization error, and active learning. The ambiguity is defined as the variation of the output of ensemble members averaged over unlabeled data, so it quantifies the disagreement among the networks. It is discussed how to use the ambiguity in combination with cross-validation to give a reliable estimate of the ensemble generalization error, and how this type of ensemble cross-validation can sometimes improve performance. It is shown how to estimate the optimal weights of the ensemble members using unlabeled data. By a generalization of query by committee, it is finally shown how the ambiguity can be used to select new training data to be labeled in an active learning scheme. 1 INTRODUCTION It is well known that a combination of many different predictors can improve predictions. In the neural networks community "ensembles" of neural networks h...},
  file = {/Users/salvatorykessy/Zotero/storage/3TRPBKPE/Krogh, Vedelsby - 1995 - Neural Network Ensembles, Cross Validation, and Active Learning(2).pdf;/Users/salvatorykessy/Zotero/storage/YSY766GY/Akaike_weights.pdf},
  journal = {Advances in Neural Information Processing Systems 7}
}

@article{leBayesInterpretationStacking2017,
  ids = {leBayesInterpretationStacking2017a},
  title = {A {{Bayes Interpretation}} of {{Stacking}} for \$\textbackslash mathcal\{\vphantom\}{{M}}\vphantom\{\}\$ -{{Complete}} and \$\textbackslash mathcal\{\vphantom\}{{M}}\vphantom\{\}\$ -{{Open Settings}}},
  author = {Le, Tri and Clarke, Bertrand},
  year = {2017},
  month = sep,
  volume = {12},
  pages = {807--829},
  issn = {1936-0975},
  doi = {10.1214/16-BA1023},
  abstract = {In M-open problems where no true model can be conceptualized, it is common to back off from modeling and merely seek good prediction. Even in M-complete problems, taking a predictive approach can be very useful. Stacking is a model averaging procedure that gives a composite predictor by combining individual predictors from a list of models using weights that optimize a crossvalidation criterion. We show that the stacking weights also asymptotically minimize a posterior expected loss. Hence we formally provide a Bayesian justification for cross-validation. Often the weights are constrained to be positive and sum to one. For greater generality, we omit the positivity constraint and relax the `sum to one' constraint.},
  file = {/Users/salvatorykessy/Zotero/storage/C9ZAWJKW/Le and Clarke - 2017 - A Bayes Interpretation of Stacking for $mathcal M.pdf;/Users/salvatorykessy/Zotero/storage/FT9IQ6IX/Le and Clarke - 2017 - A Bayes Interpretation of Stacking for $mathcal M.pdf},
  journal = {Bayesian Analysis},
  language = {en},
  number = {3}
}

@article{leblancCombiningEstiamatesRegression2016,
  title = {Combining {{Estiamates}} in {{Regression}} and {{Classification Stable URL}} : {{http://www.jstor.org/stable/2291591}} {{Linked}} References Are Available on {{JSTOR}} for This Article : {{Combining Estimates}} in {{Regression}} and {{Classification}}},
  author = {Leblanc, Michael and Tibshirani, Robert and Leblanc, Michael and Tibshirani, Robert},
  year = {2016},
  volume = {91},
  pages = {1641--1650},
  file = {/Users/salvatorykessy/Zotero/storage/ZS26VWTW/Leblanc et al. - 2016 - Combining Estiamates in Regression and Classification Stable URL httpwww.jstor.orgstable2291591 Linked referenc.pdf},
  keywords = {bootstrap,cross-validation,model combination},
  number = {436}
}

@article{leeEvaluatingPerformanceLeecarter2001,
  title = {Evaluating the Performance of the Lee-Carter Method for Forecasting Mortality},
  author = {Lee, Ronald and Miller, Timothy},
  year = {2001},
  volume = {38},
  pages = {13},
  file = {/Users/salvatorykessy/Zotero/storage/PDQGH72V/Lee and Miller - 2001 - Evaluating the performance of the lee-carter metho.pdf},
  language = {en},
  number = {4}
}

@article{leeModelingForecastingMortality1992,
  title = {Modeling and {{Forecasting U}} . {{S}} . {{Mortality Author}} ( s ): {{Carter Published}} by : {{American}}},
  author = {Lee, Ronald D . and Carter, Lawrence R},
  year = {1992},
  volume = {87},
  pages = {659--671},
  file = {/Users/salvatorykessy/Zotero/storage/X6UU6LIH/Lee, Carter - 1992 - Modeling and Forecasting U . S . Mortality Author ( s ) Carter Published by American.pdf},
  journal = {Statistical Association Stable},
  keywords = {75 years,continue to rise at,data and evaluate,demographic model to u,demography,forecast,from 1900 to 1988,if it were to,life expectancy,life expectancy in the,mortality,next we fit the,population,projection,rose from 47 to,s,this,united states},
  number = {419}
}

@article{leePerformanceEnsembleClassifier2010,
  title = {Performance of Ensemble Classifier for Location Prediction Task: Emphasis on {{Markov Blanket}} Perspective},
  author = {Lee, Kun Chang and Cho, Heeryon},
  year = {2010},
  volume = {3},
  pages = {1--12},
  abstract = {As the ubiquitous computing becomes popular, its applications come to real life as a form of a wide variety of ubiquitous decision support systems (UDSS). However, such ubiquity should be supported by prediction capability no matter which kind of contexts users are in. In this sense, context prediction capability, which is to predict future contexts users are going to enter sooner or later, becomes an extremely important part of ubiquitous decision support systems. This study proposes a new breed of context prediction mechanism using the Markov Blanket obtained from General Bayesian Network (GBN) as a main vehicle. To improve the prediction accuracy, ensemble of robust prediction classifiers is suggested on the basis of the GBN Markov Blanket. Three classifiers included in the ensemble mechanism are Bayesian networks, decision classifiers, and an SVM (Support Vector Machine). The proposed GBN Markov blanket-assisted ensemble classifier is applied to a real dataset of location prediction. Results were promising enough to conclude that the proposed ensemble classifier based on the GBN Markov Blanket is worthwhile for being adopted in developing a powerful context prediction purpose UDSS. Practical implications are also discussed with future research issues. [ABSTRACT FROM AUTHOR]},
  journal = {International Journal of u-and e-Service, Science and Technology},
  keywords = {BAYESIAN analysis,C4.5,CART,Context Prediction,DECISION support systems,DECISION theory,Ensemble Methods,GBN Markov Blanket-Assisted Ensemble Classifier,General Bayesian Network,ID3,Location Prediction,MARKOV processes,SUPPORT vector machines,SVM,UBIQUITOUS computing},
  number = {3}
}

@article{leePerformanceEnsembleClassifier2010a,
  title = {Performance of Ensemble Classifier for Location Prediction Task: Emphasis on {{Markov Blanket}} Perspective},
  author = {Lee, Kun Chang and Cho, Heeryon},
  year = {2010},
  volume = {3},
  pages = {1--12},
  abstract = {As the ubiquitous computing becomes popular, its applications come to real life as a form of a wide variety of ubiquitous decision support systems (UDSS). However, such ubiquity should be supported by prediction capability no matter which kind of contexts users are in. In this sense, context prediction capability, which is to predict future contexts users are going to enter sooner or later, becomes an extremely important part of ubiquitous decision support systems. This study proposes a new breed of context prediction mechanism using the Markov Blanket obtained from General Bayesian Network (GBN) as a main vehicle. To improve the prediction accuracy, ensemble of robust prediction classifiers is suggested on the basis of the GBN Markov Blanket. Three classifiers included in the ensemble mechanism are Bayesian networks, decision classifiers, and an SVM (Support Vector Machine). The proposed GBN Markov blanket-assisted ensemble classifier is applied to a real dataset of location prediction. Results were promising enough to conclude that the proposed ensemble classifier based on the GBN Markov Blanket is worthwhile for being adopted in developing a powerful context prediction purpose UDSS. Practical implications are also discussed with future research issues. [ABSTRACT FROM AUTHOR]},
  journal = {International Journal of u-and e-Service, Science and Technology},
  keywords = {BAYESIAN analysis,C4.5,CART,Context Prediction,DECISION support systems,DECISION theory,Ensemble Methods,GBN Markov Blanket-Assisted Ensemble Classifier,General Bayesian Network,ID3,Location Prediction,MARKOV processes,SUPPORT vector machines,SVM,UBIQUITOUS computing},
  number = {3}
}

@article{leePerformanceEnsembleClassifier2010b,
  title = {Performance of Ensemble Classifier for Location Prediction Task: Emphasis on {{Markov Blanket}} Perspective},
  author = {Lee, Kun Chang and Cho, Heeryon},
  year = {2010},
  volume = {3},
  pages = {1--12},
  abstract = {As the ubiquitous computing becomes popular, its applications come to real life as a form of a wide variety of ubiquitous decision support systems (UDSS). However, such ubiquity should be supported by prediction capability no matter which kind of contexts users are in. In this sense, context prediction capability, which is to predict future contexts users are going to enter sooner or later, becomes an extremely important part of ubiquitous decision support systems. This study proposes a new breed of context prediction mechanism using the Markov Blanket obtained from General Bayesian Network (GBN) as a main vehicle. To improve the prediction accuracy, ensemble of robust prediction classifiers is suggested on the basis of the GBN Markov Blanket. Three classifiers included in the ensemble mechanism are Bayesian networks, decision classifiers, and an SVM (Support Vector Machine). The proposed GBN Markov blanket-assisted ensemble classifier is applied to a real dataset of location prediction. Results were promising enough to conclude that the proposed ensemble classifier based on the GBN Markov Blanket is worthwhile for being adopted in developing a powerful context prediction purpose UDSS. Practical implications are also discussed with future research issues. [ABSTRACT FROM AUTHOR]},
  journal = {International Journal of u-and e-Service, Science and Technology},
  keywords = {BAYESIAN analysis,C4.5,CART,Context Prediction,DECISION support systems,DECISION theory,Ensemble Methods,GBN Markov Blanket-Assisted Ensemble Classifier,General Bayesian Network,ID3,Location Prediction,MARKOV processes,SUPPORT vector machines,SVM,UBIQUITOUS computing},
  number = {3}
}

@article{lemkeMetalearningTimeSeries2010,
  title = {Meta-Learning for Time Series Forecasting and Forecast Combination},
  author = {Lemke, Christiane and Gabrys, Bogdan},
  year = {2010},
  month = jun,
  volume = {73},
  pages = {2006--2016},
  issn = {09252312},
  doi = {10.1016/j.neucom.2009.09.020},
  abstract = {In research of time series forecasting, a lot of uncertainty is still related to the task of selecting an appropriate forecasting method for a problem. It is not only the individual algorithms that are available in great quantities; combination approaches have been equally popular in the last decades. Alone the question of whether to choose the most promising individual method or a combination is not straightforward to answer. Usually, expert knowledge is needed to make an informed decision, however, in many cases this is not feasible due to lack of resources like time, money and manpower. This work identifies an extensive feature set describing both the time series and the pool of individual forecasting methods. The applicability of different meta-learning approaches are investigated, first to gain knowledge on which model works best in which situation, later to improve forecasting performance. Results show the superiority of a rankingbased combination of methods over simple model selection approaches.},
  file = {/Users/salvatorykessy/Zotero/storage/FX9LEC78/Lemke and Gabrys - 2010 - Meta-learning for time series forecasting and fore.pdf},
  journal = {Neurocomputing},
  language = {en},
  number = {10-12}
}

@article{levantesiApplicationMachineLearning2019,
  title = {Application of {{Machine Learning}} to {{Mortality Modeling}} and {{Forecasting}}},
  author = {Levantesi, Susanna and Pizzorusso, Virginia},
  year = {2019},
  volume = {7},
  pages = {26--26},
  doi = {10.3390/risks7010026},
  abstract = {Estimation of future mortality rates still plays a central role among life insurers in pricing their products and managing longevity risk. In the literature on mortality modeling, a wide number of stochastic models have been proposed, most of them forecasting future mortality rates by extrapolating one or more latent factors. The abundance of proposed models shows that forecasting future mortality from historical trends is non-trivial. Following the idea proposed in Deprez et al. (2017), we use machine learning algorithms, able to catch patterns that are not commonly identifiable, to calibrate a parameter (the machine learning estimator), improving the goodness of fit of standard stochastic mortality models. The machine learning estimator is then forecasted according to the Lee-Carter framework, allowing one to obtain a higher forecasting quality of the standard stochastic models. Out-of sample forecasts are provided to verify the model accuracy.},
  file = {/Users/salvatorykessy/Zotero/storage/SF2AT6XX/Levantesi, Pizzorusso - 2019 - Application of Machine Learning to Mortality Modeling and Forecasting.pdf},
  journal = {Risks},
  keywords = {forecasting,lee-carter model,machine learning,mortality},
  number = {1}
}

@article{levantesiApplicationMachineLearning2019a,
  title = {Application of {{Machine Learning}} to {{Mortality Modeling}} and {{Forecasting}}},
  author = {Levantesi, Susanna and Pizzorusso, Virginia},
  year = {2019},
  month = feb,
  volume = {7},
  pages = {26},
  issn = {2227-9091},
  doi = {10.3390/risks7010026},
  abstract = {Estimation of future mortality rates still plays a central role among life insurers in pricing their products and managing longevity risk. In the literature on mortality modeling, a wide number of stochastic models have been proposed, most of them forecasting future mortality rates by extrapolating one or more latent factors. The abundance of proposed models shows that forecasting future mortality from historical trends is non-trivial. Following the idea proposed in Deprez et al. (2017), we use machine learning algorithms, able to catch patterns that are not commonly identifiable, to calibrate a parameter (the machine learning estimator), improving the goodness of fit of standard stochastic mortality models. The machine learning estimator is then forecasted according to the Lee-Carter framework, allowing one to obtain a higher forecasting quality of the standard stochastic models. Out-of sample forecasts are provided to verify the model accuracy.},
  file = {/Users/salvatorykessy/Zotero/storage/9K6JNYJC/Levantesi and Pizzorusso - 2019 - Application of Machine Learning to Mortality Model.pdf},
  journal = {Risks},
  language = {en},
  number = {1}
}

@article{levantesiApplicationMachineLearning2019b,
  title = {Application of {{Machine Learning}} to {{Mortality Modeling}} and {{Forecasting}}},
  author = {Levantesi, Susanna and Pizzorusso, Virginia},
  year = {2019},
  volume = {7},
  pages = {26--26},
  doi = {10.3390/risks7010026},
  abstract = {Estimation of future mortality rates still plays a central role among life insurers in pricing their products and managing longevity risk. In the literature on mortality modeling, a wide number of stochastic models have been proposed, most of them forecasting future mortality rates by extrapolating one or more latent factors. The abundance of proposed models shows that forecasting future mortality from historical trends is non-trivial. Following the idea proposed in Deprez et al. (2017), we use machine learning algorithms, able to catch patterns that are not commonly identifiable, to calibrate a parameter (the machine learning estimator), improving the goodness of fit of standard stochastic mortality models. The machine learning estimator is then forecasted according to the Lee-Carter framework, allowing one to obtain a higher forecasting quality of the standard stochastic models. Out-of sample forecasts are provided to verify the model accuracy.},
  file = {/Users/salvatorykessy/Zotero/storage/GP7L7AL7/Levantesi, Pizzorusso - 2019 - Application of Machine Learning to Mortality Modeling and Forecasting.pdf},
  journal = {Risks},
  keywords = {forecasting,lee-carter model,machine learning,mortality},
  number = {1}
}

@article{liCoherentMortalityForecasts2005,
  title = {Coherent {{Mortality Forecasts}} for a {{Group}} of {{Populations}}: {{An Extension}} of the {{Lee}}-{{Carter Method}}},
  shorttitle = {Coherent {{Mortality Forecasts}} for a {{Group}} of {{Populations}}},
  author = {Li, Nan and Lee, Ronald Demos},
  year = {2005},
  volume = {42},
  pages = {575--594},
  issn = {1533-7790},
  doi = {10.1353/dem.2005.0021},
  file = {/Users/salvatorykessy/Zotero/storage/8B5BQUZN/Li and Lee - 2005 - Coherent Mortality Forecasts for a Group of Popula.pdf},
  journal = {Demography},
  language = {en},
  number = {3}
}

@article{liModelingCauseofdeathMortality2019,
  title = {Modeling Cause-of-Death Mortality Using Hierarchical {{Archimedean}} Copula},
  author = {Li, Hong and Lu, Yang},
  year = {2019},
  month = mar,
  volume = {2019},
  pages = {247--272},
  issn = {0346-1238, 1651-2030},
  doi = {10.1080/03461238.2018.1546224},
  abstract = {Studying changes in cause-specific (or competing risks) mortality rates may provide significant insights for the insurance business as well as the pension systems, as they provide more information than the aggregate mortality data. However, the forecasting of cause-specific mortality rates requires new tools to capture the dependence among the competing causes. This paper introduces a class of hierarchical Archimedean copula (HAC) models for cause-specific mortality data. The approach extends the standard Archimedean copula models by allowing for asymmetric dependence among competing risks, while preserving closed-form expressions for mortality forecasts. Moreover, the HAC model allows for a convenient analysis of the impact of hypothetical reduction, or elimination, of mortality of one or more causes on the life expectancy. Using US cohort mortality data, we analyze the historical mortality patterns of different causes of death, provide an explanation for the `failure' of the War on Cancer, and evaluate the impact on life expectancy of hypothetical scenarios where cancer mortality is reduced or eliminated. We find that accounting for longevity improvement across cohorts can alter the results found in existing studies that are focused on one single cohort.},
  file = {/Users/salvatorykessy/Zotero/storage/XM5SKCPF/Li and Lu - 2019 - Modeling cause-of-death mortality using hierarchic.pdf},
  journal = {Scandinavian Actuarial Journal},
  language = {en},
  number = {3}
}

@article{lingModelEnsembleClick2018,
  title = {Model {{Ensemble}} for {{Click Prediction}} in {{Bing Search Ads}}},
  author = {Ling, Xiaoliang and Deng, Weiwei and Gu, Chen and Zhou, Hucheng and Li, Cui and Sun, Feng},
  year = {2018},
  pages = {689--698},
  doi = {10.1145/3041021.3054192},
  abstract = {Accurate estimation of the click-through rate (CTR) in sponsored ads significantly impacts the user search experience and businesses' revenue, even 0.1\% of accuracy improvement would yield greater earnings in the hundreds of millions of dollars. CTR prediction is generally formulated as a supervised classification problem. In this paper, we share our experience and learning on model ensemble de- sign and our innovation. Specifically, we present 8 ensemble meth- ods and evaluate them on our production data. Boosting neural net- works with gradient boosting decision trees turns out to be the best. With larger training data, there is a nearly 0.9\% AUC improvement in offline testing and 2.7\% click yield gains in online traffic. In addition, we share our experience and learning on improving the quality of training.},
  keywords = {click prediction,dnn,gbdt,model ensemble},
  number = {5}
}

@article{lingModelEnsembleClick2018a,
  title = {Model {{Ensemble}} for {{Click Prediction}} in {{Bing Search Ads}}},
  author = {Ling, Xiaoliang and Deng, Weiwei and Gu, Chen and Zhou, Hucheng and Li, Cui and Sun, Feng},
  year = {2018},
  pages = {689--698},
  issn = {9781450349147},
  doi = {10.1145/3041021.3054192},
  abstract = {Accurate estimation of the click-through rate (CTR) in sponsored ads significantly impacts the user search experience and businesses' revenue, even 0.1\% of accuracy improvement would yield greater earnings in the hundreds of millions of dollars. CTR prediction is generally formulated as a supervised classification problem. In this paper, we share our experience and learning on model ensemble de- sign and our innovation. Specifically, we present 8 ensemble meth- ods and evaluate them on our production data. Boosting neural net- works with gradient boosting decision trees turns out to be the best. With larger training data, there is a nearly 0.9\% AUC improvement in offline testing and 2.7\% click yield gains in online traffic. In addition, we share our experience and learning on improving the quality of training.},
  keywords = {click prediction,dnn,gbdt,model ensemble},
  number = {5}
}

@article{lingModelEnsembleClick2018b,
  title = {Model {{Ensemble}} for {{Click Prediction}} in {{Bing Search Ads}}},
  author = {Ling, Xiaoliang and Deng, Weiwei and Gu, Chen and Zhou, Hucheng and Li, Cui and Sun, Feng},
  year = {2018},
  pages = {689--698},
  issn = {9781450349147},
  doi = {10.1145/3041021.3054192},
  abstract = {Accurate estimation of the click-through rate (CTR) in sponsored ads significantly impacts the user search experience and businesses' revenue, even 0.1\% of accuracy improvement would yield greater earnings in the hundreds of millions of dollars. CTR prediction is generally formulated as a supervised classification problem. In this paper, we share our experience and learning on model ensemble de- sign and our innovation. Specifically, we present 8 ensemble meth- ods and evaluate them on our production data. Boosting neural net- works with gradient boosting decision trees turns out to be the best. With larger training data, there is a nearly 0.9\% AUC improvement in offline testing and 2.7\% click yield gains in online traffic. In addition, we share our experience and learning on improving the quality of training.},
  file = {/Users/salvatorykessy/Zotero/storage/UT7NSHB8/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.ris},
  keywords = {click prediction,dnn,gbdt,model ensemble},
  number = {5}
}

@article{linSupportVectorMachinery2008,
  title = {Support {{Vector Machinery}} for {{Infinite Ensemble Learning}}},
  author = {Lin, Hsuan-Tien and Li, Ling},
  year = {2008},
  volume = {9},
  pages = {285--312},
  abstract = {Ensemble learning algorithms such as boosting can achieve better performance by averaging over the predictions of some base hypotheses. Nevertheless, most existing algorithms are limited to combining only a finite number of hypotheses, and the generated ensemble is usually sparse. Thus, it is not clear whether we should construct an ensemble classifier with a larger or even an infinite number of hypotheses. In addition, constructing an infinite ensemble itself is a challenging task. In this paper, we formulate an infinite ensemble learning framework based on the support vector machine (SVM). The framework can output an infinite and nonsparse ensemble through embedding infinitely many hypotheses into an SVM kernel. We use the framework to derive two novel kernels, the stump kernel and the perceptron kernel. The stump kernel embodies infinitely many decision stumps, and the perceptron kernel embodies infinitely many perceptrons. We also show that the Laplacian radial basis function kernel embodies infinitely many decision trees, and can thus be explained through infinite ensemble learning. Experimental results show that SVM with these kernels is superior to boosting with the same base hypothesis set. In addition, SVM with the stump kernel or the perceptron kernel performs similarly to SVM with the Gaussian radial basis function kernel, but enjoys the benefit of faster parameter selection. These properties make the novel kernels favorable choices in practice.},
  file = {/Users/salvatorykessy/Zotero/storage/CG5GL4TU/Lin, Li - 2008 - Support Vector Machinery for Infinite Ensemble Learning.pdf},
  journal = {Journal of Machine Learning Research},
  keywords = {boosting,ensemble learning,kernel,support vector machine}
}

@article{linSupportVectorMachinery2008a,
  title = {Support {{Vector Machinery}} for {{Infinite Ensemble Learning}}},
  author = {Lin, Hsuan-Tien and Li, Ling},
  year = {2008},
  volume = {9},
  pages = {285--312},
  abstract = {Ensemble learning algorithms such as boosting can achieve better performance by averaging over the predictions of some base hypotheses. Nevertheless, most existing algorithms are limited to combining only a finite number of hypotheses, and the generated ensemble is usually sparse. Thus, it is not clear whether we should construct an ensemble classifier with a larger or even an infinite number of hypotheses. In addition, constructing an infinite ensemble itself is a challenging task. In this paper, we formulate an infinite ensemble learning framework based on the support vector machine (SVM). The framework can output an infinite and nonsparse ensemble through embedding infinitely many hypotheses into an SVM kernel. We use the framework to derive two novel kernels, the stump kernel and the perceptron kernel. The stump kernel embodies infinitely many decision stumps, and the perceptron kernel embodies infinitely many perceptrons. We also show that the Laplacian radial basis function kernel embodies infinitely many decision trees, and can thus be explained through infinite ensemble learning. Experimental results show that SVM with these kernels is superior to boosting with the same base hypothesis set. In addition, SVM with the stump kernel or the perceptron kernel performs similarly to SVM with the Gaussian radial basis function kernel, but enjoys the benefit of faster parameter selection. These properties make the novel kernels favorable choices in practice.},
  journal = {Journal of Machine Learning Research},
  keywords = {boosting,ensemble learning,kernel,support vector machine}
}

@article{lopezdepradoTenApplicationsFinancial2019,
  title = {Ten {{Applications}} of {{Financial Machine Learning}}},
  author = {{L{\'o}pez de Prado}, Marcos},
  year = {2019},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3365271},
  abstract = {This article reviews ten notable financial applications where Machine Learning (ML) has moved beyond hype and proven its usefulness. This success does not mean that the use of ML in finance does not face important challenges. The main conclusion from this article is that there is a strong case for applying ML to current financial problems, and that financial ML has a promising future ahead.},
  file = {/Users/salvatorykessy/Zotero/storage/9JDDK8ZV/López de Prado - 2019 - Ten Applications of Financial Machine Learning.pdf},
  journal = {SSRN Electronic Journal},
  language = {en}
}

@article{ludkovskiGaussianProcessModels2016,
  title = {Gaussian {{Process Models}} for {{Mortality Rates}} and {{Improvement Factors}}},
  author = {Ludkovski, Michael and Risk, James and Zail, Howard},
  year = {2016},
  doi = {10.2139/ssrn.2831831},
  abstract = {In Ludkovski, Risk, and Zail (2018), the email address for Jimmy Risk appeared incorrectly. Jimmy Risk's email address should appear as jrisk@cpp.edu . The original article has been corrected to rectify this error.},
  file = {/Users/salvatorykessy/Zotero/storage/GBF4I4V7/Ludkovski, Risk, Zail - 2016 - Gaussian Process Models for Mortality Rates and Improvement Factors.pdf},
  journal = {Ssrn},
  keywords = {Actuarial Science,C53,Gaussian Processes,I13,Kriging,Mortality Modeling}
}

@article{ludkovskiGaussianProcessModels2016a,
  title = {Gaussian {{Process Models}} for {{Mortality Rates}} and {{Improvement Factors}}},
  author = {Ludkovski, Michael and Risk, James and Zail, Howard},
  year = {2016},
  doi = {10.2139/ssrn.2831831},
  abstract = {In Ludkovski, Risk, and Zail (2018), the email address for Jimmy Risk appeared incorrectly. Jimmy Risk's email address should appear as jrisk@cpp.edu . The original article has been corrected to rectify this error.},
  file = {/Users/salvatorykessy/Zotero/storage/C5KK4C5Z/Ludkovski, Risk, Zail - 2016 - Gaussian Process Models for Mortality Rates and Improvement Factors.pdf},
  journal = {Ssrn},
  keywords = {Actuarial Science,C53,Gaussian Processes,I13,Kriging,Mortality Modeling}
}

@article{lyuMultiPopulationApproachForecasting2018,
  title = {A {{Multi}}-{{Population Approach}} to {{Forecasting All}}-{{Cause Mortality Using Cause}}-{{Specific Mortality Data}}},
  author = {Lyu, Pintao and Melenberg, Bertrand and Waegenaere, Anja De},
  year = {2018},
  pages = {44},
  abstract = {Existing literature has argued that all-cause mortality projections based on causespecific mortality experience have a number of serious drawbacks. There is a widely shared consensus that projections of all-cause mortality based on cause-specific mortality generally perform worse than the direct projection of all-cause mortality because if (1) the inferior cause of death mortality data and (2) the complex dependence structure between causes of death. In this paper, we use the recent WHO access version causes of death data to address this issue in a multi-population context. We propose a two-stage beta-convergence test to capture the cause-specific mortality dynamics between different countries and between different causes, summarizing the observed causes of death dependence structure. We incorporate international coherence and inter-cause coherence suggested by the two-stage beta-convergence test in a new nestedCoDLi-Lee model. We show that the all-cause mortality projections produced by the nestedCoDLi-Lee model, perform more or less equally well in sample as the ones from the Lee-Carter model and the ones from the Li-Lee model. However, in contrast to results from earlier studies, we find that the all-cause mortality projections of nestedCoDLi-Lee have a better out-of-sample performance in a long forecast horizon. Moreover, for the case of the Netherlands, about 2 years higher remaining life expectancy projections of 67-year-old Dutch males is obtained by the nestedCoDLiLee model.},
  file = {/Users/salvatorykessy/Zotero/storage/TX6R2NP8/Lyu et al. - A Multi-Population Approach to Forecasting All-Cau.pdf},
  language = {en}
}

@article{maEnsembleMachineLearning2018,
  title = {Ensemble of Machine Learning Algorithms Using the Stacked Generalization Approach to Estimate the Warfarin Dose},
  author = {Ma, Zhiyuan and Wang, Ping and Gao, Zehui and Wang, Ruobing and Khalighi, Koroush},
  editor = {Huk, Maciej},
  year = {2018},
  month = oct,
  volume = {13},
  pages = {e0205872},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0205872},
  abstract = {Warfarin dosing remains challenging due to narrow therapeutic index and highly individual variability. Incorrect warfarin dosing is associated with devastating adverse events. Remarkable efforts have been made to develop the machine learning based warfarin dosing algorithms incorporating clinical factors and genetic variants such as polymorphisms in CYP2C9 and VKORC1. The most widely validated pharmacogenetic algorithm is the IWPC algorithm based on multivariate linear regression (MLR). However, with only a single algorithm, the prediction performance may reach an upper limit even with optimal parameters. Here, we present novel algorithms using stacked generalization frameworks to estimate the warfarin dose, within which different types of machine learning algorithms function together through a meta-machine learning model to maximize the prediction accuracy. Compared to the IWPC-derived MLR algorithm, Stack 1 and 2 based on stacked generalization frameworks performed significantly better overall. Subgroup analysis revealed that the mean of the percentage of patients whose predicted dose of warfarin within 20\% of the actual stable therapeutic dose (mean percentage within 20\%) for Stack 1 was improved by 12.7\% (from 42.47\% to 47.86\%) in Asians and by 13.5\% (from 22.08\% to 25.05\%) in the low-dose group compared to that for MLR, respectively. These data suggest that our algorithms would especially benefit patients requiring low warfarin maintenance dose, as subtle changes in warfarin dose could lead to adverse clinical events (thrombosis or bleeding) in patients with low dose. Our study offers novel pharmacogenetic algorithms for clinical trials and practice.},
  file = {/Users/salvatorykessy/Zotero/storage/Y2S9APMX/Ma et al. - 2018 - Ensemble of machine learning algorithms using the .pdf},
  journal = {PLOS ONE},
  language = {en},
  number = {10}
}

@article{maEnsembleMachineLearning2018a,
  title = {Ensemble of Machine Learning Algorithms Using the Stacked Generalization Approach to Estimate the Warfarin Dose},
  author = {Ma, Zhiyuan and Wang, Ping and Gao, Zehui and Wang, Ruobing and Khalighi, Koroush},
  year = {2018},
  volume = {13},
  pages = {1--12},
  issn = {1111111111},
  doi = {10.1371/journal.pone.0205872},
  abstract = {Warfarin dosing remains challenging due to narrow therapeutic index and highly individual variability. Incorrect warfarin dosing is associated with devastating adverse events. Remarkable efforts have been made to develop the machine learning based warfarin dosing algorithms incorporating clinical factors and genetic variants such as polymorphisms in CYP2C9 and VKORC1. The most widely validated pharmacogenetic algorithm is the IWPC algorithm based on multivariate linear regression (MLR). However, with only a single algorithm, the prediction performance may reach an upper limit even with optimal parameters. Here, we present novel algorithms using stacked generalization frameworks to estimate the warfarin dose, within which different types of machine learning algorithms function together through a meta-machine learning model to maximize the prediction accuracy. Compared to the IWPC-derived MLR algorithm, Stack 1 and 2 based on stacked generalization frameworks performed significantly better overall. Subgroup analysis revealed that the mean of the percentage of patients whose predicted dose of warfarin within 20\% of the actual stable therapeutic dose (mean percentage within 20\%) for Stack 1 was improved by 12.7\% (from 42.47\% to 47.86\%) in Asians and by 13.5\% (from 22.08\% to 25.05\%) in the low-dose group compared to that for MLR, respectively. These data suggest that our algorithms would especially benefit patients requiring low warfarin maintenance dose, as subtle changes in warfarin dose could lead to adverse clinical events (thrombosis or bleeding) in patients with low dose. Our study offers novel pharmacogenetic algorithms for clinical trials and practice.},
  file = {/Users/salvatorykessy/Zotero/storage/ETJZBM2D/Ma et al. - 2018 - Ensemble of machine learning algorithms using the stacked generalization approach to estimate the warfarin dose.pdf},
  journal = {PLoS ONE},
  number = {10}
}

@article{maEnsembleMachineLearning2018b,
  title = {Ensemble of Machine Learning Algorithms Using the Stacked Generalization Approach to Estimate the Warfarin Dose},
  author = {Ma, Zhiyuan and Wang, Ping and Gao, Zehui and Wang, Ruobing and Khalighi, Koroush},
  year = {2018},
  volume = {13},
  pages = {1--12},
  issn = {1111111111},
  doi = {10.1371/journal.pone.0205872},
  abstract = {Warfarin dosing remains challenging due to narrow therapeutic index and highly individual variability. Incorrect warfarin dosing is associated with devastating adverse events. Remarkable efforts have been made to develop the machine learning based warfarin dosing algorithms incorporating clinical factors and genetic variants such as polymorphisms in CYP2C9 and VKORC1. The most widely validated pharmacogenetic algorithm is the IWPC algorithm based on multivariate linear regression (MLR). However, with only a single algorithm, the prediction performance may reach an upper limit even with optimal parameters. Here, we present novel algorithms using stacked generalization frameworks to estimate the warfarin dose, within which different types of machine learning algorithms function together through a meta-machine learning model to maximize the prediction accuracy. Compared to the IWPC-derived MLR algorithm, Stack 1 and 2 based on stacked generalization frameworks performed significantly better overall. Subgroup analysis revealed that the mean of the percentage of patients whose predicted dose of warfarin within 20\% of the actual stable therapeutic dose (mean percentage within 20\%) for Stack 1 was improved by 12.7\% (from 42.47\% to 47.86\%) in Asians and by 13.5\% (from 22.08\% to 25.05\%) in the low-dose group compared to that for MLR, respectively. These data suggest that our algorithms would especially benefit patients requiring low warfarin maintenance dose, as subtle changes in warfarin dose could lead to adverse clinical events (thrombosis or bleeding) in patients with low dose. Our study offers novel pharmacogenetic algorithms for clinical trials and practice.},
  file = {/Users/salvatorykessy/Zotero/storage/5Y9VV4Q6/Ma et al. - 2018 - Ensemble of machine learning algorithms using the stacked generalization approach to estimate the warfarin dose.pdf},
  journal = {PLoS ONE},
  number = {10}
}

@article{magnuson-skeelsUsingMachineLearning2016,
  title = {Using {{Machine Learning}} to {{Statistically Predict Natural Flow}}: {{The Sacramento Watershed}} under {{Dry Conditions}}},
  author = {{Magnuson-Skeels}, Bonnie Rose},
  year = {2016},
  pages = {86--86},
  abstract = {Committee in Charge 2016 ii ABSTRACT Machine learning techniques were applied to climatic, geologic, and geographic data to statistically model natural river flows in dry years in California's Intermountain and Xeric ecoregions. The model is tailored to predict flows during dry years for use as inputs for the Sacramento River version of the Drought Water Rights Allocation Model (DWRAT), a water rights curtailment model developed at the University of California, Davis. The modeling approach builds on a general-purpose statistical model developed by the US Geological Survey designed to predict natural flows at national and regional scales. Multiple machine learning algorithms were applied, using different techniques to select variables and reduce dimensionality and restricting training data to drier years. The ability of the resulting models to predict flows in dry water years was evaluated with multiple test metrics. The new models consistently tested as well as or superior to the corresponding general-purpose models when used to predict dry year flows, and in some cases, they performed far better. This improvement in predicting natural flows in dry years allows for more accurate estimation of available water and should help make DWRAT more useful for informing water rights curtailment decisions. This research also provides a high-level Python package for easily exploring and evaluating various combinations of machine learning techniques.},
  file = {/Users/salvatorykessy/Zotero/storage/G3JDR7AL/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.ris}
}

@article{magnuson-skeelsUsingMachineLearning2016a,
  title = {Using {{Machine Learning}} to {{Statistically Predict Natural Flow}}: {{The Sacramento Watershed}} under {{Dry Conditions}}},
  author = {{Magnuson-Skeels}, Bonnie Rose},
  year = {2016},
  pages = {86--86},
  abstract = {Committee in Charge 2016 ii ABSTRACT Machine learning techniques were applied to climatic, geologic, and geographic data to statistically model natural river flows in dry years in California's Intermountain and Xeric ecoregions. The model is tailored to predict flows during dry years for use as inputs for the Sacramento River version of the Drought Water Rights Allocation Model (DWRAT), a water rights curtailment model developed at the University of California, Davis. The modeling approach builds on a general-purpose statistical model developed by the US Geological Survey designed to predict natural flows at national and regional scales. Multiple machine learning algorithms were applied, using different techniques to select variables and reduce dimensionality and restricting training data to drier years. The ability of the resulting models to predict flows in dry water years was evaluated with multiple test metrics. The new models consistently tested as well as or superior to the corresponding general-purpose models when used to predict dry year flows, and in some cases, they performed far better. This improvement in predicting natural flows in dry years allows for more accurate estimation of available water and should help make DWRAT more useful for informing water rights curtailment decisions. This research also provides a high-level Python package for easily exploring and evaluating various combinations of machine learning techniques.},
  file = {/Users/salvatorykessy/Zotero/storage/8ZSKP8IM/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/HF46MGQ4/superior.pdf}
}

@article{magnuson-skeelsUsingMachineLearning2016b,
  title = {Using {{Machine Learning}} to {{Statistically Predict Natural Flow}}: {{The Sacramento Watershed}} under {{Dry Conditions}}},
  author = {{Magnuson-Skeels}, Bonnie Rose},
  year = {2016},
  pages = {86--86},
  abstract = {Committee in Charge 2016 ii ABSTRACT Machine learning techniques were applied to climatic, geologic, and geographic data to statistically model natural river flows in dry years in California's Intermountain and Xeric ecoregions. The model is tailored to predict flows during dry years for use as inputs for the Sacramento River version of the Drought Water Rights Allocation Model (DWRAT), a water rights curtailment model developed at the University of California, Davis. The modeling approach builds on a general-purpose statistical model developed by the US Geological Survey designed to predict natural flows at national and regional scales. Multiple machine learning algorithms were applied, using different techniques to select variables and reduce dimensionality and restricting training data to drier years. The ability of the resulting models to predict flows in dry water years was evaluated with multiple test metrics. The new models consistently tested as well as or superior to the corresponding general-purpose models when used to predict dry year flows, and in some cases, they performed far better. This improvement in predicting natural flows in dry years allows for more accurate estimation of available water and should help make DWRAT more useful for informing water rights curtailment decisions. This research also provides a high-level Python package for easily exploring and evaluating various combinations of machine learning techniques.}
}

@article{makridakisM3CompetitionResultsConclusions2000,
  title = {The {{M3}}-{{Competition}}: Results, Conclusions and Implications},
  shorttitle = {The {{M3}}-{{Competition}}},
  author = {Makridakis, Spyros and Hibon, Mich{\`e}le},
  year = {2000},
  month = oct,
  volume = {16},
  pages = {451--476},
  issn = {01692070},
  doi = {10.1016/S0169-2070(00)00057-1},
  abstract = {This paper describes the M3-Competition, the latest of the M-Competitions. It explains the reasons for conducting the competition and summarizes its results and conclusions. In addition, the paper compares such results / conclusions with those of the previous two M-Competitions as well as with those of other major empirical studies. Finally, the implications of these results and conclusions are considered, their consequences for both the theory and practice of forecasting are explored and directions for future research are contemplated. \textcopyright{} 2000 Elsevier Science B.V. All rights reserved.},
  file = {/Users/salvatorykessy/Zotero/storage/43EURDNT/Makridakis and Hibon - 2000 - The M3-Competition results, conclusions and impli.pdf},
  journal = {International Journal of Forecasting},
  language = {en},
  number = {4}
}

@article{makridakisM4Competition1002019,
  title = {The {{M4 Competition}}: 100,000 Time Series and 61 Forecasting Methods},
  shorttitle = {The {{M4 Competition}}},
  author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  year = {2019},
  month = jul,
  pages = {S0169207019301128},
  issn = {01692070},
  doi = {10.1016/j.ijforecast.2019.04.014},
  abstract = {The M4 Competition follows on from the three previous M competitions, the purpose of which was to learn from empirical evidence both how to improve the forecasting accuracy and how such learning could be used to advance the theory and practice of forecasting. The aim of M4 was to replicate and extend the three previous competitions by: (a) significantly increasing the number of series, (b) expanding the number of forecasting methods, and (c) including prediction intervals in the evaluation process as well as point forecasts. This paper covers all aspects of M4 in detail, including its organization and running, the presentation of its results, the top-performing methods overall and by categories, its major findings and their implications, and the computational requirements of the various methods. Finally, it summarizes its main conclusions and states the expectation that its series will become a testing ground for the evaluation of new methods and the improvement of the practice of forecasting, while also suggesting some ways forward for the field.},
  file = {/Users/salvatorykessy/Zotero/storage/9KMKW6HH/Makridakis et al. - 2019 - The M4 Competition 100,000 time series and 61 for.pdf},
  journal = {International Journal of Forecasting},
  language = {en}
}

@article{maSelectedStackingELMs2016,
  title = {Selected an {{Stacking ELMs}} for {{Time Series Prediction}}},
  author = {Ma, Zhongchen and Dai, Qun},
  year = {2016},
  volume = {44},
  pages = {831--856},
  issn = {1106301694999},
  doi = {10.1007/s11063-016-9499-9},
  abstract = {\textcopyright{} 2016, Springer Science+Business Media New York. Extreme learning machine (ELM) has several interesting and significant features. In this paper, a novel pruned Stacking ELMs (PS-ELMs) algorithm for time series prediction (TSP) is proposed. It employs ELM as the level-0 algorithm to train several models for Stacking. And our previously proposed reduce-error pruning for TSP (ReTSP)-Trend pruning technique is used to solve the problem that the level-0 learners might make many correlated error predictions. ReTSP-Trend refers to an evaluation measure for reduce-error pruning for TSP (ReTSP), which takes into account the time series trend and the forecasting error direction. What's more, ELM and simple averaging are used to generate the level-1 model. With the development of PS-ELMs, firstly, those essential advantages of ELM will be naturally inherited. Secondly, those specific defects of ELM are ameliorated to some extent, with the help of ensemble pruning paradigm. Thirdly, ensemble pruning is employed to raise the robustness and accuracy of time series forecasting, making up for the shortages of the existing research. Fourthly, our previously proposed pruning measure ReTSP-Trend is employed in PS-ELMs, which indeed guarantees that the remaining predictor which supplements the subensemble the most will be selected. And finally, the development of PS-ELMs will promote our investigation to the popular ensemble technique of Stacked Generalization. The experimental results on four benchmark financial time series datasets verified the validity of the proposed PS-ELMs algorithm.},
  file = {/Users/salvatorykessy/Zotero/storage/WB5ZVTI7/Ma, Dai - 2016 - Selected an Stacking ELMs for Time Series Prediction.pdf},
  journal = {Neural Processing Letters},
  keywords = {Extreme learning machine (ELM),Financial time series forecasting,Pruned Stacking extreme learning machines (PS-ELMs),Reduce-error pruning for time series prediction (ReTSP),ReTSP-Trend,Stacked Generalization (Stacking)},
  number = {3}
}

@article{maSelectedStackingELMs2016a,
  title = {Selected an {{Stacking ELMs}} for {{Time Series Prediction}}},
  author = {Ma, Zhongchen and Dai, Qun},
  year = {2016},
  volume = {44},
  pages = {831--856},
  issn = {1106301694999},
  doi = {10.1007/s11063-016-9499-9},
  abstract = {\textcopyright{} 2016, Springer Science+Business Media New York. Extreme learning machine (ELM) has several interesting and significant features. In this paper, a novel pruned Stacking ELMs (PS-ELMs) algorithm for time series prediction (TSP) is proposed. It employs ELM as the level-0 algorithm to train several models for Stacking. And our previously proposed reduce-error pruning for TSP (ReTSP)-Trend pruning technique is used to solve the problem that the level-0 learners might make many correlated error predictions. ReTSP-Trend refers to an evaluation measure for reduce-error pruning for TSP (ReTSP), which takes into account the time series trend and the forecasting error direction. What's more, ELM and simple averaging are used to generate the level-1 model. With the development of PS-ELMs, firstly, those essential advantages of ELM will be naturally inherited. Secondly, those specific defects of ELM are ameliorated to some extent, with the help of ensemble pruning paradigm. Thirdly, ensemble pruning is employed to raise the robustness and accuracy of time series forecasting, making up for the shortages of the existing research. Fourthly, our previously proposed pruning measure ReTSP-Trend is employed in PS-ELMs, which indeed guarantees that the remaining predictor which supplements the subensemble the most will be selected. And finally, the development of PS-ELMs will promote our investigation to the popular ensemble technique of Stacked Generalization. The experimental results on four benchmark financial time series datasets verified the validity of the proposed PS-ELMs algorithm.},
  file = {/Users/salvatorykessy/Zotero/storage/QJ6QIINS/Ma, Dai - 2016 - Selected an Stacking ELMs for Time Series Prediction.pdf},
  journal = {Neural Processing Letters},
  keywords = {Extreme learning machine (ELM),Financial time series forecasting,Pruned Stacking extreme learning machines (PS-ELMs),Reduce-error pruning for time series prediction (ReTSP),ReTSP-Trend,Stacked Generalization (Stacking)},
  number = {3}
}

@article{mathurMachineLearningApplications2018,
  title = {Machine {{Learning Applications Using Python}}},
  author = {Mathur, Puneet},
  year = {2018},
  doi = {10.1007/978-1-4842-3787-8},
  journal = {Machine Learning Applications Using Python}
}

@article{mathurMachineLearningApplications2018a,
  title = {Machine {{Learning Applications Using Python}}},
  author = {Mathur, Puneet},
  year = {2018},
  doi = {10.1007/978-1-4842-3787-8},
  file = {/Users/salvatorykessy/Zotero/storage/5I3EQFQ6/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf},
  journal = {Machine Learning Applications Using Python}
}

@article{mathurMachineLearningApplications2018b,
  title = {Machine {{Learning Applications Using Python}}},
  author = {Mathur, Puneet},
  year = {2018},
  doi = {10.1007/978-1-4842-3787-8},
  journal = {Machine Learning Applications Using Python}
}

@article{mathurMachineLearningApplications2018c,
  title = {Machine {{Learning Applications Using Python}}},
  author = {Mathur, Puneet},
  year = {2018},
  doi = {10.1007/978-1-4842-3787-8},
  file = {/Users/salvatorykessy/Zotero/storage/ZZ7BGE9U/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.ris},
  journal = {Machine Learning Applications Using Python}
}

@article{mendes-moreiraEnsembleApproachesRegression2012,
  title = {Ensemble Approaches for Regression: {{A}} Survey},
  shorttitle = {Ensemble Approaches for Regression},
  author = {{Mendes-Moreira}, Jo{\~a}o and Soares, Carlos and Jorge, Al{\'i}pio M{\'a}rio and Sousa, Jorge Freire De},
  year = {2012},
  month = nov,
  volume = {45},
  pages = {1--40},
  issn = {03600300},
  doi = {10.1145/2379776.2379786},
  file = {/Users/salvatorykessy/Zotero/storage/RLREGJYD/Mendes-Moreira et al. - 2012 - Ensemble approaches for regression A survey.pdf},
  journal = {ACM Computing Surveys},
  language = {en},
  number = {1}
}

@article{meyrickeInsuranceMathematicsEconomics2013,
  title = {Insurance : {{Mathematics}} and {{Economics The}} Determinants of Mortality Heterogeneity and Implications for Pricing Annuities},
  author = {Meyricke, Ramona and Sherris, Michael},
  year = {2013},
  volume = {53},
  pages = {379--387},
  doi = {10.1016/j.insmatheco.2013.06.002},
  file = {/Users/salvatorykessy/Zotero/storage/3TGS8CG3/Meyricke, Sherris - 2013 - Insurance Mathematics and Economics The determinants of mortality heterogeneity and implications for pricing.pdf},
  journal = {Insurance: Mathematics and Economics},
  keywords = {Annuity pricing,Frailty modelling,Generalized linear mixed models,Longitudinal data,mortality heterogeneity,Mortality heterogeneity},
  number = {2}
}

@article{meyrickeInsuranceMathematicsEconomics2013a,
  title = {Insurance : {{Mathematics}} and {{Economics The}} Determinants of Mortality Heterogeneity and Implications for Pricing Annuities},
  author = {Meyricke, Ramona and Sherris, Michael},
  year = {2013},
  volume = {53},
  pages = {379--387},
  doi = {10.1016/j.insmatheco.2013.06.002},
  file = {/Users/salvatorykessy/Zotero/storage/9GJY75N9/Meyricke, Sherris - 2013 - Insurance Mathematics and Economics The determinants of mortality heterogeneity and implications for pricing.pdf},
  journal = {Insurance: Mathematics and Economics},
  keywords = {Annuity pricing,Frailty modelling,Generalized linear mixed models,Longitudinal data,mortality heterogeneity,Mortality heterogeneity},
  number = {2}
}

@article{mirzaMachineLearningIntegrative2019,
  title = {Machine {{Learning}} and {{Integrative Analysis}} of {{Biomedical Big Data}}},
  author = {Mirza, Bilal and Wang, Wei and Wang, Jie and Choi, Howard and Chung, Neo Christopher and Ping, Peipei},
  year = {2019},
  volume = {10},
  pages = {87--87},
  doi = {10.3390/genes10020087},
  abstract = {Recent developments in high-throughput technologies have accelerated the accumulation of massive amounts of omics data from multiple sources: genome, epigenome, transcriptome, proteome, metabolome, etc. Traditionally, data from each source (e.g., genome) is analyzed in isolation using statistical and machine learning (ML) methods. Integrative analysis of multi-omics and clinical data is key to new biomedical discoveries and advancements in precision medicine. However, data integration poses new computational challenges as well as exacerbates the ones associated with single-omics studies. Specialized computational approaches are required to effectively and efficiently perform integrative analysis of biomedical data acquired from diverse modalities. In this review, we discuss state-of-the-art ML-based approaches for tackling five specific computational challenges associated with integrative analysis: curse of dimensionality, data heterogeneity, missing data, class imbalance and scalability issues.},
  journal = {Genes},
  keywords = {class imbalance,curse of dimensionality,data,data integration,heterogeneous,machine learning,missing data,multi-omics,scalability},
  number = {2}
}

@article{mirzaMachineLearningIntegrative2019a,
  title = {Machine {{Learning}} and {{Integrative Analysis}} of {{Biomedical Big Data}}},
  author = {Mirza, Bilal and Wang, Wei and Wang, Jie and Choi, Howard and Chung, Neo Christopher and Ping, Peipei},
  year = {2019},
  volume = {10},
  pages = {87--87},
  doi = {10.3390/genes10020087},
  abstract = {Recent developments in high-throughput technologies have accelerated the accumulation of massive amounts of omics data from multiple sources: genome, epigenome, transcriptome, proteome, metabolome, etc. Traditionally, data from each source (e.g., genome) is analyzed in isolation using statistical and machine learning (ML) methods. Integrative analysis of multi-omics and clinical data is key to new biomedical discoveries and advancements in precision medicine. However, data integration poses new computational challenges as well as exacerbates the ones associated with single-omics studies. Specialized computational approaches are required to effectively and efficiently perform integrative analysis of biomedical data acquired from diverse modalities. In this review, we discuss state-of-the-art ML-based approaches for tackling five specific computational challenges associated with integrative analysis: curse of dimensionality, data heterogeneity, missing data, class imbalance and scalability issues.},
  file = {/Users/salvatorykessy/Zotero/storage/IYPQBF2V/Mirza et al. - 2019 - Machine Learning and Integrative Analysis of Biome.pdf},
  journal = {Genes},
  keywords = {class imbalance,curse of dimensionality,data,data integration,heterogeneous,machine learning,missing data,multi-omics,scalability},
  number = {2}
}

@article{mirzaMachineLearningIntegrative2019b,
  title = {Machine {{Learning}} and {{Integrative Analysis}} of {{Biomedical Big Data}}},
  author = {Mirza, Bilal and Wang, Wei and Wang, Jie and Choi, Howard and Chung, Neo Christopher and Ping, Peipei},
  year = {2019},
  volume = {10},
  pages = {87--87},
  doi = {10.3390/genes10020087},
  abstract = {Recent developments in high-throughput technologies have accelerated the accumulation of massive amounts of omics data from multiple sources: genome, epigenome, transcriptome, proteome, metabolome, etc. Traditionally, data from each source (e.g., genome) is analyzed in isolation using statistical and machine learning (ML) methods. Integrative analysis of multi-omics and clinical data is key to new biomedical discoveries and advancements in precision medicine. However, data integration poses new computational challenges as well as exacerbates the ones associated with single-omics studies. Specialized computational approaches are required to effectively and efficiently perform integrative analysis of biomedical data acquired from diverse modalities. In this review, we discuss state-of-the-art ML-based approaches for tackling five specific computational challenges associated with integrative analysis: curse of dimensionality, data heterogeneity, missing data, class imbalance and scalability issues.},
  journal = {Genes},
  keywords = {class imbalance,curse of dimensionality,data,data integration,heterogeneous,machine learning,missing data,multi-omics,scalability},
  number = {2}
}

@article{ModelConfidenceSet2011,
  title = {The {{Model Confidence Set}}},
  year = {2011},
  volume = {79},
  pages = {453--497},
  issn = {0012-9682},
  doi = {10.3982/ECTA5771},
  file = {/Users/salvatorykessy/Zotero/storage/9BM9GQ37/2011 - The Model Confidence Set.pdf},
  journal = {Econometrica},
  language = {en},
  number = {2}
}

@article{modellingResearchMotivationStatement,
  title = {Research {{Motivation Statement}}},
  author = {Modelling, Longevity},
  pages = {17--18},
  file = {/Users/salvatorykessy/Zotero/storage/N6IELFLK/Modelling - Unknown - Research Motivation Statement.pdf}
}

@article{modellingResearchMotivationStatementa,
  title = {Research {{Motivation Statement}}},
  author = {Modelling, Longevity},
  pages = {17--18},
  file = {/Users/salvatorykessy/Zotero/storage/5IA69AC9/Modelling - Unknown - Research Motivation Statement.pdf}
}

@article{naimiStackedGeneralizationIntroduction2018,
  title = {Stacked Generalization: An Introduction to Super Learning},
  author = {Naimi, Ashley I and Balzer, Laura B},
  year = {2018},
  volume = {33},
  pages = {459--464},
  doi = {10.1007/s10654-018-0390-z},
  abstract = {Stacked generalization is an ensemble method that allows researchers to combine several different prediction algorithms into one. Since its introduction in the early 1990s, the method has evolved several times into what is now known as \&quot;Super Learner\&quot;. Super Learner uses V-fold cross-validation to build the optimal weighted combination of predictions from a library of candidate algorithms. Optimality is defined by a user-specified objective function, such as minimizing mean squared error or maximizing the area under the receiver operating characteristic curve. Although relatively simple in nature, use of the Super Learner by epidemiologists has been hampered by limitations in understanding conceptual and technical details. We work step-by-step through two examples to illustrate concepts and address common concerns.},
  file = {/Users/salvatorykessy/Zotero/storage/P3HN5WVV/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.ris},
  journal = {European Journal of Epidemiology},
  keywords = {Ensemble learning,Machine learning,Stacked generalization,Stacked regression,Super Learner},
  number = {5}
}

@article{naimiStackedGeneralizationIntroduction2018a,
  title = {Stacked Generalization: An Introduction to Super Learning},
  author = {Naimi, Ashley I. and Balzer, Laura B.},
  year = {2018},
  volume = {33},
  pages = {459--464},
  doi = {10.1007/s10654-018-0390-z},
  abstract = {Stacked generalization is an ensemble method that allows researchers to combine several different prediction algorithms into one. Since its introduction in the early 1990s, the method has evolved several times into what is now known as \&quot;Super Learner\&quot;. Super Learner uses V-fold cross-validation to build the optimal weighted combination of predictions from a library of candidate algorithms. Optimality is defined by a user-specified objective function, such as minimizing mean squared error or maximizing the area under the receiver operating characteristic curve. Although relatively simple in nature, use of the Super Learner by epidemiologists has been hampered by limitations in understanding conceptual and technical details. We work step-by-step through two examples to illustrate concepts and address common concerns.},
  file = {/Users/salvatorykessy/Zotero/storage/G6IRUQW3/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf},
  journal = {European Journal of Epidemiology},
  keywords = {Ensemble learning,Machine learning,Stacked generalization,Stacked regression,Super Learner},
  number = {5}
}

@article{naimiStackedGeneralizationIntroduction2018b,
  title = {Stacked Generalization: An Introduction to Super Learning},
  author = {Naimi, Ashley I. and Balzer, Laura B.},
  year = {2018},
  volume = {33},
  pages = {459--464},
  doi = {10.1007/s10654-018-0390-z},
  abstract = {Stacked generalization is an ensemble method that allows researchers to combine several different prediction algorithms into one. Since its introduction in the early 1990s, the method has evolved several times into what is now known as \&quot;Super Learner\&quot;. Super Learner uses V-fold cross-validation to build the optimal weighted combination of predictions from a library of candidate algorithms. Optimality is defined by a user-specified objective function, such as minimizing mean squared error or maximizing the area under the receiver operating characteristic curve. Although relatively simple in nature, use of the Super Learner by epidemiologists has been hampered by limitations in understanding conceptual and technical details. We work step-by-step through two examples to illustrate concepts and address common concerns.},
  journal = {European Journal of Epidemiology},
  keywords = {Ensemble learning,Machine learning,Stacked generalization,Stacked regression,Super Learner},
  number = {5}
}

@article{neuroscienceBrainActivityInherited2017,
  title = {Brain Activity Is Inherited , May Inform Treatment for {{ADHD}} , Autism},
  author = {Neuroscience, Network},
  year = {2017},
  pages = {1--2},
  doi = {10.1162/NETN},
  keywords = {ensemble learning,information theory,network analysis,Network analysis; Network motifs; Simulation and m,network motifs,simulation and modeling,synaptic connectivity}
}

@article{neuroscienceBrainActivityInherited2017a,
  title = {Brain Activity Is Inherited , May Inform Treatment for {{ADHD}} , Autism},
  author = {Neuroscience, Network},
  year = {2017},
  pages = {1--2},
  doi = {10.1162/NETN},
  keywords = {ensemble learning,information theory,network analysis,Network analysis; Network motifs; Simulation and m,network motifs,simulation and modeling,synaptic connectivity}
}

@article{neuroscienceBrainActivityInherited2017b,
  title = {Brain Activity Is Inherited , May Inform Treatment for {{ADHD}} , Autism},
  author = {Neuroscience, Network},
  year = {2017},
  pages = {1--2},
  doi = {10.1162/NETN},
  keywords = {ensemble learning,information theory,network analysis,Network analysis; Network motifs; Simulation and m,network motifs,simulation and modeling,synaptic connectivity}
}

@article{nigriDeepLearningIntegrated2019,
  title = {A Deep Learning Integrated Lee\textendash Carter Model},
  author = {Nigri, Andrea and Levantesi, Susanna and Marino, Mario and Scognamiglio, Salvatore and Perla, Francesca},
  year = {2019},
  volume = {7},
  pages = {1--16},
  doi = {10.3390/risks7010033},
  abstract = {In the field of mortality, the Lee\textendash Carter based approach can be considered the milestone to forecast mortality rates among stochastic models. We could define a ``Lee\textendash Carter model family'' that embraces all developments of this model, including its first formulation (1992) that remains the benchmark for comparing the performance of future models. In the Lee\textendash Carter model, the {$\kappa$} t parameter, describing the mortality trend over time, plays an important role about the future mortality behavior. The traditional ARIMA process usually used to model {$\kappa$} t shows evident limitations to describe the future mortality shape. Concerning forecasting phase, academics should approach a more plausible way in order to think a nonlinear shape of the projected mortality rates. Therefore, we propose an alternative approach the ARIMA processes based on a deep learning technique. More precisely, in order to catch the pattern of {$\kappa$} t series over time more accurately, we apply a Recurrent Neural Network with a Long Short-Term Memory architecture and integrate the Lee\textendash Carter model to improve its predictive capacity. The proposed approach provides significant performance in terms of predictive accuracy and also allow for avoiding the time-chunks' a priori selection. Indeed, it is a common practice among academics to delete the time in which the noise is overflowing or the data quality is insufficient. The strength of the Long Short-Term Memory network lies in its ability to treat this noise and adequately reproduce it into the forecasted trend, due to its own architecture enabling to take into account significant long-term patterns.},
  file = {/Users/salvatorykessy/Zotero/storage/9IX3ZPLF/Nigri et al. - 2019 - A deep learning integrated lee–carter model.pdf},
  journal = {Risks},
  keywords = {Deep learning,Forecasting,Lee–Carter model,Long short-term memory,Mortality},
  number = {1}
}

@article{nigriDeepLearningIntegrated2019a,
  title = {A Deep Learning Integrated Lee\textendash Carter Model},
  author = {Nigri, Andrea and Levantesi, Susanna and Marino, Mario and Scognamiglio, Salvatore and Perla, Francesca},
  year = {2019},
  volume = {7},
  pages = {1--16},
  doi = {10.3390/risks7010033},
  abstract = {In the field of mortality, the Lee\textendash Carter based approach can be considered the milestone to forecast mortality rates among stochastic models. We could define a ``Lee\textendash Carter model family'' that embraces all developments of this model, including its first formulation (1992) that remains the benchmark for comparing the performance of future models. In the Lee\textendash Carter model, the {$\kappa$} t parameter, describing the mortality trend over time, plays an important role about the future mortality behavior. The traditional ARIMA process usually used to model {$\kappa$} t shows evident limitations to describe the future mortality shape. Concerning forecasting phase, academics should approach a more plausible way in order to think a nonlinear shape of the projected mortality rates. Therefore, we propose an alternative approach the ARIMA processes based on a deep learning technique. More precisely, in order to catch the pattern of {$\kappa$} t series over time more accurately, we apply a Recurrent Neural Network with a Long Short-Term Memory architecture and integrate the Lee\textendash Carter model to improve its predictive capacity. The proposed approach provides significant performance in terms of predictive accuracy and also allow for avoiding the time-chunks' a priori selection. Indeed, it is a common practice among academics to delete the time in which the noise is overflowing or the data quality is insufficient. The strength of the Long Short-Term Memory network lies in its ability to treat this noise and adequately reproduce it into the forecasted trend, due to its own architecture enabling to take into account significant long-term patterns.},
  file = {/Users/salvatorykessy/Zotero/storage/NJIHXN8N/Nigri et al. - 2019 - A deep learning integrated lee–carter model.pdf},
  journal = {Risks},
  keywords = {Deep learning,Forecasting,Lee–Carter model,Long short-term memory,Mortality},
  number = {1}
}

@article{oeppenBrokenLimitsLife2002,
  title = {Broken {{Limits}} to {{Life Expectancy}}},
  author = {Oeppen, James and Vaupel, James},
  year = {2002},
  month = jan,
  volume = {296},
  journal = {Science, v.296, 1029-1031 (2002)}
}

@article{oeppenCoherentForecastingMultipledecrement2008,
  title = {Coherent Forecasting of Multiple-Decrement Life Tables: A Test Using {{Japanese}} Cause of Death Data.},
  author = {Oeppen, Jim},
  year = {2008},
  pages = {23},
  abstract = {Planners in public and private institutions would like coherent forecasts of the components of age-specific mortality, such as causes of death. This has been di cult to achieve because the relative values of the forecast components often fail to behave in a way that is coherent with historical experience. In addition, when the group forecasts are combined the result is often incompatible with an all-groups forecast. It has been shown that cause-specific mortality forecasts are pessimistic when compared with all-cause forecasts (Wilmoth, 1995). This paper abandons the conventional approach of using log mortality rates and forecasts the density of deaths in the life table. Since these values obey a unit sum constraint for both conventional single-decrement life tables (only one absorbing state) and multiple-decrement tables (more than one absorbing state), they are intrinsically relative rather than absolute values across decrements as well as ages. Using the methods of Compositional Data Analysis pioneered by Aitchison (1986), death densities are transformed into the real space so that the full range of multivariate statistics can be applied, then back-transformed to positive values so that the unit sum constraint is honoured. The structure of the best-known, single-decrement mortality-rate forecasting model, devised by Lee and Carter (1992), is expressed in compositional form and the results from the two models are compared. The compositional model is extended to a multiple-decrement form and used to forecast mortality by cause of death for Japan.},
  file = {/Users/salvatorykessy/Zotero/storage/ZZK7FHD7/Oeppen - Coherent forecasting of multiple-decrement life ta.pdf},
  language = {en}
}

@article{oeppenCoherentForecastingMultipledecrement2008a,
  title = {Coherent Forecasting of Multiple-Decrement Life Tables: A Test Using {{Japanese}} Cause of Death Data.},
  author = {Oeppen, Jim},
  year = {‎2008},
  pages = {23},
  abstract = {Planners in public and private institutions would like coherent forecasts of the components of age-specific mortality, such as causes of death. This has been difficult to achieve because the relative values of the forecast components often fail to behave in a way that is coherent with historical experience. In addition, when the group forecasts are combined the result is often incompatible with an all-groups forecast. It has been shown that cause-specific mortality forecasts are pessimistic when compared with all-cause forecasts (Wilmoth, 1995). This paper abandons the conventional approach of using log mortality rates and forecasts the density of deaths in the life table. Since these values obey a unit sum constraint for both conventional single-decrement life tables (only one absorbing state) and multiple-decrement tables (more than one absorbing state), they are intrinsically relative rather than absolute values across decrements as well as ages. Using the methods of Compositional Data Analysis pioneered by Aitchison (1986), death densities are transformed into the real space so that the full range of multivariate statistics can be applied, then back-transformed to positive values so that the unit sum constraint is honoured. The structure of the best-known, single-decrement mortality-rate forecasting model, devised by Lee and Carter (1992), is expressed in compositional form and the results from the two models are compared. The compositional model is extended to a multiple-decrement form and used to forecast mortality by cause of death for Japan.},
  file = {/Users/salvatorykessy/Zotero/storage/UAGEKV69/Oeppen - Coherent forecasting of multiple-decrement life ta.pdf},
  language = {en}
}

@article{palaniyammalPerformanceEvaluationData2018,
  title = {Performance and {{Evaluation}} of {{Data Mining Ensemble Classifiers}}},
  author = {Palaniyammal, V},
  year = {2018},
  pages = {6--9},
  file = {/Users/salvatorykessy/Zotero/storage/582WKD7X/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.ris},
  keywords = {boosting,classifiers bagging,data mining weka ensemble,stacking wbcd set}
}

@article{palaniyammalPerformanceEvaluationData2018a,
  title = {Performance and {{Evaluation}} of {{Data Mining Ensemble Classifiers}}},
  author = {Palaniyammal, V},
  year = {2018},
  pages = {6--9},
  file = {/Users/salvatorykessy/Zotero/storage/XF3GMMAR/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf},
  keywords = {boosting,classifiers bagging,data mining weka ensemble,stacking wbcd set}
}

@article{palaniyammalPerformanceEvaluationData2018b,
  title = {Performance and {{Evaluation}} of {{Data Mining Ensemble Classifiers}}},
  author = {Palaniyammal, V},
  year = {2018},
  pages = {6--9},
  keywords = {boosting,classifiers bagging,data mining weka ensemble,stacking wbcd set}
}

@article{pampelForecastingSexDifferences2005,
  title = {Forecasting Sex Differences in Mortality in High Income Nations: {{The}} Contribution of Smoking},
  shorttitle = {Forecasting Sex Differences in Mortality in High Income Nations},
  author = {Pampel, Fred},
  year = {2005},
  month = nov,
  volume = {13},
  pages = {455--484},
  issn = {1435-9871},
  doi = {10.4054/DemRes.2005.13.18},
  abstract = {To address the question of whether sex differences in mortality will in the future rise, fall, or stay the same, this study uses relative smoking prevalence among males and females to forecast future changes in relative smoking-attributed mortality. Data on 21 high income nations from 1975 to 2000 and a lag between smoking prevalence and mortality allow forecasts up to 2020. Averaged across nations, the results for logged male/female ratios in smoking mortality reveal equalization of the sex differential. However, continued divergence in non-smoking mortality rates would counter convergence in smoking mortality rates and lead to future increases in the female advantage overall, particularly in nations at late stages of the cigarette epidemic (such as the United States and the United Kingdom).},
  file = {/Users/salvatorykessy/Zotero/storage/EWTJFICY/Pampel - 2005 - Forecasting sex differences in mortality in high i.pdf},
  journal = {Demographic Research},
  language = {en}
}

@article{pangFuzzyKprototypeClustering2013,
  title = {A Fuzzy K-Prototype Clustering Algorithm for Mixed Numeric and Categorical Data},
  author = {Pang, Wei},
  year = {2013},
  doi = {10.1016/j.knosys.2012.01.006},
  file = {/Users/salvatorykessy/Zotero/storage/XW4STLK5/Pang - 2013 - A fuzzy k-prototype clustering algorithm for mixed numeric and categorical data.pdf},
  number = {November}
}

@article{Paper1Pdf,
  title = {Paper1.Pdf},
  file = {/Users/salvatorykessy/Zotero/storage/GXRHVX64/Unknown - Unknown - paper1.pdf.pdf}
}

@article{Paper1Pdfa,
  title = {Paper1.Pdf},
  file = {/Users/salvatorykessy/Zotero/storage/M9TH5ASQ/Unknown - Unknown - paper1.pdf.pdf}
}

@article{Paper1Pdfb,
  title = {Paper1.Pdf},
  file = {/Users/salvatorykessy/Zotero/storage/4Q7D469K/Unknown - Unknown - paper1.pdf.pdf}
}

@article{Paper4Pdf,
  title = {Paper4.Pdf},
  file = {/Users/salvatorykessy/Zotero/storage/5MPYBYRY/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.ris}
}

@article{Paper4Pdfa,
  title = {Paper4.Pdf},
  file = {/Users/salvatorykessy/Zotero/storage/9ZW5IULT/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf}
}

@article{Paper4Pdfb,
  title = {Paper4.Pdf}
}

@article{Paper5Pdf,
  title = {Paper5.Pdf},
  file = {/Users/salvatorykessy/Zotero/storage/D8MMRVAN/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf}
}

@article{Paper5Pdfa,
  title = {Paper5.Pdf}
}

@article{Paper5Pdfb,
  title = {Paper5.Pdf},
  file = {/Users/salvatorykessy/Zotero/storage/53G6VIT3/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.ris}
}

@article{pilatowskaCombinedForecastsUsing2009,
  title = {Combined {{Forecasts Using}} the {{Akaike Weights}}},
  author = {Pi{\l}atowska, Mariola},
  year = {2009},
  month = jul,
  volume = {9},
  pages = {5},
  issn = {1234-3862},
  doi = {10.12775/DEM.2009.001},
  file = {/Users/salvatorykessy/Zotero/storage/TABTSIP3/Piłatowska - 2009 - Combined Forecasts Using the Akaike Weights.pdf},
  journal = {Dynamic Econometric Models},
  language = {en},
  number = {0}
}

@article{piscopoApplyingSpectralBiclustering2017,
  title = {Applying Spectral Biclustering to Mortality Data},
  author = {Piscopo, Gabriella and Resta, Marina},
  year = {2017},
  volume = {5},
  pages = {24--24},
  issn = {3901020950},
  doi = {10.3390/risks5020024},
  file = {/Users/salvatorykessy/Zotero/storage/3FWBYNPT/Piscopo, Resta - 2017 - Applying spectral biclustering to mortality data.pdf},
  journal = {Risks},
  keywords = {biclustering,cohort effect,mortality data},
  number = {2}
}

@article{platStochasticMortalityModeling2009,
  title = {On Stochastic Mortality Modeling},
  author = {Plat, Richard},
  year = {2009},
  volume = {45},
  pages = {393--404},
  doi = {10.1016/j.insmatheco.2009.08.006},
  abstract = {In the last decennium a vast literature on stochastic mortality models has been developed. All well-known models have nice features but also disadvantages. In this paper a stochastic mortality model is proposed that aims at combining the nice features from the existing models, while eliminating the disadvantages. More specifically, the model fits historical data very well, is applicable to a full age range, captures the cohort effect, has a non-trivial (but not too complex) correlation structure and has no robustness problems, while the structure of the model remains relatively simple. Also, the paper describes how to incorporate parameter uncertainty in the model. Furthermore, a risk neutral version of the model is given, that can be used for pricing. \textcopyright{} 2009 Elsevier B.V. All rights reserved.},
  file = {/Users/salvatorykessy/Zotero/storage/PFHPQ94J/Plat - 2009 - On stochastic mortality modeling(2).pdf},
  journal = {Insurance: Mathematics and Economics},
  keywords = {Longevity risk,Monte Carlo simulation,Pricing,Solvency 2,Stochastic mortality models},
  number = {3}
}

@article{platStochasticMortalityModeling2009a,
  title = {On Stochastic Mortality Modeling},
  author = {Plat, Richard},
  year = {2009},
  volume = {45},
  pages = {393--404},
  doi = {10.1016/j.insmatheco.2009.08.006},
  abstract = {In the last decennium a vast literature on stochastic mortality models has been developed. All well-known models have nice features but also disadvantages. In this paper a stochastic mortality model is proposed that aims at combining the nice features from the existing models, while eliminating the disadvantages. More specifically, the model fits historical data very well, is applicable to a full age range, captures the cohort effect, has a non-trivial (but not too complex) correlation structure and has no robustness problems, while the structure of the model remains relatively simple. Also, the paper describes how to incorporate parameter uncertainty in the model. Furthermore, a risk neutral version of the model is given, that can be used for pricing. \textcopyright{} 2009 Elsevier B.V. All rights reserved.},
  file = {/Users/salvatorykessy/Zotero/storage/XIIXBIVX/Plat - 2009 - On stochastic mortality modeling(2).pdf},
  journal = {Insurance: Mathematics and Economics},
  keywords = {Longevity risk,Monte Carlo simulation,Pricing,Solvency 2,Stochastic mortality models},
  number = {3}
}

@article{polleyUniversityCaliforniaBerkeley2010,
  title = {University of {{California}} , {{Berkeley Super Learner In Prediction}}},
  author = {Polley, Eric C and van der Laan, Mark J.},
  year = {2010},
  file = {/Users/salvatorykessy/Zotero/storage/VYQ4IXLE/Polley, Laan - 2010 - University of California , Berkeley Super Learner In Prediction.pdf},
  journal = {UC Berkeley Working Paper Series},
  keywords = {cross-validation,estimator selection,machine learning,model assessment,model selection,Prediction}
}

@article{polleyUniversityCaliforniaBerkeley2010a,
  title = {University of {{California}} , {{Berkeley Super Learner In Prediction}}},
  author = {Polley, Eric C and van der Laan, Mark J.},
  year = {2010},
  file = {/Users/salvatorykessy/Zotero/storage/HZ3FEYIR/Polley, Laan - 2010 - University of California , Berkeley Super Learner In Prediction.pdf},
  journal = {UC Berkeley Working Paper Series},
  keywords = {cross-validation,estimator selection,machine learning,model assessment,model selection,Prediction}
}

@article{polleyUniversityCaliforniaBerkeley2010b,
  title = {University of {{California}} , {{Berkeley Super Learner In Prediction}}},
  author = {Polley, Eric C and van der Laan, Mark J.},
  year = {2010},
  file = {/Users/salvatorykessy/Zotero/storage/5YMIMBJ5/Polley, Laan - 2010 - University of California , Berkeley Super Learner In Prediction(4).pdf},
  journal = {UC Berkeley Working Paper Series},
  keywords = {cross-validation,estimator selection,machine learning,model assessment,model selection,Prediction}
}

@article{prudencioMetalearningApproachesSelecting2004,
  title = {Meta-Learning Approaches to Selecting Time Series Models},
  author = {Prud{\^e}ncio, Ricardo B.C. and Ludermir, Teresa B.},
  year = {2004},
  month = oct,
  volume = {61},
  pages = {121--137},
  issn = {09252312},
  doi = {10.1016/j.neucom.2004.03.008},
  abstract = {We present here an original work that applies meta-learning approaches to select models for time-series forecasting. In our work, we investigated two meta-learning approaches, each one used in a di erent case study. Initially, we used a single machine learning algorithm to select among two models to forecast stationary time series (case study I). Following, we used the NOEMON approach, a more recent work in the meta-learning area, to rank three models used to forecast time series of the M3-Competition (case study II). The experiments performed in both case studies revealed encouraging results.},
  file = {/Users/salvatorykessy/Zotero/storage/GNDYEJFY/Prudêncio and Ludermir - 2004 - Meta-learning approaches to selecting time series .pdf},
  journal = {Neurocomputing},
  language = {en}
}

@article{puurulaKaggleLSHTC4Winning2014,
  title = {Kaggle {{LSHTC4 Winning Solution}}},
  author = {Puurula, Antti and Read, Jesse and Bifet, Albert},
  year = {2014},
  month = may,
  abstract = {Our winning submission to the 2014 Kaggle competition for Large Scale Hierarchical Text Classification (LSHTC) consists mostly of an ensemble of sparse generative models extending Multinomial Naive Bayes. The base-classifiers consist of hierarchically smoothed models combining document, label, and hierarchy level Multinomials, with feature pre-processing using variants of TF-IDF and BM25. Additional diversification is introduced by different types of folds and random search optimization for different measures. The ensemble algorithm optimizes macroFscore by predicting the documents for each label, instead of the usual prediction of labels per document. Scores for documents are predicted by weighted voting of base-classifier outputs with a variant of Feature-Weighted Linear Stacking. The number of documents per label is chosen using label priors and thresholding of vote scores. This document describes the models and software used to build our solution. Reproducing the results for our solution can be done by running the scripts included in the Kaggle package. A package omitting precomputed result files is also distributed. All code is open source, released under GNU GPL 2.0, and GPL 3.0 for Weka and Meka dependencies.},
  annote = {Comment: Kaggle LSHTC winning solution description},
  archivePrefix = {arXiv},
  eprint = {1405.0546},
  eprinttype = {arxiv},
  file = {/Users/salvatorykessy/Zotero/storage/8FI3DA3X/Puurula et al. - 2014 - Kaggle LSHTC4 Winning Solution.pdf},
  journal = {arXiv:1405.0546 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  language = {en},
  primaryClass = {cs}
}

@article{rabbiMortalityLifeExpectancy2018,
  title = {Mortality and Life Expectancy Forecast for (Comparatively) High Mortality Countries},
  author = {Rabbi, Ahbab Mohammad Fazle and Mazzuco, Stefano},
  year = {2018},
  month = dec,
  volume = {74},
  pages = {18},
  issn = {2035-5556},
  doi = {10.1186/s41118-018-0042-x},
  abstract = {Background: The Lee\textendash Carter method and its later variants are widely accepted extrapolative methods for forecasting mortality and life expectancy in industrial countries due to their simplicity and availability of high quality long time series data.
Objective: We compared and contrasted mortality forecasting models for higher mortality regimes that lack long time series data of good quality, which is common in several Central and Eastern European (CEE) countries. Data and methods: We utilized seven different variants of the Lee\textendash Carter method and coherent mortality forecasts of various CEE countries, and the Bayesian Hierarchical Model used by the United Nations to produce probabilistic forecasts. The data of nine CEE countries with comparatively higher mortality have been considered.
Results: The performance of the forecasting models for the nine CEE countries was found to be lower than that observed for low-mortality countries. No model gives uniquely best performance for all the nine CEE countries. Most of the LC variants produced lower forecasts of life expectancies than current life expectancy values for Belarus, Russia, and Ukraine. A coherent mortality forecast could not overcome the limitations of single population forecasting techniques due to increasing mortality differences between these countries over the fitting period (mortality divergence). In the same context, the use of the probabilistic forecasting technique from the Bayesian framework resulted in a better forecast than some of the extrapolative methods but also produced a wider prediction interval for several countries. The more detailed analysis for Hungary indicates that a better fit of certain forecasting methods may occur in the later part of the life span rather than the whole life span.
Conclusion: These findings imply the necessity of inventing a new forecasting technique for high-mortality countries.},
  file = {/Users/salvatorykessy/Zotero/storage/C3BPCSXS/Rabbi and Mazzuco - 2018 - Mortality and life expectancy forecast for (compar.pdf},
  journal = {Genus},
  language = {en},
  number = {1}
}

@article{rafteryBayesianProbabilisticProjections2013,
  title = {Bayesian {{Probabilistic Projections}} of {{Life Expectancy}} for {{All Countries}}},
  author = {Raftery, Adrian E. and Chunn, Jennifer L. and Gerland, Patrick and {\v S}ev{\v c}{\'i}kov{\'a}, Hana},
  year = {2013},
  month = jun,
  volume = {50},
  pages = {777--801},
  issn = {0070-3370, 1533-7790},
  doi = {10.1007/s13524-012-0193-x},
  abstract = {We propose a Bayesian hierarchical model for producing probabilistic forecasts of male period life expectancy at birth for all the countries of the world to 2100. Such forecasts would be an input to the production of probabilistic population projections for all countries, which is currently being considered by the United Nations. To evaluate the method, we conducted an out-of-sample cross-validation experiment, fitting the model to the data from 1950\textendash 1995 and using the estimated model to forecast for the subsequent 10 years. The 10-year predictions had a mean absolute error of about 1 year, about 40 \% less than the current UN methodology. The probabilistic forecasts were calibrated in the sense that, for example, the 80 \% prediction intervals contained the truth about 80 \% of the time. We illustrate our method with results from Madagascar (a typical country with steadily improving life expectancy), Latvia (a country that has had a mortality crisis), and Japan (a leading country). We also show aggregated results for South Asia, a region with eight countries. Free, publicly available R software packages called bayesLife and bayesDem are available to implement the method.},
  file = {/Users/salvatorykessy/Zotero/storage/DKL6L4Q7/Raftery et al. - 2013 - Bayesian Probabilistic Projections of Life Expecta.pdf},
  journal = {Demography},
  language = {en},
  number = {3}
}

@article{rayPredictionInfectiousDisease2018,
  title = {Prediction of Infectious Disease Epidemics via Weighted Density Ensembles},
  author = {Ray, Evan L. and Reich, Nicholas G.},
  year = {2018},
  month = feb,
  volume = {14},
  pages = {e1005910},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005910},
  abstract = {Accurate and reliable predictions of infectious disease dynamics can be valuable to public health organizations that plan interventions to decrease or prevent disease transmission. A great variety of models have been developed for this task, using different model structures, covariates, and targets for prediction. Experience has shown that the performance of these models varies; some tend to do better or worse in different seasons or at different points within a season. Ensemble methods combine multiple models to obtain a single prediction that leverages the strengths of each model. We considered a range of ensemble methods that each form a predictive density for a target of interest as a weighted sum of the predictive densities from component models. In the simplest case, equal weight is assigned to each component model; in the most complex case, the weights vary with the region, prediction target, week of the season when the predictions are made, a measure of component model uncertainty, and recent observations of disease incidence. We applied these methods to predict measures of influenza season timing and severity in the United States, both at the national and regional levels, using three component models. We trained the models on retrospective predictions from 14 seasons (1997/1998 - 2010/2011) and evaluated each model's prospective, out-of-sample performance in the five subsequent influenza seasons. In this test phase, the ensemble methods showed overall performance that was similar to the best of the component models, but offered more consistent performance across seasons than the component models. Ensemble methods offer the potential to deliver more reliable predictions to public health decision makers.},
  annote = {Comment: 20 pages, 6 figures},
  archivePrefix = {arXiv},
  eprint = {1703.10936},
  eprinttype = {arxiv},
  file = {/Users/salvatorykessy/Zotero/storage/AWNZI6I5/Ray and Reich - 2018 - Prediction of infectious disease epidemics via wei.pdf},
  journal = {PLOS Computational Biology},
  keywords = {Statistics - Machine Learning},
  language = {en},
  number = {2}
}

@article{renshawCohortbasedExtensionLeeCarter2006,
  title = {A Cohort-Based Extension to the {{Lee}}-{{Carter}} Model for Mortality Reduction Factors},
  author = {Renshaw, A. E. and Haberman, S.},
  year = {2006},
  volume = {38},
  pages = {556--570},
  doi = {10.1016/j.insmatheco.2005.12.001},
  abstract = {The Lee-Carter modelling framework is extended through the introduction of a wider class of generalised, parametric, non-linear models. This permits the modelling and extrapolation of age-specific cohort effects as well as the more familiar age-specific period effects. The choice of error distribution is generalised. \textcopyright{} 2005 Elsevier B.V. All rights reserved.},
  file = {/Users/salvatorykessy/Zotero/storage/8H4FSHQT/Renshaw, Haberman - 2006 - A cohort-based extension to the Lee-Carter model for mortality reduction factors.pdf},
  journal = {Insurance: Mathematics and Economics},
  keywords = {Cohort effects,Generalised non-linear models,Mortality projections,Mortality reduction factors,Time series},
  number = {3}
}

@article{renshawLeeCarterMortalityForecasting2003,
  title = {Lee-{{Carter}} Mortality Forecasting with Age-Specific Enhancement},
  author = {Renshaw, A. E. and Haberman, S.},
  year = {2003},
  volume = {33},
  pages = {255--272},
  journal = {Insurance: Mathematics and Economics},
  number = {2}
}

@article{richmanAIActuarialScience2018,
  title = {{{AI}} in {{Actuarial Science}}},
  author = {Richman, Ronald},
  year = {2018},
  pages = {24--25},
  doi = {10.2139/ssrn.3218082},
  abstract = {Rapid advances in Artificial Intelligence and Machine Learning are creating products and services with the potential not only to change the environment in which actuaries operate, but also to provide new opportunities within actuarial science. These advances are based on a modern approach to designing, fitting and applying neural networks, generally referred to as "Deep Learning". This paper investigates how actuarial science may adapt and evolve in the coming years to incorporate these new techniques and methodologies. After providing some background on machine learning and deep learning, and providing a heuristic for where actuaries might benefit from applying these techniques, the paper surveys emerging applications of artificial intelligence in actuarial science, with examples from mortality modelling, claims reserving, non-life pricing and telematics. For some of the examples, code has been provided on GitHub so that the interested reader can experiment with these techniques for themselves. The paper concludes with an outlook on the potential for actuaries to integrate deep learning into their activities.},
  file = {/Users/salvatorykessy/Zotero/storage/S58GS92P/Richman - 2018 - AI in Actuarial Science.pdf},
  journal = {SSRN Electronic Journal},
  keywords = {actuarial science,deep learning,insurance,machine learning,telematics},
  number = {October}
}

@article{richmanNeuralNetworkExtension2018,
  title = {A {{Neural Network Extension}} of the {{Lee}}-{{Carter Model}} to {{Multiple Populations}}},
  author = {Richman, Ronald and Wuthrich, Mario V.},
  year = {2018},
  pages = {1--21},
  doi = {10.2139/ssrn.3270877},
  file = {/Users/salvatorykessy/Zotero/storage/5NXG9E6G/Richman, Wuthrich - 2018 - A Neural Network Extension of the Lee-Carter Model to Multiple Populations.pdf},
  journal = {SSRN Electronic Journal},
  keywords = {lee-carter model,mortality forecasting,multiple populations,neural net-}
}

@book{rojasTimeSeriesAnalysis2016,
  title = {Time {{Series Analysis}} and {{Forecasting}}: {{Selected Contributions}} from the {{ITISE Conference}}},
  shorttitle = {Time {{Series Analysis}} and {{Forecasting}}},
  editor = {Rojas, Ignacio and Pomares, H{\'e}ctor},
  year = {2016},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-28725-6},
  file = {/Users/salvatorykessy/Zotero/storage/6YW4RLTI/Rojas and Pomares - 2016 - Time Series Analysis and Forecasting Selected Con.pdf},
  isbn = {978-3-319-28723-2 978-3-319-28725-6},
  language = {en},
  series = {Contributions to {{Statistics}}}
}

@article{rokachEnsembleMethodsClassifiers2006a,
  ids = {rokachEnsembleMethodsClassifiers2006,rokachEnsembleMethodsClassifiers2006b},
  title = {Ensemble {{Methods}} for {{Classifiers}}},
  author = {Rokach, Lior},
  year = {2006},
  pages = {957--980},
  doi = {10.1007/0-387-25465-x_45},
  abstract = {The idea of ensemble methodology is to build a predictive model by integrating multiple models. It is well-known that ensemble methods can be used for improving prediction performance. In this chapter we provide an overview of ensemble methods in classification tasks. We present all important types of ensemble method including boosting and bagging. Combining methods and modeling issues such as ensemble diversity and ensemble size are discussed.},
  journal = {Data Mining and Knowledge Discovery Handbook}
}

@article{roseMachineLearningMethods2014a,
  ids = {roseMachineLearningMethods2014,roseMachineLearningMethods2014b},
  title = {Machine {{Learning Methods}} for {{Prediction}} in {{Epidemiology}}},
  author = {Rose, Sherri and Einstein, Albert},
  year = {2014},
  pages = {3--4}
}

@article{roseMortalityRiskScore2013a,
  ids = {roseMortalityRiskScore2013,roseMortalityRiskScore2013b,roseMortalityRiskScore2013c,roseMortalityRiskScore2013d},
  title = {Mortality Risk Score Prediction in an Elderly Population Using Machine Learning},
  author = {Rose, Sherri},
  year = {2013},
  volume = {177},
  pages = {443--452},
  doi = {10.1093/aje/kws241},
  abstract = {Standard practice for prediction often relies on parametric regression methods. Interesting new methods from the machine learning literature have been introduced in epidemiologic studies, such as random forest and neural networks. However, a priori, an investigator will not know which algorithm to select and may wish to try several. Here I apply the super learner, an ensembling machine learning approach that combines multiple algorithms into a single algorithm and returns a prediction function with the best cross-validated mean squared error. Super learning is a generalization of stacking methods. I used super learning in the Study of Physical Performance and Age-Related Changes in Sonomans (SPPARCS) to predict death among 2,066 residents of Sonoma, California, aged 54 years or more during the period 1993-1999. The super learner for predicting death (risk score) improved upon all single algorithms in the collection of algorithms, although its performance was similar to that of several algorithms. Super learner outperformed the worst algorithm (neural networks) by 44\% with respect to estimated cross-validated mean squared error and had an R2 value of 0.201. The improvement of super learner over random forest with respect to R2 was approximately 2-fold. Alternatives for risk score prediction include the super learner, which can provide improved performance.},
  file = {/Users/salvatorykessy/Zotero/storage/D78MVFI8/CoDDifferentmethods.pdf;/Users/salvatorykessy/Zotero/storage/MQWV47SK/Rose - 2013 - Mortality risk score prediction in an elderly population using machine learning.pdf;/Users/salvatorykessy/Zotero/storage/RQGQ26NZ/Rose - 2013 - Mortality risk score prediction in an elderly population using machine learning.pdf;/Users/salvatorykessy/Zotero/storage/RZMZ87LU/Rose - 2013 - Mortality risk score prediction in an elderly popu.pdf},
  journal = {American Journal of Epidemiology},
  keywords = {aging,estimation techniques,machine learning,mortality,regression analysis},
  number = {5}
}

@article{rossiModelConfidenceSet2011,
  title = {The {{Model Confidence Set}}},
  author = {Rossi, Barbara and Stock, Jim and Wolf, Michael},
  year = {2011},
  volume = {79},
  pages = {453--497},
  doi = {10.3982/ecta5771},
  abstract = {This paper introduces the model confidence set (MCS) and applies it to the selection of models. An MCS is a set of models that is constructed so that it will contain the best model with a given level of confidence. The MCS is in this sense analogous to a confidence interval for a parameter. The MCS acknowledges the limitations of the data; uninformative data yield an MCS with many models whereas informative data yield an MCS with only a few models. The MCS procedure does not assume that a particular model is the true model; in fact, the MCS procedure can be used to compare more general objects, beyond the comparison of models. We apply the MCS procedure to two empirical problems. First, we revisit the inflation forecasting problem posed by Stock and Watson (1999) and compute the MCS for their set of inflation forecasts. Second, we compare a number of Taylor rule regressions and determine the MCS of the best in terms of in-sample likelihood criteria.},
  file = {/Users/salvatorykessy/Zotero/storage/3UGPBBP7/Rossi, Stock, Wolf - 2011 - The Model Confidence Set.pdf},
  journal = {Econometrica},
  number = {2}
}

@article{rossiModelConfidenceSet2011a,
  title = {The {{Model Confidence Set}}},
  author = {Rossi, Barbara and Stock, Jim and Wolf, Michael},
  year = {2011},
  volume = {79},
  pages = {453--497},
  doi = {10.3982/ecta5771},
  abstract = {This paper introduces the model confidence set (MCS) and applies it to the selection of models. An MCS is a set of models that is constructed so that it will contain the best model with a given level of confidence. The MCS is in this sense analogous to a confidence interval for a parameter. The MCS acknowledges the limitations of the data; uninformative data yield an MCS with many models whereas informative data yield an MCS with only a few models. The MCS procedure does not assume that a particular model is the true model; in fact, the MCS procedure can be used to compare more general objects, beyond the comparison of models. We apply the MCS procedure to two empirical problems. First, we revisit the inflation forecasting problem posed by Stock and Watson (1999) and compute the MCS for their set of inflation forecasts. Second, we compare a number of Taylor rule regressions and determine the MCS of the best in terms of in-sample likelihood criteria.},
  file = {/Users/salvatorykessy/Zotero/storage/RCG5PYCI/Rossi, Stock, Wolf - 2011 - The Model Confidence Set.pdf},
  journal = {Econometrica},
  number = {2}
}

@article{sauerPredictingDeathUsing,
  title = {Predicting {{Death Using Random Forests}}},
  author = {Sauer, Torsten and Rau, Roland},
  pages = {11},
  abstract = {Machine learning methods have become very popular in various scientific disciplines. Using Breimann's random forests and data from the National Health Interview Survey (NHIS) and its mortality follow-up, we wanted to know 1) Could these methods be used to predict the occurrence of death? 2) Which variables are important for these predictions? We checked the accuracy of the forests by estimating the area under the ROC curve (AUC) for test data and showed that they perform relatively well, with an AUC from 0.83 to 0.87. To indicate the predictive power of every variable we estimated the mean decrease in accuracy (MDA). Not surprisingly ''age'' is by far the most predictive, followed by ''mobility limitations'' and ''self-rated health''. Typical sociodemographic mortality determinants like ''sex'', ''education'', and ''income'' seem to be very weak in their predictive ability in each of the six selected intervals.},
  file = {/Users/salvatorykessy/Zotero/storage/AB32ER92/Sauer and Rau - Predicting Death Using Random Forests.pdf},
  language = {en}
}

@article{sauerPredictingDeathUsing2018,
  title = {Predicting {{Death Using Random Forests}}},
  author = {Sauer, Torsten and Rau, Roland},
  year = {2018},
  file = {/Users/salvatorykessy/Zotero/storage/TUE6R49P/Sauer, Rau - 2018 - Predicting Death Using Random Forests.pdf}
}

@article{sauerPredictingDeathUsing2018a,
  title = {Predicting {{Death Using Random Forests}}},
  author = {Sauer, Torsten and Rau, Roland},
  year = {2018},
  file = {/Users/salvatorykessy/Zotero/storage/KMBJN37N/Sauer, Rau - 2018 - Predicting Death Using Random Forests.pdf}
}

@article{schwarzEstimatingDimensionModel1978,
  title = {Estimating the {{Dimension}} of a {{Model}}},
  author = {Schwarz, Gideon},
  year = {1978},
  month = mar,
  volume = {6},
  pages = {461--464},
  issn = {0090-5364},
  doi = {10.1214/aos/1176344136},
  abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
  journal = {Ann. Statist.},
  keywords = {Akaike information criterion,asymptotics,Dimension},
  language = {en},
  number = {2}
}

@article{shangModelConfidenceSets2018,
  title = {Model Confidence Sets and Forecast Combination: An Application to Age-Specific Mortality},
  author = {Shang, Han Lin and Haberman, Steven},
  year = {2018},
  volume = {74},
  doi = {10.1186/s41118-018-0043-9},
  abstract = {Model averaging combines forecasts obtained from a range of models, and it often produces more accurate forecasts than a forecast from a single model. The crucial part of forecast accuracy improvement in using the model averaging lies in the determination of optimal weights from a finite sample. If the weights are selected sub-optimally, this can affect the accuracy of the model-averaged forecasts. Instead of choosing the optimal weights, we consider trimming a set of models before equally averaging forecasts from the selected superior models. Motivated by Hansen, Lunde and Nason (2011), we apply and evaluate the model confidence set procedure when combining mortality forecasts. The proposed model averaging procedure is motivated by Samuels and Sekkel (2017) based on the concept of model confidence sets as proposed by Hansen et al. (2011) that incorporates the statistical significance of the forecasting performance. As the model confidence level increases, the set of superior models generally decreases. The proposed model averaging procedure is demonstrated via national and sub-national Japanese mortality for retirement ages between 60 and 100+. Illustrated by national and sub-national Japanese mortality for ages between 60 and 100+, the proposed model-average procedure gives the smallest interval forecast errors, especially for males. We find that robust out-of-sample point and interval forecasts may be obtained from the trimming method. By robust, we mean robustness against model misspecification.},
  file = {/Users/salvatorykessy/Zotero/storage/DL6L8LH7/Shang, Haberman - 2018 - Model confidence sets and forecast combination an application to age-specific mortality.pdf},
  journal = {Genus},
  keywords = {Equal predictability test,Japanese human mortality database,Mean interval score,Model averaging,Root mean square forecast error},
  number = {1}
}

@article{shangPointIntervalForecasts2012,
  title = {Point and Interval Forecasts of Age-Specific Life Expectancies: {{A}} Model Averaging Approach},
  author = {Shang, Han Lin},
  year = {2012},
  volume = {27},
  pages = {593--644},
  doi = {10.4054/DemRes.2012.27.21},
  abstract = {Background: Any improvement in the forecast accuracy of life expectancy would be beneficial for policy decision regarding the allocation of current and future resources. In this paper, I revisit some methods for forecasting age-specific life expectancies. Objective: This paper proposes a model averaging approach to produce accurate point forecasts of age-specific life expectancies. Methods: Illustrated by data from fourteen developed countries, we compare point and interval fore-casts among ten principal component methods, two random walk methods, and two uni-variate time-series methods. Results: Based on averaged one-step-ahead and ten-step-ahead forecast errors, random walk with drift and Lee-Miller methods are the two most accurate methods for producing point fore-casts. By combining their forecasts, point forecast accuracy is improved. As measured by averaged coverage probability deviance, the Hyndman-Ullah methods generally provide more accurate interval forecasts than the Lee-Carter methods. However, the Hyndman-Ullah methods produce wider half-widths of prediction interval than the Lee-Carter meth-ods. Conclusions: Model averaging approach should be considered to produce more accurate point forecasts. \textcopyright{} 2012 Han Lin Shang.},
  file = {/Users/salvatorykessy/Zotero/storage/QR9A58DD/Shang - 2012 - Point and interval forecasts of age-specific life expectancies A model averaging approach.pdf},
  journal = {Demographic Research}
}

@article{shangPointIntervalForecasts2012a,
  title = {Point and Interval Forecasts of Age-Specific Life Expectancies: {{A}} Model Averaging Approach},
  author = {Shang, Han Lin},
  year = {2012},
  volume = {27},
  pages = {593--644},
  doi = {10.4054/DemRes.2012.27.21},
  abstract = {Background: Any improvement in the forecast accuracy of life expectancy would be beneficial for policy decision regarding the allocation of current and future resources. In this paper, I revisit some methods for forecasting age-specific life expectancies. Objective: This paper proposes a model averaging approach to produce accurate point forecasts of age-specific life expectancies. Methods: Illustrated by data from fourteen developed countries, we compare point and interval fore-casts among ten principal component methods, two random walk methods, and two uni-variate time-series methods. Results: Based on averaged one-step-ahead and ten-step-ahead forecast errors, random walk with drift and Lee-Miller methods are the two most accurate methods for producing point fore-casts. By combining their forecasts, point forecast accuracy is improved. As measured by averaged coverage probability deviance, the Hyndman-Ullah methods generally provide more accurate interval forecasts than the Lee-Carter methods. However, the Hyndman-Ullah methods produce wider half-widths of prediction interval than the Lee-Carter meth-ods. Conclusions: Model averaging approach should be considered to produce more accurate point forecasts. \textcopyright{} 2012 Han Lin Shang.},
  file = {/Users/salvatorykessy/Zotero/storage/A4N5NU55/Shang - 2012 - Point and interval forecasts of age-specific life expectancies A model averaging approach.pdf},
  journal = {Demographic Research}
}

@article{shangStatisticallyTestedComparisons2015,
  title = {Statistically Tested Comparisons of the Accuracy of Forecasting Methods for Age-Specific and Sex-Specific Mortality and Life Expectancy},
  author = {Shang, Han Lin},
  year = {2015},
  month = sep,
  volume = {69},
  pages = {317--335},
  issn = {0032-4728, 1477-4747},
  doi = {10.1080/00324728.2015.1074268},
  file = {/Users/salvatorykessy/Zotero/storage/5YWA2MWL/Shang - 2015 - Statistically tested comparisons of the accuracy o.pdf},
  journal = {Population Studies},
  language = {en},
  number = {3}
}

@article{sharabianiMultistageHeterogeneousEnsemble2016,
  title = {Multi-Stage Heterogeneous Ensemble Meta-Learning with Hands-off User-Interface and Stand-Alone Prediction Using Principal Components Regression: {{The R}} Package {{EnsemblePCReg}}},
  author = {Sharabiani, Mansour T A and Mahani, Alireza S},
  year = {2016},
  pages = {32},
  abstract = {Despite the fact that ensemble meta-learning of a heterogeneous collection of base learners is an effective means to reduce the generalization error in predictive models, several factors have impeded a broad adoption of such techniques among practitioners. These factors include an intractable number of choices of base learners and their tuning parameters, complex methodology required for integration of base learners, the ensuing complexity of software needed to support stand-alone prediction, and significant CPU and memory consumption of heterogeneous ensemble meta-learning techniques. The R package EnsemblePCReg overcomes the above barriers by combining several features. Sensible base-learner parameter grids provide a hands-off API for non-experts while allowing expert users to exert control by overriding default settings. Sophisticated ensemble generation and integration methods, combining stacked generalization and principal components regression, offer favorable generalization performance. Finally, computational optimizations such as advanded thread scheduling for improved parallelization scaling and file methods for relieving memory consumption during training and prediction, significantly increase the range of data sizes that can be handled on personal computers. In combining these features, EnsemblePCReg significantly lowers the barrier for practitioners to apply heterogeneous ensemble meta-learning techniques to their everyday regression problems.},
  file = {/Users/salvatorykessy/Zotero/storage/8JRQZW4T/Sharabiani and Mahani - Multi-stage heterogeneous ensemble meta-learning w.pdf},
  language = {en}
}

@article{sharmaAnalysisEnsembleModels2018,
  ids = {sharmaAnalysisEnsembleModels2018a,sharmaAnalysisEnsembleModels2018b},
  title = {Analysis of {{Ensemble Models}} for {{Aging Related Bug Prediction}} in {{Software Systems}}},
  author = {Sharma, Shubham and Kumar, Sandeep},
  year = {2018},
  pages = {256--263},
  issn = {9789897583209},
  doi = {10.5220/0006847702560263},
  file = {/Users/salvatorykessy/Zotero/storage/NULC7Y5G/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/TXA7VSGQ/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.ris},
  number = {Icsoft}
}

@article{shawFiftyYearsUnited2007,
  title = {Fifty Years of {{United Kingdom}} National Population Projections: How Accurate Have They Been?},
  author = {Shaw, Chris},
  year = {2007},
  pages = {16},
  file = {/Users/salvatorykessy/Zotero/storage/MT86CLLW/Shaw - 2007 - Fifty years of United Kingdom national population .pdf},
  journal = {Population Trends},
  language = {en}
}

@book{sheatherSpringerTextsStatistics2009,
  title = {Springer {{Texts}} in {{Statistics}}},
  author = {Sheather, Simon J. and Pitman, Jim and Sheather, Simon J.},
  year = {2009},
  volume = {102},
  doi = {10.1016/j.peva.2007.06.006},
  abstract = {Review From the reviews: .,."There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006 "The reader sees not only how measure theory is used to develop probability theory, but also how probability theory is used in applications. a The discourse is delivered in a theorem proof format and thus is better suited for classroom a . The authors prose is generally well thought out a . will make an attractive choice for a two-semester course on measure and probability, or as a second course for students with a semester of measure or probability theory under their belt." (Peter C. Kiessler, Journal of the American Statistical Association, Vol. 102 (479), 2007) "The book is a well written self-contained textbook on measure and probability theory. It consists of 18 chapters. Every chapter contains many well chosen examples and ends with several problems related to the earlier developed theory (some with hints). a At the very end of the book there is an appendix collecting necessary facts from set theory, calculus and metric spaces. The authors suggest a few possibilities on how to use their book." (Kazimierz Musial, Zentralblatt MATH, Vol. 1125 (2), 2008) "The title of the book consists of the names of its two basic parts. The booka (TM)s third part is comprised of some special topics from probability theory. a The authors suggest using the book intwo-semester graduate programs in statistics or a one-semester seminar on special topics. The material of the book is standard a is clear, comprehensive and a \textasciitilde without being intimidatinga (TM)." (Rimas NorvaiAa, Mathematical Reviews, Issue 2007 f) Product Description This is a graduate level textbook on measure theory and probability theory. The book can be used as a text for a two semester sequence of courses in measure theory and probability theory, with an option to include supplemental material on stochastic processes and special topics. It is intended primarily for first year Ph.D. students in mathematics and statistics although mathematically advanced students from engineering and economics would also find the book useful. Prerequisites are kept to the minimal level of an understanding of basic real analysis concepts such as limits, continuity, differentiability, Riemann integration, and convergence of sequences and series. A review of this material is included in the appendix. The book starts with an informal introduction that provides some heuristics into the abstract concepts of measure and integration theory, which are then rigorously developed. The first part of the book can be used for a standard real analysis course for both mathematics and statistics Ph.D. students as it provides full coverage of topics such as the construction of Lebesgue-Stieltjes measures on real line and Euclidean spaces, the basic convergence theorems, L p spaces, signed measures, Radon-Nikodym theorem, Lebesgue's decomposition theorem and the fundamental theorem of Lebesgue integration on R, product spaces and product measures, and Fubini-Tonelli theorems. It also provides an elementary introduction to Banach and Hilbert spaces, convolutions, Fourier series and Fourier and Plancherel transforms. Thus part I would be particularly useful for students in a typical Statistics Ph.D. program if a separate course on real analysis is not a standard requirement. Part II (chapters 6-13) provides full coverage of standard graduate level probability theory. It starts with Kolmogorov's probability model and Kolmogorov's existence theorem. It then treats thoroughly the laws of large numbers including renewal theory and ergodic theorems with applications and then weak convergence of probability distributions, characteristic functions, the Levy-Cramer continuity theorem and the central limit theorem as well as stable laws. It ends with conditional expectations and conditional probability, and an introduction to the theory of discrete time martingales. Part III (chapters 14-18) provides a modest coverage of discrete time Markov chains with countable and general state spaces, MCMC, continuous time discrete space jump Markov processes, Brownian motion, mixing sequences, bootstrap methods, and branching processes. It could be used for a topics/seminar course or as an introduction to stochastic processes. From the reviews: "...There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006},
  file = {/Users/salvatorykessy/Zotero/storage/ZIYGZDCK/Sheather, Pitman, Sheather - 2009 - Springer Texts in Statistics.pdf},
  isbn = {978-0-387-78188-4}
}

@book{sheatherSpringerTextsStatistics2009a,
  title = {Springer {{Texts}} in {{Statistics}}},
  author = {Sheather, Simon J. and Pitman, Jim and Sheather, Simon J.},
  year = {2009},
  volume = {102},
  doi = {10.1016/j.peva.2007.06.006},
  abstract = {Review From the reviews: .,."There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006 "The reader sees not only how measure theory is used to develop probability theory, but also how probability theory is used in applications. a The discourse is delivered in a theorem proof format and thus is better suited for classroom a . The authors prose is generally well thought out a . will make an attractive choice for a two-semester course on measure and probability, or as a second course for students with a semester of measure or probability theory under their belt." (Peter C. Kiessler, Journal of the American Statistical Association, Vol. 102 (479), 2007) "The book is a well written self-contained textbook on measure and probability theory. It consists of 18 chapters. Every chapter contains many well chosen examples and ends with several problems related to the earlier developed theory (some with hints). a At the very end of the book there is an appendix collecting necessary facts from set theory, calculus and metric spaces. The authors suggest a few possibilities on how to use their book." (Kazimierz Musial, Zentralblatt MATH, Vol. 1125 (2), 2008) "The title of the book consists of the names of its two basic parts. The booka (TM)s third part is comprised of some special topics from probability theory. a The authors suggest using the book intwo-semester graduate programs in statistics or a one-semester seminar on special topics. The material of the book is standard a is clear, comprehensive and a \textasciitilde without being intimidatinga (TM)." (Rimas NorvaiAa, Mathematical Reviews, Issue 2007 f) Product Description This is a graduate level textbook on measure theory and probability theory. The book can be used as a text for a two semester sequence of courses in measure theory and probability theory, with an option to include supplemental material on stochastic processes and special topics. It is intended primarily for first year Ph.D. students in mathematics and statistics although mathematically advanced students from engineering and economics would also find the book useful. Prerequisites are kept to the minimal level of an understanding of basic real analysis concepts such as limits, continuity, differentiability, Riemann integration, and convergence of sequences and series. A review of this material is included in the appendix. The book starts with an informal introduction that provides some heuristics into the abstract concepts of measure and integration theory, which are then rigorously developed. The first part of the book can be used for a standard real analysis course for both mathematics and statistics Ph.D. students as it provides full coverage of topics such as the construction of Lebesgue-Stieltjes measures on real line and Euclidean spaces, the basic convergence theorems, L p spaces, signed measures, Radon-Nikodym theorem, Lebesgue's decomposition theorem and the fundamental theorem of Lebesgue integration on R, product spaces and product measures, and Fubini-Tonelli theorems. It also provides an elementary introduction to Banach and Hilbert spaces, convolutions, Fourier series and Fourier and Plancherel transforms. Thus part I would be particularly useful for students in a typical Statistics Ph.D. program if a separate course on real analysis is not a standard requirement. Part II (chapters 6-13) provides full coverage of standard graduate level probability theory. It starts with Kolmogorov's probability model and Kolmogorov's existence theorem. It then treats thoroughly the laws of large numbers including renewal theory and ergodic theorems with applications and then weak convergence of probability distributions, characteristic functions, the Levy-Cramer continuity theorem and the central limit theorem as well as stable laws. It ends with conditional expectations and conditional probability, and an introduction to the theory of discrete time martingales. Part III (chapters 14-18) provides a modest coverage of discrete time Markov chains with countable and general state spaces, MCMC, continuous time discrete space jump Markov processes, Brownian motion, mixing sequences, bootstrap methods, and branching processes. It could be used for a topics/seminar course or as an introduction to stochastic processes. From the reviews: "...There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006},
  file = {/Users/salvatorykessy/Zotero/storage/ALIDCT3G/demsar.pdf;/Users/salvatorykessy/Zotero/storage/TXKW7X8Y/Sheather, Pitman, Sheather - 2009 - Springer Texts in Statistics.pdf},
  isbn = {978-0-387-78188-4}
}

@article{shernoffIntegrativeConsensusSystematic2014a,
  ids = {shernoffIntegrativeConsensusSystematic2014,shernoffIntegrativeConsensusSystematic2014b},
  title = {Integrative Consensus: {{A}} Systematic Approach to Integrating Comprehensive Assessment Data for Young Children with Behavior Problems},
  author = {Shernoff, Elisa S. and Hill, Carri and Danis, Barbara and Leventhal, Bennett L. and Wakschlag, Lauren S.},
  year = {2014},
  volume = {27},
  pages = {92--110},
  doi = {10.1097/IYC.0000000000000008},
  abstract = {Comprehensive assessments that include parents and teachers are essential when assessing young children vulnerable to emotional and behavioral problems given the multiple systems and contexts that influence and support optimal development (U. Bronfenbrenner \{\&\} P. A. Morris, 2006; M. J. Guralnick, 2011). However, more data complicate clinical and educational decision making given the challenge of integrating comprehensive data. We report on initial efforts to develop and apply Integrative Consensus procedures designed to synthesize comprehensive assessment data using developmentally informed guidelines. Mother-teacher dyads (N = 295) reported on disruptive behavior in a sample of 295 low-\-income 3- to 5-\-year-\-olds; one-\-third referred for disruptive behaviors, one-\-third nonreferred with behavioral concerns, and one-\-third nonreferred. Two clinicians trained in Integrative Consensus procedures independently applied the framework, with findings highlighting that children identified as disruptive by Integrative Consensus ratings plus mother or teacher ratings significantly predicted behavior problems and impaired social skills. Children identified as disruptive via Integrative Consensus were 4 times more likely to be rated as impaired by their mother at follow\--up than by mother or teacher report. Reliability estimates were high (? = 0.84), suggesting that the method has promise for identifying young children with behavior problems while systematically integrating comprehensive data.},
  file = {/Users/salvatorykessy/Zotero/storage/RQBI94AG/Shernoff et al. - 2014 - Integrative consensus A systematic approach to in.pdf},
  journal = {Infants and Young Children},
  keywords = {Behavior problems,Clinical judgment,Comprehensive assessments,Multicontext assessments},
  number = {2}
}

@article{sillFeatureWeightedLinearStacking2009,
  title = {Feature-{{Weighted Linear Stacking}}},
  author = {Sill, Joseph and Takacs, Gabor and Mackey, Lester and Lin, David},
  year = {2009},
  month = nov,
  abstract = {Ensemble methods, such as stacking, are designed to boost predictive accuracy by blending the predictions of multiple machine learning models. Recent work has shown that the use of meta-features, additional inputs describing each example in a dataset, can boost the performance of ensemble methods, but the greatest reported gains have come from nonlinear procedures requiring significant tuning and training time. Here, we present a linear technique, Feature-Weighted Linear Stacking (FWLS), that incorporates meta-features for improved accuracy while retaining the well-known virtues of linear regression regarding speed, stability, and interpretability. FWLS combines model predictions linearly using coefficients that are themselves linear functions of meta-features. This technique was a key facet of the solution of the second place team in the recently concluded Netflix Prize competition. Significant increases in accuracy over standard linear stacking are demonstrated on the Netflix Prize collaborative filtering dataset.},
  annote = {Comment: 17 pages, 1 figure, 2 tables},
  archivePrefix = {arXiv},
  eprint = {0911.0460},
  eprinttype = {arxiv},
  file = {/Users/salvatorykessy/Zotero/storage/Y5JXJNNN/Sill et al. - 2009 - Feature-Weighted Linear Stacking.pdf},
  journal = {arXiv:0911.0460 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{smylHybridMethodExponential2020,
  title = {A Hybrid Method of Exponential Smoothing and Recurrent Neural Networks for Time Series Forecasting},
  author = {Smyl, Slawek},
  year = {2020},
  month = jan,
  volume = {36},
  pages = {75--85},
  issn = {01692070},
  doi = {10.1016/j.ijforecast.2019.03.017},
  abstract = {This paper presents the winning submission of the M4 forecasting competition. The submission utilizes a Dynamic Computational Graph Neural Network system that enables mixing of a standard Exponential Smoothing model with advanced Long Short Term Memory networks into a common framework. The result is a hybrid and hierarchical forecasting method.},
  file = {/Users/salvatorykessy/Zotero/storage/CIDVXMKE/Smyl - 2020 - A hybrid method of exponential smoothing and recur.pdf},
  journal = {International Journal of Forecasting},
  language = {en},
  number = {1}
}

@article{sonkarApplicationSupervisedMachine2017,
  title = {Application of {{Supervised Machine Learning}} to {{Predict}} the {{Mortality Risk}} in {{Elderly Using Biomarkers}}},
  author = {Sonkar, Priyanka},
  year = {2017},
  abstract = {Sonkar, P. (2017) Application of supervised machine learning to predict the mortality risk in elderly using biomarkers. Masters dissertation, DIT, 2017.},
  file = {/Users/salvatorykessy/Zotero/storage/BWYKLKSD/Sonkar - 2017 - Application of Supervised Machine Learning to Predict the Mortality Risk in Elderly Using Biomarkers.pdf}
}

@article{sonkarApplicationSupervisedMachine2017a,
  title = {Application of {{Supervised Machine Learning}} to {{Predict}} the {{Mortality Risk}} in {{Elderly Using Biomarkers}}},
  author = {Sonkar, Priyanka},
  year = {2017},
  abstract = {Sonkar, P. (2017) Application of supervised machine learning to predict the mortality risk in elderly using biomarkers. Masters dissertation, DIT, 2017.},
  file = {/Users/salvatorykessy/Zotero/storage/7WT8539P/Sonkar - 2017 - Application of Supervised Machine Learning to Predict the Mortality Risk in Elderly Using Biomarkers.pdf}
}

@article{sridaranDilanSridaran2018,
  title = {Dilan Sridaran},
  author = {Sridaran, Dilan},
  year = {2018},
  file = {/Users/salvatorykessy/Zotero/storage/FPR63ZE2/Sridaran - 2018 - Dilan sridaran(2).pdf},
  number = {November}
}

@article{sridaranDilanSridaran2018a,
  title = {Dilan Sridaran},
  author = {Sridaran, Dilan},
  year = {2018},
  file = {/Users/salvatorykessy/Zotero/storage/2HQ5WYXN/Sridaran - 2018 - Dilan sridaran(2).pdf},
  number = {November}
}

@article{sridaranDilanSridaran2018b,
  title = {Dilan Sridaran},
  author = {Sridaran, Dilan},
  year = {2018},
  file = {/Users/salvatorykessy/Zotero/storage/UKD8WRPQ/Sridaran - 2018 - Dilan sridaran.pdf},
  number = {November}
}

@article{StackingEnsembleApproach,
  title = {The Stacking Ensemble Approach},
  pages = {82--95},
  file = {/Users/salvatorykessy/Zotero/storage/8A5K8Q2Q/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.ris}
}

@article{StackingEnsembleApproacha,
  title = {The Stacking Ensemble Approach},
  pages = {82--95},
  file = {/Users/salvatorykessy/Zotero/storage/TH8YXU5Y/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf}
}

@article{StackingEnsembleApproachb,
  title = {The Stacking Ensemble Approach},
  pages = {82--95}
}

@article{stoeldraijerImpactDifferentMortality2013,
  title = {Impact of Different Mortality Forecasting Methods and Explicit Assumptions on Projected Future Life Expectancy: {{The}} Case of the {{Netherlands}}},
  shorttitle = {Impact of Different Mortality Forecasting Methods and Explicit Assumptions on Projected Future Life Expectancy},
  author = {Stoeldraijer, Lenny and {van Duin}, Coen and {van Wissen}, L.J.G and Janssen, Fanny},
  year = {2013},
  month = aug,
  volume = {29},
  pages = {323--354},
  issn = {1435-9871},
  doi = {10.4054/DemRes.2013.29.13},
  abstract = {BACKGROUND With the rapid aging of the population, mortality forecasting becomes increasingly important, especially for the insurance and pension industries. However, a wide variety of projection methods are in use, both between and within countries, that produce different outcomes.},
  file = {/Users/salvatorykessy/Zotero/storage/FW7K7Q8D/Stoeldraijer et al. - 2013 - Impact of different mortality forecasting methods .pdf},
  journal = {Demographic Research},
  language = {en}
}

@article{strategiesResearchMotivationStatement2018,
  title = {Research {{Motivation Statement}} : {{Doreen Kabuche Project}} 1 {{Project}} 2 {{Project}} 3},
  author = {Strategies, Risk Management},
  year = {2018},
  pages = {2013--2014},
  file = {/Users/salvatorykessy/Zotero/storage/4IPU6VKU/Strategies - 2018 - Research Motivation Statement Doreen Kabuche Project 1 Project 2 Project 3.pdf}
}

@article{strategiesResearchMotivationStatement2018a,
  title = {Research {{Motivation Statement}} : {{Doreen Kabuche Project}} 1 {{Project}} 2 {{Project}} 3},
  author = {Strategies, Risk Management},
  year = {2018},
  pages = {2013--2014},
  file = {/Users/salvatorykessy/Zotero/storage/FZ4C3BQF/Strategies - 2018 - Research Motivation Statement Doreen Kabuche Project 1 Project 2 Project 3.pdf;/Users/salvatorykessy/Zotero/storage/ZGDCFMYC/clusteringmortality.pdf}
}

@book{sugiyamaIntroductionStatisticalMachine2015,
  title = {Introduction to {{Statistical Machine Learning}}},
  author = {Sugiyama, Masashi},
  year = {2015},
  doi = {10.1016/C2014-0-01992-2},
  abstract = {Machine learning allows computers to learn and discern patterns without actually being programmed. When Statistical techniques and machine learning are combined together they are a powerful tool for analysing various kinds of data in many computer science/engineering areas including, image processing, speech processing, natural language processing, robot control, as well as in fundamental sciences such as biology, medicine, astronomy, physics, and materials. Introduction to Statistical Machine Learning provides a general introduction to machine learning that covers a wide range of topics concisely and will help you bridge the gap between theory and practice. Part I discusses the fundamental concepts of statistics and probability that are used in describing machine learning algorithms. Part II and Part III explain the two major approaches of machine learning techniques; generative methods and discriminative methods. While Part III provides an in-depth look at advanced topics that play essential roles in making machine learning algorithms more useful in practice. The accompanying MATLAB/Octave programs provide you with the necessary practical skills needed to accomplish a wide range of data analysis tasks. Provides the necessary background material to understand machine learning such as statistics, probability, linear algebra, and calculus. Complete coverage of the generative approach to statistical pattern recognition and the discriminative approach to statistical machine learning. Includes MATLAB/Octave programs so that readers can test the algorithms numerically and acquire both mathematical and practical skills in a wide range of data analysis tasks. Discusses a wide range of applications in machine learning and statistics and provides examples drawn from image processing, speech processing, natural language processing, robot control, as well as biology, medicine, astronomy, physics, and materials.},
  file = {/Users/salvatorykessy/Zotero/storage/GY6UKDWT/Sugiyama - 2015 - Introduction to Statistical Machine Learning.pdf},
  isbn = {978-0-12-802121-7}
}

@book{sugiyamaIntroductionStatisticalMachine2015a,
  title = {Introduction to {{Statistical Machine Learning}}},
  author = {Sugiyama, Masashi},
  year = {2015},
  doi = {10.1016/C2014-0-01992-2},
  abstract = {Machine learning allows computers to learn and discern patterns without actually being programmed. When Statistical techniques and machine learning are combined together they are a powerful tool for analysing various kinds of data in many computer science/engineering areas including, image processing, speech processing, natural language processing, robot control, as well as in fundamental sciences such as biology, medicine, astronomy, physics, and materials. Introduction to Statistical Machine Learning provides a general introduction to machine learning that covers a wide range of topics concisely and will help you bridge the gap between theory and practice. Part I discusses the fundamental concepts of statistics and probability that are used in describing machine learning algorithms. Part II and Part III explain the two major approaches of machine learning techniques; generative methods and discriminative methods. While Part III provides an in-depth look at advanced topics that play essential roles in making machine learning algorithms more useful in practice. The accompanying MATLAB/Octave programs provide you with the necessary practical skills needed to accomplish a wide range of data analysis tasks. Provides the necessary background material to understand machine learning such as statistics, probability, linear algebra, and calculus. Complete coverage of the generative approach to statistical pattern recognition and the discriminative approach to statistical machine learning. Includes MATLAB/Octave programs so that readers can test the algorithms numerically and acquire both mathematical and practical skills in a wide range of data analysis tasks. Discusses a wide range of applications in machine learning and statistics and provides examples drawn from image processing, speech processing, natural language processing, robot control, as well as biology, medicine, astronomy, physics, and materials.},
  file = {/Users/salvatorykessy/Zotero/storage/28QAGBST/Sugiyama - 2015 - Introduction to Statistical Machine Learning.pdf},
  isbn = {978-0-12-802121-7}
}

@article{talagalaMetalearningHowForecast2018,
  title = {Meta-Learning How to Forecast Time Series},
  author = {Talagala, Thiyanga S and Hyndman, Rob J and Athanasopoulos, George},
  year = {2018},
  pages = {30},
  abstract = {A crucial task in time series forecasting is the identification of the most suitable forecasting method. We present a general framework for forecast-model selection using meta-learning. A random forest is used to identify the best forecasting method using only time series features. The framework is evaluated using time series from the M1 and M3 competitions and is shown to yield accurate forecasts comparable to several benchmarks and other commonly used automated approaches of time series forecasting. A key advantage of our proposed framework is that the time-consuming process of building a classifier is handled in advance of the forecasting task at hand.},
  file = {/Users/salvatorykessy/Zotero/storage/ZHXKDGBY/Talagala et al. - 2018 - Meta-learning how to forecast time series.pdf},
  language = {en}
}

@book{tattarHandsEnsembleLearning,
  title = {Hands- o n {{Ensemble Learning}} with {{R A}} Beginner ' s Guide to Combining},
  author = {Tattar, Prabhanjan Narayanachar},
  file = {/Users/salvatorykessy/Zotero/storage/8QJUJT5Z/Tattar - Unknown - Hands- o n Ensemble Learning with R A beginner ' s guide to combining.pdf},
  isbn = {978-1-78862-414-5}
}

@book{tattarHandsEnsembleLearninga,
  title = {Hands- o n {{Ensemble Learning}} with {{R A}} Beginner ' s Guide to Combining},
  author = {Tattar, Prabhanjan Narayanachar},
  file = {/Users/salvatorykessy/Zotero/storage/BPND75LD/Tattar - Unknown - No Title.pdf},
  isbn = {978-1-78862-414-5}
}

@article{therneauIntroductionRecursivePartitioning1997,
  title = {An Introduction to Recursive Partitioning Using the Rpart Routines. {{Technical}} Report No. 61},
  author = {Therneau, Terry M. and Atkinson, Elizabeth J.},
  year = {1997},
  pages = {67 pp-67 pp},
  file = {/Users/salvatorykessy/Zotero/storage/5JFR8BSA/Therneau, Atkinson - 1997 - An introduction to recursive partitioning using the rpart routines. Technical report no. 61.pdf}
}

@article{therneauIntroductionRecursivePartitioning1997a,
  title = {An Introduction to Recursive Partitioning Using the Rpart Routines. {{Technical}} Report No. 61},
  author = {Therneau, Terry M. and Atkinson, Elizabeth J.},
  year = {1997},
  pages = {67 pp-67 pp},
  file = {/Users/salvatorykessy/Zotero/storage/MUQSLFGF/Therneau, Atkinson - 1997 - An introduction to recursive partitioning using the rpart routines. Technical report no. 61.pdf}
}

@article{tibshiraniValeriePatrickHastie2016,
  title = {Valerie and {{Patrick Hastie}}},
  author = {Tibshirani, Sami and Wainwright, John},
  year = {2016},
  pages = {362},
  file = {/Users/salvatorykessy/Zotero/storage/ZMXINGF5/Tibshirani and Wainwright - Valerie and Patrick Hastie.pdf},
  language = {en}
}

@article{tingIssuesStackedGeneralization1999,
  title = {Issues in Stacked Generalization},
  author = {Ting, Kai Ming and Witten, Ian H},
  year = {1999},
  volume = {10},
  pages = {271--289},
  abstract = {Stacked generalization is a general method of using a high-level model to combine lower-level models to achieve greater predictive accuracy. In this paper we address two crucial issues which have been considered to be a `black art' in classification tasks ever since the introduction of stacked generalization in 1992 by Wolpert: the type of generalizer that is suitable to derive the higher-level model, and the kind of attributes that should be used as its input. We find that best results are obtained when the higher-level model combines the confidence (and not just the predictions) of the lower-level ones. We demonstrate the effectiveness of stacked generalization for combining three different types of learning algorithms for classification tasks. We also compare the performance of stacked generalization with majority vote and published results of arcing and bagging.},
  file = {/Users/salvatorykessy/Zotero/storage/3JQA84GU/hybrids.pdf;/Users/salvatorykessy/Zotero/storage/T637CZPV/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.ris},
  journal = {Journal of Artificial Intelligence Research}
}

@article{tingIssuesStackedGeneralization1999a,
  title = {Issues in Stacked Generalization},
  author = {Ting, Kai Ming and Witten, Ian H.},
  year = {1999},
  volume = {10},
  pages = {271--289},
  abstract = {Stacked generalization is a general method of using a high-level model to combine lower-level models to achieve greater predictive accuracy. In this paper we address two crucial issues which have been considered to be a `black art' in classification tasks ever since the introduction of stacked generalization in 1992 by Wolpert: the type of generalizer that is suitable to derive the higher-level model, and the kind of attributes that should be used as its input. We find that best results are obtained when the higher-level model combines the confidence (and not just the predictions) of the lower-level ones. We demonstrate the effectiveness of stacked generalization for combining three different types of learning algorithms for classification tasks. We also compare the performance of stacked generalization with majority vote and published results of arcing and bagging.},
  file = {/Users/salvatorykessy/Zotero/storage/ULUHV2HV/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf},
  journal = {Journal of Artificial Intelligence Research}
}

@article{tingIssuesStackedGeneralization1999b,
  title = {Issues in Stacked Generalization},
  author = {Ting, Kai Ming and Witten, Ian H.},
  year = {1999},
  volume = {10},
  pages = {271--289},
  abstract = {Stacked generalization is a general method of using a high-level model to combine lower-level models to achieve greater predictive accuracy. In this paper we address two crucial issues which have been considered to be a `black art' in classification tasks ever since the introduction of stacked generalization in 1992 by Wolpert: the type of generalizer that is suitable to derive the higher-level model, and the kind of attributes that should be used as its input. We find that best results are obtained when the higher-level model combines the confidence (and not just the predictions) of the lower-level ones. We demonstrate the effectiveness of stacked generalization for combining three different types of learning algorithms for classification tasks. We also compare the performance of stacked generalization with majority vote and published results of arcing and bagging.},
  journal = {Journal of Artificial Intelligence Research}
}

@article{toscherBigChaosSolutionNetflix,
  title = {The {{BigChaos Solution}} to the {{Netflix Grand Prize}}},
  author = {Toscher, Andreas and Jahrer, Michael and Bell, Robert M},
  pages = {52},
  file = {/Users/salvatorykessy/Zotero/storage/3QJ42GY5/Toscher et al. - The BigChaos Solution to the Netﬂix Grand Prize.pdf},
  language = {en}
}

@article{treiberWindPowerPrediction2016,
  title = {Wind {{Power Prediction}} with {{Machine Learning BT}}  - {{Computational Sustainability}}},
  author = {Treiber, Nils Andr{\'e} and Heinermann, Justin and Kramer, Oliver},
  year = {2016},
  pages = {13--29},
  doi = {10.1007/978-3-319-31858-5_2},
  abstract = {Better prediction models for the upcoming supply of renewable energy are important to decrease the need of controlling energy provided by conventional power plants. Especially for successful power grid integration of the highly volatile wind power production, a reliable forecast is crucial. In this chapter, we focus on short-term wind power prediction and employ data from the National Renewable Energy Laboratory (NREL), which are designed for a wind integration study in the western part of the United States. In contrast to physical approaches based on very complex differential equations, our model derives functional dependencies directly from the observations. Hereby, we formulate the prediction task as regression problem and test different regression techniques such as linear regression, k-nearest neighbors and support vector regression. In our experiments, we analyze predictions for individual turbines as well as entire wind parks and show that a machine learning approach yields feasible results for short-term wind power prediction.},
  number = {September}
}

@article{treiberWindPowerPrediction2016a,
  title = {Wind {{Power Prediction}} with {{Machine Learning BT}}  - {{Computational Sustainability}}},
  author = {Treiber, Nils Andr{\'e} and Heinermann, Justin and Kramer, Oliver},
  year = {2016},
  pages = {13--29},
  issn = {978-3-319-31858-5},
  doi = {10.1007/978-3-319-31858-5_2},
  abstract = {Better prediction models for the upcoming supply of renewable energy are important to decrease the need of controlling energy provided by conventional power plants. Especially for successful power grid integration of the highly volatile wind power production, a reliable forecast is crucial. In this chapter, we focus on short-term wind power prediction and employ data from the National Renewable Energy Laboratory (NREL), which are designed for a wind integration study in the western part of the United States. In contrast to physical approaches based on very complex differential equations, our model derives functional dependencies directly from the observations. Hereby, we formulate the prediction task as regression problem and test different regression techniques such as linear regression, k-nearest neighbors and support vector regression. In our experiments, we analyze predictions for individual turbines as well as entire wind parks and show that a machine learning approach yields feasible results for short-term wind power prediction.},
  file = {/Users/salvatorykessy/Zotero/storage/HD3FMSXG/Treiber et al. - 2016 - Wind Power Prediction with Machine Learning BT  - .pdf},
  number = {September}
}

@article{treiberWindPowerPrediction2016b,
  title = {Wind {{Power Prediction}} with {{Machine Learning BT}}  - {{Computational Sustainability}}},
  author = {Treiber, Nils Andr{\'e} and Heinermann, Justin and Kramer, Oliver},
  year = {2016},
  pages = {13--29},
  issn = {978-3-319-31858-5},
  doi = {10.1007/978-3-319-31858-5_2},
  abstract = {Better prediction models for the upcoming supply of renewable energy are important to decrease the need of controlling energy provided by conventional power plants. Especially for successful power grid integration of the highly volatile wind power production, a reliable forecast is crucial. In this chapter, we focus on short-term wind power prediction and employ data from the National Renewable Energy Laboratory (NREL), which are designed for a wind integration study in the western part of the United States. In contrast to physical approaches based on very complex differential equations, our model derives functional dependencies directly from the observations. Hereby, we formulate the prediction task as regression problem and test different regression techniques such as linear regression, k-nearest neighbors and support vector regression. In our experiments, we analyze predictions for individual turbines as well as entire wind parks and show that a machine learning approach yields feasible results for short-term wind power prediction.},
  file = {/Users/salvatorykessy/Zotero/storage/AEZBGBB5/Treiber et al. - 2016 - Wind Power Prediction with Machine Learning BT  - .pdf},
  number = {September}
}

@article{turnerHybridPensionsRisk2014,
  title = {Hybrid {{Pensions}}: {{Risk Sharing Arrangements}} for {{Pension Plan Sponsors}} and {{Participants}}},
  author = {Turner, John},
  year = {2014},
  pages = {44},
  file = {/Users/salvatorykessy/Zotero/storage/RMMQ8GCS/Turner - 2014 - Hybrid Pensions Risk Sharing Arrangements for Pen.pdf},
  language = {en}
}

@article{villegasCOMPARATIVESTUDYTWOPOPULATION2017,
  title = {A {{COMPARATIVE STUDY}} of {{TWO}}-{{POPULATION MODELS}} for the {{ASSESSMENT}} of {{BASIS RISK}} in {{LONGEVITY HEDGES}}},
  author = {Villegas, Andr{\'e}s M. and Haberman, Steven and Kaishev, Vladimir K. and Millossovich, Pietro},
  year = {2017},
  volume = {47},
  pages = {631--679},
  doi = {10.1017/asb.2017.18},
  abstract = {Longevity swaps have been one of the major success stories of pension scheme de-risking in recent years. However, with some few exceptions, all of the transactions to date have been bespoke longevity swaps based upon the mortality experience of a portfolio of named lives. In order for this market to start to meet its true potential, solutions will ultimately be needed that provide protection for all types of members, are cost effective for large and smaller schemes, are tradable, and enable access to the wider capital markets. Index-based solutions have the potential to meet this need; however, concerns remain with these solutions. In particular, the basis risk emerging from the potential mismatch between the underlying forces of mortality for the index reference portfolio and the pension fund/annuity book being hedged is the principal issue that has, to date, prevented many schemes progressing their consideration of index-based solutions. Two-population stochastic mortality models offer an alternative to overcome this obstacle as they allow market participants to compare and project the mortality experience for the reference and target populations and thus assess the amount of demographic basis risk involved in an index-based longevity hedge. In this paper, we systematically assess the suitability of several multi-population stochastic mortality models for assessing basis risks and provide guidelines on how to use these models in practical situations paying particular attention to the data requirements for the appropriate calibration and forecasting of such models.},
  file = {/Users/salvatorykessy/Zotero/storage/BUWLGVDR/clusteringmortality.pdf;/Users/salvatorykessy/Zotero/storage/BZBHCP5R/Villegas et al. - 2017 - A COMPARATIVE STUDY of TWO-POPULATION MODELS for the ASSESSMENT of BASIS RISK in LONGEVITY HEDGES(2).pdf;/Users/salvatorykessy/Zotero/storage/XQSM7HI5/WhatToForecast(2).pdf},
  journal = {ASTIN Bulletin},
  keywords = {hedge effectiveness,index-based hedges,longevity basis risk,Stochastic morality,two-population mortality models},
  number = {3}
}

@article{villegasMortalityModellingSocioeconomic2012,
  title = {Mortality: {{Modelling}} and {{Socio}}-Economic {{Differentials}}},
  author = {Villegas, Andres},
  year = {2012},
  pages = {111},
  file = {/Users/salvatorykessy/Zotero/storage/VQQPGNA9/Villegas - Mortality Modelling and Socio-economic Diﬀerentia.pdf},
  language = {en}
}

@article{villegasStMoMoPackageStochastic2015,
  title = {{{StMoMo}}: {{An R Package}} for {{Stochastic Mortality Modelling}}},
  author = {Villegas, Andrrs and Kaishev, Vladimir K. and Millossovich, Pietro},
  year = {2015},
  doi = {10.2139/ssrn.2698729},
  abstract = {In this paper we mirror the framework of generalised (non-)linear models to de ne the family of generalised Age-Period-Cohort stochastic mortality models which encompasses the vast majority of stochastic mortality projection models proposed to date, including the well-known Lee-Carter and Cairns-Blake-Dowd models. We also introduce the R package StMoMo which exploits the unifying framework of the generalised Age-Period-Cohort family to provide tools for  tting stochastic mortality models, assessing their goodness of  t and performing mortality projections. We illustrate some of the capabilities of the package by performing a comparison of several stochastic mortality models applied to the England and Wales population.},
  file = {/Users/salvatorykessy/Zotero/storage/M6QAW5Y2/Villegas, Kaishev, Millossovich - 2015 - StMoMo An R Package for Stochastic Mortality Modelling.pdf},
  journal = {SSRN Electronic Journal},
  keywords = {age-period-cohort,generalized non-,mortality forecasting,mortality modeling},
  number = {1992}
}

@article{villegasStMoMoPackageStochastic2019,
  title = {{{StMoMo}}: {{An R Package}} for {{Stochastic Mortality Modelling}}},
  author = {Villegas, Andrrs and Kaishev, Vladimir K. and Millossovich, Pietro},
  year = {2019},
  doi = {10.2139/ssrn.2698729},
  abstract = {In this paper we mirror the framework of generalised (non-)linear models to de ne the family of generalised Age-Period-Cohort stochastic mortality models which encompasses the vast majority of stochastic mortality projection models proposed to date, including the well-known Lee-Carter and Cairns-Blake-Dowd models. We also introduce the R package StMoMo which exploits the unifying framework of the generalised Age-Period-Cohort family to provide tools for  tting stochastic mortality models, assessing their goodness of  t and performing mortality projections. We illustrate some of the capabilities of the package by performing a comparison of several stochastic mortality models applied to the England and Wales population.},
  file = {/Users/salvatorykessy/Zotero/storage/4S4BKTVF/Villegas, Kaishev, Millossovich - 2015 - StMoMo An R Package for Stochastic Mortality Modelling.pdf},
  journal = {SSRN Electronic Journal},
  keywords = {age-period-cohort,generalized non-,mortality forecasting,mortality modeling},
  number = {1992}
}

@article{wangOptimalInvestmentStrategies2019,
  title = {Optimal Investment Strategies and Risk-Sharing Arrangements for a Hybrid Pension Plan},
  author = {Wang, Suxin and Lu, Yi},
  year = {2019},
  month = nov,
  volume = {89},
  pages = {46--62},
  issn = {01676687},
  doi = {10.1016/j.insmatheco.2019.09.005},
  abstract = {A continuous time stochastic model is used to study a hybrid pension plan, where both the contribution and benefit levels are adjusted depending on the performance of the plan, with risk sharing between different generations. The pension fund is invested in a risk-free asset and multiple risky assets. The objective is to seek an optimal investment strategy and optimal risk-sharing arrangements for plan trustees and participants so that this proposed hybrid pension system provides adequate and stable income to retirees while adjusting contributions effectively, as well as keeping its sustainability in the long run. These goals are achieved by minimizing the expected discount disutility of intermediate adjustment for both benefits and contributions and that of terminal wealth in finite time horizon. Using the stochastic optimal control approach, closed-form solutions are derived under quadratic loss function and exponential loss function. Numerical analysis is presented to illustrate the sensitivity of the optimal strategies to parameters of the financial market and how the optimal benefit changes with respect to different risk aversions. Through numerical analysis, we find that the optimal strategies do adjust the contributions and retirement benefits according to fund performance and model objectives so the intergenerational risk sharing seem effectively achieved for this collective hybrid pension plan. \textcopyright{} 2019 Elsevier B.V. All rights reserved.},
  file = {/Users/salvatorykessy/Zotero/storage/3MEHYTEW/bowers.pdf;/Users/salvatorykessy/Zotero/storage/QHSZZVV8/Wang and Lu - 2019 - Optimal investment strategies and risk-sharing arr.pdf},
  journal = {Insurance: Mathematics and Economics},
  language = {en}
}

@article{wilmothAreMortalityProjections1995,
  title = {Are Mortality Projections Always More Pessimistic When Disaggregated by Cause of Death?},
  author = {Wilmoth, John R.},
  year = {1995},
  month = dec,
  volume = {5},
  pages = {293--319},
  issn = {0889-8480},
  doi = {10.1080/08898489509525409},
  annote = {doi: 10.1080/08898489509525409},
  journal = {Mathematical Population Studies},
  number = {4}
}

@article{wisniowskiBayesianPopulationForecasting2015,
  title = {Bayesian {{Population Forecasting}}: {{Extending}} the {{Lee}}-{{Carter Method}}},
  shorttitle = {Bayesian {{Population Forecasting}}},
  author = {Wi{\'s}niowski, Arkadiusz and Smith, Peter W. F. and Bijak, Jakub and Raymer, James and Forster, Jonathan J.},
  year = {2015},
  month = jun,
  volume = {52},
  pages = {1035--1059},
  issn = {0070-3370, 1533-7790},
  doi = {10.1007/s13524-015-0389-y},
  abstract = {In this article, we develop a fully integrated and dynamic Bayesian approach to forecast populations by age and sex. The approach embeds the Lee-Carter type models for forecasting the age patterns, with associated measures of uncertainty, of fertility, mortality, immigration, and emigration within a cohort projection model. The methodology may be adapted to handle different data types and sources of information. To illustrate, we analyze time series data for the United Kingdom and forecast the components of population change to the year 2024. We also compare the results obtained from different forecast models for age-specific fertility, mortality, and migration. In doing so, we demonstrate the flexibility and advantages of adopting the Bayesian approach for population forecasting and highlight areas where this work could be extended.},
  file = {/Users/salvatorykessy/Zotero/storage/2DQ9L9LJ/StackedGeneralization.pdf;/Users/salvatorykessy/Zotero/storage/DAKT3SZL/Wiśniowski et al. - 2015 - Bayesian Population Forecasting Extending the Lee.pdf},
  journal = {Demography},
  language = {en},
  number = {3}
}

@article{wolpertSTACKEDGENERALIZATION,
  title = {{{STACKED GENERALIZATION}}},
  author = {Wolpert, David H},
  pages = {58},
  abstract = {This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory.},
  file = {/Users/salvatorykessy/Zotero/storage/H25F2V9J/Wolpert - STACKED GENERALIZATION.pdf},
  language = {en}
}

@article{wong-fupuyProjectingMortalityTrends2004,
  title = {Projecting {{Mortality Trends}}: {{Recent Developments}} in the {{United Kingdom}} and the {{United States}}},
  shorttitle = {Projecting {{Mortality Trends}}},
  author = {{Wong-Fupuy}, Carlos and Haberman, Steven},
  year = {2004},
  month = apr,
  volume = {8},
  pages = {56--83},
  issn = {1092-0277, 2325-0453},
  doi = {10.1080/10920277.2004.10596137},
  abstract = {The sustained reduction in mortality rates and its systematic underestimation has been attracting the significant interest of researchers in recent times because of its potential impact on population size and structure, social security systems, and (from an actuarial perspective) the life insurance and pensions industry worldwide. Despite the number of papers published in recent years, a comprehensive review has not yet been developed.},
  file = {/Users/salvatorykessy/Zotero/storage/K56D65HI/Wong-Fupuy and Haberman - 2004 - Projecting Mortality Trends Recent Developments i.pdf;/Users/salvatorykessy/Zotero/storage/NYHWK62U/bowers.pdf;/Users/salvatorykessy/Zotero/storage/UQWHLAD4/5oyears.pdf},
  journal = {North American Actuarial Journal},
  language = {en},
  number = {2}
}

@article{wongBayesianMortalityForecasting2018,
  title = {Bayesian Mortality Forecasting with Overdispersion},
  author = {Wong, Jackie S.T. and Forster, Jonathan J. and Smith, Peter W.F.},
  year = {2018},
  month = nov,
  volume = {83},
  pages = {206--221},
  issn = {01676687},
  doi = {10.1016/j.insmatheco.2017.09.023},
  abstract = {The ability to produce accurate mortality forecasts, accompanied by a set of representative uncertainty bands, is crucial in the planning of public retirement funds and various life-related businesses. In this paper, we focus on one of the drawbacks of the Poisson Lee\textendash Carter model (Brouhns et al., 2002) that imposes mean\textendash variance equality, restricting mortality variations across individuals. Specifically, we present two models to potentially account for overdispersion. We propose to fit these models within the Bayesian framework for various advantages, but primarily for coherency. Markov Chain Monte Carlo (MCMC) methods are implemented to carry out parameter estimation. Several comparisons are made with the Bayesian Poisson Lee\textendash Carter model (Czado et al., 2005) to highlight the importance of accounting for overdispersion. We demonstrate that the methodology we developed prevents over-fitting and yields better calibrated prediction intervals for the purpose of mortality projections. Bridge sampling is used to approximate the marginal likelihood of each candidate model to compare the models quantitatively.},
  file = {/Users/salvatorykessy/Zotero/storage/9CV9KA9N/Wong et al. - 2018 - Bayesian mortality forecasting with overdispersion.pdf},
  journal = {Insurance: Mathematics and Economics},
  language = {en}
}

@article{wongImprovementBayesPrediction2004,
  title = {Improvement over Bayes Prediction in Small Samples in the Presence of Model Uncertainty},
  author = {Wong, Hubert and Clarke, Bertrand},
  year = {2004},
  volume = {32},
  pages = {269--283},
  doi = {10.2307/3315929},
  file = {/Users/salvatorykessy/Zotero/storage/5MZZMWLW/lifeexpectance.pdf;/Users/salvatorykessy/Zotero/storage/HV2CZCIS/Wong, Clarke - 2004 - Improvement over bayes prediction in small samples in the presence of model uncertainty.pdf},
  journal = {Canadian Journal of Statistics},
  number = {3}
}

@article{wongImprovementBayesPrediction2004a,
  title = {Improvement over Bayes Prediction in Small Samples in the Presence of Model Uncertainty},
  author = {Wong, Hubert and Clarke, Bertrand},
  year = {2004},
  volume = {32},
  pages = {269--283},
  doi = {10.2307/3315929},
  file = {/Users/salvatorykessy/Zotero/storage/L42CIE96/Wong, Clarke - 2004 - Improvement over bayes prediction in small samples in the presence of model uncertainty.pdf},
  journal = {Canadian Journal of Statistics},
  number = {3}
}

@article{xuSystematicMortalityImprovement2019,
  title = {Systematic {{Mortality Improvement Trends}} and {{Mortality Heterogeneity}}: {{Insights}} from {{Individual}}-{{Level HRS Data}}},
  author = {Xu, Mengyi and Sherris, Michael and Meyricke, Ramona},
  year = {2019},
  volume = {0277},
  doi = {10.1080/10920277.2018.1513369},
  abstract = {Providers of life annuities and pensions need to consider both systematic mortality improvement trends and mortality hetero-geneity. Although how mortality improvement varies with age and gender at the population level is well studied, how trends vary with risk factors remains relatively unexplored. This article assesses how systematic mortality improvement trends vary with individual risk characteristics using individual-level longitudinal data from the U.S. Health and Retirement Study between 1994 and 2009. Initially a Lee-Carter model is used to assess mortality improvement trends by grouping individuals with similar risk characteristics of gender, education, and race. We then fit a longitudinal mortality model to individual-level data allowing for hetero-geneity and time trends in individual-level risk factors. Our results show how survey data can provide valuable insights into both mortality heterogeneity and improvement trends more effectively than commonly used aggregate models. We show how mortality improvement differs across individuals with different risk factors. Significantly, at an individual level, mortality improvement trends have been driven by changes in health history such as high blood pressure, cancer, and heart problems rather than risk factors such as education, marital status, body mass index, and smoker status.},
  file = {/Users/salvatorykessy/Zotero/storage/B7RWM3BT/Xu, Sherris, Meyricke - 2019 - Systematic Mortality Improvement Trends and Mortality Heterogeneity Insights from Individual-Level HRS Da.pdf},
  journal = {North American Actuarial Journal}
}

@article{xuSystematicMortalityImprovement2019a,
  title = {Systematic {{Mortality Improvement Trends}} and {{Mortality Heterogeneity}}: {{Insights}} from {{Individual}}-{{Level HRS Data}}},
  author = {Xu, Mengyi and Sherris, Michael and Meyricke, Ramona},
  year = {2019},
  volume = {0277},
  doi = {10.1080/10920277.2018.1513369},
  abstract = {Providers of life annuities and pensions need to consider both systematic mortality improvement trends and mortality hetero-geneity. Although how mortality improvement varies with age and gender at the population level is well studied, how trends vary with risk factors remains relatively unexplored. This article assesses how systematic mortality improvement trends vary with individual risk characteristics using individual-level longitudinal data from the U.S. Health and Retirement Study between 1994 and 2009. Initially a Lee-Carter model is used to assess mortality improvement trends by grouping individuals with similar risk characteristics of gender, education, and race. We then fit a longitudinal mortality model to individual-level data allowing for hetero-geneity and time trends in individual-level risk factors. Our results show how survey data can provide valuable insights into both mortality heterogeneity and improvement trends more effectively than commonly used aggregate models. We show how mortality improvement differs across individuals with different risk factors. Significantly, at an individual level, mortality improvement trends have been driven by changes in health history such as high blood pressure, cancer, and heart problems rather than risk factors such as education, marital status, body mass index, and smoker status.},
  file = {/Users/salvatorykessy/Zotero/storage/8IZJJ6RV/BIC.pdf;/Users/salvatorykessy/Zotero/storage/LY3ETI7B/Xu, Sherris, Meyricke - 2019 - Systematic Mortality Improvement Trends and Mortality Heterogeneity Insights from Individual-Level HRS Da.pdf},
  journal = {North American Actuarial Journal}
}

@article{yadavKnowledgeBasedSystemsHandling2018,
  title = {Knowledge-{{Based Systems Handling}} Missing Values : {{A}} Study of Popular Imputation Packages in {{R}}},
  author = {Yadav, Madan Lal and Roychoudhury, Basav},
  year = {2018},
  volume = {160},
  pages = {104--118},
  doi = {10.1016/j.knosys.2018.06.012},
  file = {/Users/salvatorykessy/Zotero/storage/3GJBITZ5/Greece_mortality.pdf;/Users/salvatorykessy/Zotero/storage/GX8TUF8J/Yadav, Roychoudhury - 2018 - Knowledge-Based Systems Handling missing values A study of popular imputation packages in R.pdf;/Users/salvatorykessy/Zotero/storage/HNKX6JGA/Newton L. Bowers, Hans U. Gerber, James C. Hickman, Donald A. Jones, Cecil J. Nesbitt - Actuarial Mathematics-Society of Actuaries (1997).pdf;/Users/salvatorykessy/Zotero/storage/ICY6PZWP/hybrid.pdf},
  journal = {Knowledge-Based Systems},
  keywords = {HMISC,Imputation Accuracy,Imputation Time,MICE,MissForest,missing value handling,Missing value handling,VIM},
  number = {December 2017}
}

@article{yadavKnowledgeBasedSystemsHandling2018a,
  title = {Knowledge-{{Based Systems Handling}} Missing Values : {{A}} Study of Popular Imputation Packages in {{R}}},
  author = {Yadav, Madan Lal and Roychoudhury, Basav},
  year = {2018},
  volume = {160},
  pages = {104--118},
  doi = {10.1016/j.knosys.2018.06.012},
  file = {/Users/salvatorykessy/Zotero/storage/W6BRXDHD/Yadav, Roychoudhury - 2018 - Knowledge-Based Systems Handling missing values A study of popular imputation packages in R.pdf},
  journal = {Knowledge-Based Systems},
  keywords = {HMISC,Imputation Accuracy,Imputation Time,MICE,MissForest,missing value handling,Missing value handling,VIM},
  number = {December 2017}
}

@article{yaoUsingStackingAverage2017,
  title = {Using Stacking to Average {{Bayesian}} Predictive Distributions},
  author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
  year = {2017},
  doi = {10.1214/17-BA1091},
  abstract = {The widely recommended procedure of Bayesian model averaging is flawed in the M-open setting in which the true data-generating process is not one of the candidate models being fit. We take the idea of stacking from the point estimation literature and generalize to the combination of predictive distributions, extending the utility function to any proper scoring rule, using Pareto smoothed importance sampling to efficiently compute the required leave-one-out posterior distributions and regularization to get more stability. We compare stacking of predictive distributions to several alternatives: stacking of means, Bayesian model averaging (BMA), pseudo-BMA using AIC-type weighting, and a variant of pseudo-BMA that is stabilized using the Bayesian bootstrap. Based on simulations and real-data applications, we recommend stacking of predictive distributions, with BB-pseudo-BMA as an approximate alternative when computation cost is an issue.},
  file = {/Users/salvatorykessy/Zotero/storage/A4TQ3WMP/Yao et al. - 2017 - Using stacking to average Bayesian predictive distributions.pdf;/Users/salvatorykessy/Zotero/storage/C6WJ5KI2/Poisson.pdf},
  keywords = {bayesian model averaging,model combination,predictive distribution,proper scoring rule,stacking,stan}
}

@article{yaoUsingStackingAverage2017a,
  title = {Using Stacking to Average {{Bayesian}} Predictive Distributions},
  author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
  year = {2017},
  doi = {10.1214/17-BA1091},
  abstract = {The widely recommended procedure of Bayesian model averaging is flawed in the M-open setting in which the true data-generating process is not one of the candidate models being fit. We take the idea of stacking from the point estimation literature and generalize to the combination of predictive distributions, extending the utility function to any proper scoring rule, using Pareto smoothed importance sampling to efficiently compute the required leave-one-out posterior distributions and regularization to get more stability. We compare stacking of predictive distributions to several alternatives: stacking of means, Bayesian model averaging (BMA), pseudo-BMA using AIC-type weighting, and a variant of pseudo-BMA that is stabilized using the Bayesian bootstrap. Based on simulations and real-data applications, we recommend stacking of predictive distributions, with BB-pseudo-BMA as an approximate alternative when computation cost is an issue.},
  file = {/Users/salvatorykessy/Zotero/storage/E9MVRA2L/Akaike_1974.pdf;/Users/salvatorykessy/Zotero/storage/Z73SFCQR/Yao et al. - 2017 - Using stacking to average Bayesian predictive distributions.pdf},
  keywords = {bayesian model averaging,model combination,predictive distribution,proper scoring rule,stacking,stan}
}

@article{zealandStackedGeneralizationDoes1992,
  title = {Stacked {{Generalization}} : W h e n Does It w o r k ? {{K}} a i {{M}} i n g {{T}} i n g and {{I}} a n {{H}} . {{W}} i t t e n {{Department}} of {{Computer Science}}},
  author = {Zealand, New},
  year = {1992},
  file = {/Users/salvatorykessy/Zotero/storage/ZXYYLY6S/Zealand - 1992 - Stacked Generalization w h e n does it w o r k K a i M i n g T i n g and I a n H . W i t t e n Department of Computer.pdf}
}

@article{zealandStackedGeneralizationDoes1992a,
  title = {Stacked {{Generalization}} : W h e n Does It w o r k ? {{K}} a i {{M}} i n g {{T}} i n g and {{I}} a n {{H}} . {{W}} i t t e n {{Department}} of {{Computer Science}}},
  author = {Zealand, New},
  year = {1992},
  file = {/Users/salvatorykessy/Zotero/storage/LCUBBMRC/Zealand - 1992 - Stacked Generalization w h e n does it w o r k K a i M i n g T i n g and I a n H . W i t t e n Department of Computer.pdf}
}

@article{zhangStrategiesCombiningTreebased2017,
  title = {Strategies for Combining Tree-Based Ensemble Models},
  author = {Zhang, Yi},
  year = {2017},
  pages = {1--150},
  abstract = {Ensemble models have proved effective in a variety of classification tasks. These models combine the predictions of several base models to achieve higher out-of-sample classification accuracy than the base models. Base models are typically trained using different subsets of training examples and input features. Ensemble classifiers are particularly effective when their constituent base models are diverse in terms of their prediction accuracy in different regions of the feature space. This dissertation investigated methods for combining ensemble models, treating them as base models. The goal is to develop a strategy for combining ensemble classifiers that results in higher classification accuracy than the constituent ensemble models. Three of the best performing tree-based ensemble methods \textendash{} rand om forest, extremely randomized tree, and extreme gradient boosting model \textendash{} were use d to generate a set of base models. Outputs from classifiers generated by these methods were then combined to create an ensemble classifier. This dissertation systematically investigated methods for (1) selecting a set of diverse base models, and (2) combining the selected base models. The methods were evaluated using public domain data sets which have been extensively used for bench-marking classification models. The research established that applying random forest as the final ensemble method to integrate selected base models and factor scores of multiple correspondence analysis turned out to be the best ensemble approach.},
  file = {/Users/salvatorykessy/Zotero/storage/BN4QVQRA/euclid_ris_20191111000000.ris;/Users/salvatorykessy/Zotero/storage/K9CGMP5N/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.ris},
  number = {1021}
}

@article{zhangStrategiesCombiningTreebased2017a,
  title = {Strategies for Combining Tree-Based Ensemble Models},
  author = {Zhang, Yi},
  year = {2017},
  pages = {1--150},
  abstract = {Ensemble models have proved effective in a variety of classification tasks. These models combine the predictions of several base models to achieve higher out-of-sample classification accuracy than the base models. Base models are typically trained using different subsets of training examples and input features. Ensemble classifiers are particularly effective when their constituent base models are diverse in terms of their prediction accuracy in different regions of the feature space. This dissertation investigated methods for combining ensemble models, treating them as base models. The goal is to develop a strategy for combining ensemble classifiers that results in higher classification accuracy than the constituent ensemble models. Three of the best performing tree-based ensemble methods \textendash{} rand om forest, extremely randomized tree, and extreme gradient boosting model \textendash{} were use d to generate a set of base models. Outputs from classifiers generated by these methods were then combined to create an ensemble classifier. This dissertation systematically investigated methods for (1) selecting a set of diverse base models, and (2) combining the selected base models. The methods were evaluated using public domain data sets which have been extensively used for bench-marking classification models. The research established that applying random forest as the final ensemble method to integrate selected base models and factor scores of multiple correspondence analysis turned out to be the best ensemble approach.},
  file = {/Users/salvatorykessy/Zotero/storage/9H7MSB7D/Johnson et al. - 2018 - Comparing Bayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored.pdf;/Users/salvatorykessy/Zotero/storage/WSUKQEGK/Demographicforecasting.pdf},
  number = {1021}
}

@article{zhangStrategiesCombiningTreebased2017b,
  title = {Strategies for Combining Tree-Based Ensemble Models},
  author = {Zhang, Yi},
  year = {2017},
  pages = {1--150},
  abstract = {Ensemble models have proved effective in a variety of classification tasks. These models combine the predictions of several base models to achieve higher out-of-sample classification accuracy than the base models. Base models are typically trained using different subsets of training examples and input features. Ensemble classifiers are particularly effective when their constituent base models are diverse in terms of their prediction accuracy in different regions of the feature space. This dissertation investigated methods for combining ensemble models, treating them as base models. The goal is to develop a strategy for combining ensemble classifiers that results in higher classification accuracy than the constituent ensemble models. Three of the best performing tree-based ensemble methods \textendash{} rand om forest, extremely randomized tree, and extreme gradient boosting model \textendash{} were use d to generate a set of base models. Outputs from classifiers generated by these methods were then combined to create an ensemble classifier. This dissertation systematically investigated methods for (1) selecting a set of diverse base models, and (2) combining the selected base models. The methods were evaluated using public domain data sets which have been extensively used for bench-marking classification models. The research established that applying random forest as the final ensemble method to integrate selected base models and factor scores of multiple correspondence analysis turned out to be the best ensemble approach.},
  number = {1021}
}

@article{zotero-221,
  type = {Article}
}


